Tokenizing tekst text into sentances: 0it [00:00, ?it/s]
cleaning sentances: 0it [00:00, ?it/s][A

embedding all sents with BERT: 0it [00:00, ?it/s][A[A


reducing sentences to a single doc embeding: 0it [00:00, ?it/s][A[A[A


reducing sentences to a single doc embeding: 1it [00:04,  4.54s/it][A[A[A

embedding all sents with BERT: 1it [00:04,  4.54s/it][A[A
cleaning sentances: 1it [00:06,  6.89s/it][ATokenizing tekst text into sentances: 1it [00:06,  6.89s/it]


reducing sentences to a single doc embeding: 2it [00:06,  3.09s/it][A[A[A

embedding all sents with BERT: 2it [00:06,  3.09s/it][A[A
cleaning sentances: 2it [00:08,  4.06s/it][ATokenizing tekst text into sentances: 2it [00:08,  4.06s/it]


reducing sentences to a single doc embeding: 3it [00:09,  2.87s/it][A[A[A

embedding all sents with BERT: 3it [00:09,  2.87s/it][A[A
cleaning sentances: 3it [00:11,  3.40s/it][ATokenizing tekst text into sentances: 3it [00:11,  3.40s/it]


reducing sentences to a single doc embeding: 4it [00:11,  2.83s/it][A[A[A

embedding all sents with BERT: 4it [00:11,  2.83s/it][A[A
cleaning sentances: 4it [00:14,  3.15s/it][ATokenizing tekst text into sentances: 4it [00:14,  3.15s/it]


reducing sentences to a single doc embeding: 5it [00:16,  3.39s/it][A[A[A

embedding all sents with BERT: 5it [00:16,  3.39s/it][A[A
cleaning sentances: 5it [00:18,  3.59s/it][ATokenizing tekst text into sentances: 5it [00:18,  3.59s/it]


reducing sentences to a single doc embeding: 6it [00:18,  2.96s/it][A[A[A

embedding all sents with BERT: 6it [00:18,  2.96s/it][A[A
cleaning sentances: 6it [00:20,  3.09s/it][ATokenizing tekst text into sentances: 6it [00:20,  3.09s/it]


reducing sentences to a single doc embeding: 7it [00:20,  2.68s/it][A[A[A

embedding all sents with BERT: 7it [00:20,  2.68s/it][A[A
cleaning sentances: 7it [00:22,  2.77s/it][ATokenizing tekst text into sentances: 7it [00:22,  2.77s/it]


reducing sentences to a single doc embeding: 8it [00:22,  2.28s/it][A[A[A

embedding all sents with BERT: 8it [00:22,  2.28s/it][A[A
cleaning sentances: 8it [00:24,  2.34s/it][ATokenizing tekst text into sentances: 8it [00:24,  2.34s/it]


reducing sentences to a single doc embeding: 9it [00:23,  2.16s/it][A[A[A

embedding all sents with BERT: 9it [00:23,  2.16s/it][A[A
cleaning sentances: 9it [00:26,  2.20s/it][ATokenizing tekst text into sentances: 9it [00:26,  2.20s/it]


reducing sentences to a single doc embeding: 10it [00:25,  1.91s/it][A[A[A

embedding all sents with BERT: 10it [00:25,  1.91s/it][A[A
cleaning sentances: 10it [00:27,  1.94s/it][ATokenizing tekst text into sentances: 10it [00:27,  1.94s/it]


reducing sentences to a single doc embeding: 11it [00:26,  1.71s/it][A[A[A

embedding all sents with BERT: 11it [00:26,  1.71s/it][A[A
cleaning sentances: 11it [00:28,  1.73s/it][ATokenizing tekst text into sentances: 11it [00:28,  1.73s/it]


reducing sentences to a single doc embeding: 12it [00:28,  1.89s/it][A[A[A

embedding all sents with BERT: 12it [00:28,  1.89s/it][A[A
cleaning sentances: 12it [00:31,  1.91s/it][ATokenizing tekst text into sentances: 12it [00:31,  1.91s/it]


reducing sentences to a single doc embeding: 13it [00:29,  1.55s/it][A[A[A

embedding all sents with BERT: 13it [00:29,  1.55s/it][A[A
cleaning sentances: 13it [00:31,  1.56s/it][ATokenizing tekst text into sentances: 13it [00:31,  1.56s/it]


reducing sentences to a single doc embeding: 14it [00:31,  1.65s/it][A[A[A

embedding all sents with BERT: 14it [00:31,  1.65s/it][A[A
cleaning sentances: 14it [00:33,  1.66s/it][ATokenizing tekst text into sentances: 14it [00:33,  1.66s/it]


reducing sentences to a single doc embeding: 15it [00:32,  1.59s/it][A[A[A

embedding all sents with BERT: 15it [00:32,  1.59s/it][A[A
cleaning sentances: 15it [00:35,  1.59s/it][ATokenizing tekst text into sentances: 15it [00:35,  1.59s/it]


reducing sentences to a single doc embeding: 16it [00:34,  1.57s/it][A[A[A

embedding all sents with BERT: 16it [00:34,  1.57s/it][A[A
cleaning sentances: 16it [00:36,  1.57s/it][ATokenizing tekst text into sentances: 16it [00:36,  1.57s/it]


reducing sentences to a single doc embeding: 17it [00:36,  1.61s/it][A[A[A

embedding all sents with BERT: 17it [00:36,  1.61s/it][A[A
cleaning sentances: 17it [00:38,  1.61s/it][ATokenizing tekst text into sentances: 17it [00:38,  1.61s/it]


reducing sentences to a single doc embeding: 18it [00:37,  1.67s/it][A[A[A

embedding all sents with BERT: 18it [00:37,  1.67s/it][A[A
cleaning sentances: 18it [00:40,  1.67s/it][ATokenizing tekst text into sentances: 18it [00:40,  1.67s/it]


reducing sentences to a single doc embeding: 19it [00:39,  1.58s/it][A[A[A

embedding all sents with BERT: 19it [00:39,  1.58s/it][A[A
cleaning sentances: 19it [00:41,  1.58s/it][ATokenizing tekst text into sentances: 19it [00:41,  1.58s/it]


reducing sentences to a single doc embeding: 20it [00:40,  1.47s/it][A[A[A

embedding all sents with BERT: 20it [00:40,  1.47s/it][A[A
cleaning sentances: 20it [00:42,  1.47s/it][ATokenizing tekst text into sentances: 20it [00:42,  1.47s/it]


reducing sentences to a single doc embeding: 21it [00:47,  3.10s/it][A[A[A

embedding all sents with BERT: 21it [00:47,  3.10s/it][A[A
cleaning sentances: 21it [00:49,  3.10s/it][ATokenizing tekst text into sentances: 21it [00:49,  3.10s/it]


reducing sentences to a single doc embeding: 22it [00:49,  2.80s/it][A[A[A

embedding all sents with BERT: 22it [00:49,  2.80s/it][A[A
cleaning sentances: 22it [00:51,  2.80s/it][ATokenizing tekst text into sentances: 22it [00:51,  2.80s/it]


reducing sentences to a single doc embeding: 23it [00:50,  2.20s/it][A[A[A

embedding all sents with BERT: 23it [00:50,  2.20s/it][A[A
cleaning sentances: 23it [00:52,  2.20s/it][ATokenizing tekst text into sentances: 23it [00:52,  2.20s/it]


reducing sentences to a single doc embeding: 24it [00:52,  2.20s/it][A[A[A

embedding all sents with BERT: 24it [00:52,  2.20s/it][A[A
cleaning sentances: 24it [00:54,  2.20s/it][ATokenizing tekst text into sentances: 24it [00:54,  2.20s/it]


reducing sentences to a single doc embeding: 25it [00:54,  2.07s/it][A[A[A

embedding all sents with BERT: 25it [00:54,  2.07s/it][A[A
cleaning sentances: 25it [00:56,  2.07s/it][ATokenizing tekst text into sentances: 25it [00:56,  2.07s/it]


reducing sentences to a single doc embeding: 26it [00:55,  1.84s/it][A[A[A

embedding all sents with BERT: 26it [00:55,  1.84s/it][A[A
cleaning sentances: 26it [00:57,  1.84s/it][ATokenizing tekst text into sentances: 26it [00:57,  1.84s/it]


reducing sentences to a single doc embeding: 27it [00:56,  1.60s/it][A[A[A

embedding all sents with BERT: 27it [00:56,  1.60s/it][A[A
cleaning sentances: 27it [00:59,  1.60s/it][ATokenizing tekst text into sentances: 27it [00:59,  1.60s/it]


reducing sentences to a single doc embeding: 28it [00:58,  1.78s/it][A[A[A

embedding all sents with BERT: 28it [00:58,  1.78s/it][A[A
cleaning sentances: 28it [01:01,  1.78s/it][ATokenizing tekst text into sentances: 28it [01:01,  1.78s/it]


reducing sentences to a single doc embeding: 29it [01:01,  1.92s/it][A[A[A

embedding all sents with BERT: 29it [01:01,  1.92s/it][A[A
cleaning sentances: 29it [01:03,  1.92s/it][ATokenizing tekst text into sentances: 29it [01:03,  1.92s/it]


reducing sentences to a single doc embeding: 30it [01:02,  1.72s/it][A[A[A

embedding all sents with BERT: 30it [01:02,  1.72s/it][A[A
cleaning sentances: 30it [01:04,  1.72s/it][ATokenizing tekst text into sentances: 30it [01:04,  1.72s/it]


reducing sentences to a single doc embeding: 31it [01:03,  1.64s/it][A[A[A

embedding all sents with BERT: 31it [01:03,  1.64s/it][A[A
cleaning sentances: 31it [01:06,  1.64s/it][ATokenizing tekst text into sentances: 31it [01:06,  1.64s/it]


reducing sentences to a single doc embeding: 32it [01:04,  1.41s/it][A[A[A

embedding all sents with BERT: 32it [01:04,  1.41s/it][A[A
cleaning sentances: 32it [01:07,  1.41s/it][ATokenizing tekst text into sentances: 32it [01:07,  1.41s/it]


reducing sentences to a single doc embeding: 33it [01:29,  8.46s/it][A[A[A

embedding all sents with BERT: 33it [01:29,  8.46s/it][A[A
cleaning sentances: 33it [01:31,  8.46s/it][ATokenizing tekst text into sentances: 33it [01:31,  8.46s/it]


reducing sentences to a single doc embeding: 34it [01:30,  6.24s/it][A[A[A

embedding all sents with BERT: 34it [01:30,  6.24s/it][A[A
cleaning sentances: 34it [01:33,  6.24s/it][ATokenizing tekst text into sentances: 34it [01:33,  6.24s/it]


reducing sentences to a single doc embeding: 35it [01:31,  4.64s/it][A[A[A

embedding all sents with BERT: 35it [01:31,  4.64s/it][A[A
cleaning sentances: 35it [01:33,  4.64s/it][ATokenizing tekst text into sentances: 35it [01:33,  4.64s/it]


reducing sentences to a single doc embeding: 36it [01:32,  3.46s/it][A[A[A

embedding all sents with BERT: 36it [01:32,  3.46s/it][A[A
cleaning sentances: 36it [01:34,  3.46s/it][ATokenizing tekst text into sentances: 36it [01:34,  3.46s/it]


reducing sentences to a single doc embeding: 37it [01:33,  2.91s/it][A[A[A

embedding all sents with BERT: 37it [01:33,  2.91s/it][A[A
cleaning sentances: 37it [01:36,  2.91s/it][ATokenizing tekst text into sentances: 37it [01:36,  2.91s/it]


reducing sentences to a single doc embeding: 38it [01:35,  2.40s/it][A[A[A

embedding all sents with BERT: 38it [01:35,  2.40s/it][A[A
cleaning sentances: 38it [01:37,  2.40s/it][ATokenizing tekst text into sentances: 38it [01:37,  2.40s/it]


reducing sentences to a single doc embeding: 39it [01:36,  1.99s/it][A[A[A

embedding all sents with BERT: 39it [01:36,  1.99s/it][A[A
cleaning sentances: 39it [01:38,  1.99s/it][ATokenizing tekst text into sentances: 39it [01:38,  1.99s/it]


reducing sentences to a single doc embeding: 40it [01:37,  1.78s/it][A[A[A

embedding all sents with BERT: 40it [01:37,  1.78s/it][A[A
cleaning sentances: 40it [01:39,  1.78s/it][ATokenizing tekst text into sentances: 40it [01:39,  1.78s/it]


reducing sentences to a single doc embeding: 41it [01:38,  1.46s/it][A[A[A

embedding all sents with BERT: 41it [01:38,  1.46s/it][A[A
cleaning sentances: 41it [01:40,  1.46s/it][ATokenizing tekst text into sentances: 41it [01:40,  1.46s/it]


reducing sentences to a single doc embeding: 42it [01:40,  1.69s/it][A[A[A

embedding all sents with BERT: 42it [01:40,  1.69s/it][A[A
cleaning sentances: 42it [01:42,  1.69s/it][ATokenizing tekst text into sentances: 42it [01:42,  1.69s/it]


reducing sentences to a single doc embeding: 43it [01:41,  1.57s/it][A[A[A

embedding all sents with BERT: 43it [01:41,  1.57s/it][A[A
cleaning sentances: 43it [01:44,  1.57s/it][ATokenizing tekst text into sentances: 43it [01:44,  1.57s/it]


reducing sentences to a single doc embeding: 44it [01:42,  1.47s/it][A[A[A

embedding all sents with BERT: 44it [01:42,  1.47s/it][A[A
cleaning sentances: 44it [01:45,  1.47s/it][ATokenizing tekst text into sentances: 44it [01:45,  1.47s/it]


reducing sentences to a single doc embeding: 45it [01:44,  1.46s/it][A[A[A

embedding all sents with BERT: 45it [01:44,  1.46s/it][A[A
cleaning sentances: 45it [01:46,  1.46s/it][ATokenizing tekst text into sentances: 45it [01:46,  1.46s/it]


reducing sentences to a single doc embeding: 46it [01:45,  1.51s/it][A[A[A

embedding all sents with BERT: 46it [01:45,  1.51s/it][A[A
cleaning sentances: 46it [01:48,  1.51s/it][ATokenizing tekst text into sentances: 46it [01:48,  1.51s/it]


reducing sentences to a single doc embeding: 47it [01:47,  1.42s/it][A[A[A

embedding all sents with BERT: 47it [01:47,  1.42s/it][A[A
cleaning sentances: 47it [01:49,  1.42s/it][ATokenizing tekst text into sentances: 47it [01:49,  1.42s/it]


reducing sentences to a single doc embeding: 48it [01:47,  1.23s/it][A[A[A

embedding all sents with BERT: 48it [01:47,  1.23s/it][A[A
cleaning sentances: 48it [01:50,  1.23s/it][ATokenizing tekst text into sentances: 48it [01:50,  1.23s/it]


reducing sentences to a single doc embeding: 49it [01:49,  1.44s/it][A[A[A

embedding all sents with BERT: 49it [01:49,  1.44s/it][A[A
cleaning sentances: 49it [01:52,  1.44s/it][ATokenizing tekst text into sentances: 49it [01:52,  1.44s/it]


reducing sentences to a single doc embeding: 50it [01:50,  1.22s/it][A[A[A

embedding all sents with BERT: 50it [01:50,  1.22s/it][A[A
cleaning sentances: 50it [01:52,  1.22s/it][ATokenizing tekst text into sentances: 50it [01:52,  1.22s/it]


reducing sentences to a single doc embeding: 51it [01:52,  1.32s/it][A[A[A

embedding all sents with BERT: 51it [01:52,  1.32s/it][A[A
cleaning sentances: 51it [01:54,  1.32s/it][ATokenizing tekst text into sentances: 51it [01:54,  1.32s/it]


reducing sentences to a single doc embeding: 52it [01:53,  1.29s/it][A[A[A

embedding all sents with BERT: 52it [01:53,  1.29s/it][A[A
cleaning sentances: 52it [01:55,  1.29s/it][ATokenizing tekst text into sentances: 52it [01:55,  1.29s/it]


reducing sentences to a single doc embeding: 53it [01:55,  1.48s/it][A[A[A

embedding all sents with BERT: 53it [01:55,  1.48s/it][A[A
cleaning sentances: 53it [01:57,  1.48s/it][ATokenizing tekst text into sentances: 53it [01:57,  1.48s/it]


reducing sentences to a single doc embeding: 54it [01:56,  1.55s/it][A[A[A

embedding all sents with BERT: 54it [01:56,  1.55s/it][A[A
cleaning sentances: 54it [01:59,  1.55s/it][ATokenizing tekst text into sentances: 54it [01:59,  1.55s/it]


reducing sentences to a single doc embeding: 55it [01:58,  1.47s/it][A[A[A

embedding all sents with BERT: 55it [01:58,  1.47s/it][A[A
cleaning sentances: 55it [02:00,  1.47s/it][ATokenizing tekst text into sentances: 55it [02:00,  1.47s/it]


reducing sentences to a single doc embeding: 56it [02:00,  1.64s/it][A[A[A

embedding all sents with BERT: 56it [02:00,  1.64s/it][A[A
cleaning sentances: 56it [02:02,  1.64s/it][ATokenizing tekst text into sentances: 56it [02:02,  1.64s/it]


reducing sentences to a single doc embeding: 57it [02:01,  1.59s/it][A[A[A

embedding all sents with BERT: 57it [02:01,  1.59s/it][A[A
cleaning sentances: 57it [02:04,  1.59s/it][ATokenizing tekst text into sentances: 57it [02:04,  1.59s/it]


reducing sentences to a single doc embeding: 58it [02:03,  1.61s/it][A[A[A

embedding all sents with BERT: 58it [02:03,  1.61s/it][A[A
cleaning sentances: 58it [02:05,  1.61s/it][ATokenizing tekst text into sentances: 58it [02:05,  1.61s/it]


reducing sentences to a single doc embeding: 59it [02:05,  1.61s/it][A[A[A

embedding all sents with BERT: 59it [02:05,  1.61s/it][A[A
cleaning sentances: 59it [02:07,  1.61s/it][ATokenizing tekst text into sentances: 59it [02:07,  1.61s/it]


reducing sentences to a single doc embeding: 60it [02:06,  1.62s/it][A[A[A

embedding all sents with BERT: 60it [02:06,  1.62s/it][A[A
cleaning sentances: 60it [02:09,  1.62s/it][ATokenizing tekst text into sentances: 60it [02:09,  1.62s/it]


reducing sentences to a single doc embeding: 61it [02:07,  1.43s/it][A[A[A

embedding all sents with BERT: 61it [02:07,  1.43s/it][A[A
cleaning sentances: 61it [02:10,  1.43s/it][ATokenizing tekst text into sentances: 61it [02:10,  1.43s/it]


reducing sentences to a single doc embeding: 62it [02:09,  1.49s/it][A[A[A

embedding all sents with BERT: 62it [02:09,  1.49s/it][A[A
cleaning sentances: 62it [02:11,  1.49s/it][ATokenizing tekst text into sentances: 62it [02:11,  1.49s/it]


reducing sentences to a single doc embeding: 63it [02:10,  1.43s/it][A[A[A

embedding all sents with BERT: 63it [02:10,  1.43s/it][A[A
cleaning sentances: 63it [02:12,  1.43s/it][ATokenizing tekst text into sentances: 63it [02:12,  1.43s/it]


reducing sentences to a single doc embeding: 64it [02:12,  1.50s/it][A[A[A

embedding all sents with BERT: 64it [02:12,  1.50s/it][A[A
cleaning sentances: 64it [02:14,  1.50s/it][ATokenizing tekst text into sentances: 64it [02:14,  1.50s/it]


reducing sentences to a single doc embeding: 65it [02:13,  1.37s/it][A[A[A

embedding all sents with BERT: 65it [02:13,  1.37s/it][A[A
cleaning sentances: 65it [02:15,  1.37s/it][ATokenizing tekst text into sentances: 65it [02:15,  1.37s/it]


reducing sentences to a single doc embeding: 66it [02:14,  1.31s/it][A[A[A

embedding all sents with BERT: 66it [02:14,  1.31s/it][A[A
cleaning sentances: 66it [02:16,  1.31s/it][ATokenizing tekst text into sentances: 66it [02:16,  1.31s/it]


reducing sentences to a single doc embeding: 67it [02:15,  1.32s/it][A[A[A

embedding all sents with BERT: 67it [02:15,  1.32s/it][A[A
cleaning sentances: 67it [02:18,  1.32s/it][ATokenizing tekst text into sentances: 67it [02:18,  1.32s/it]


reducing sentences to a single doc embeding: 68it [02:16,  1.22s/it][A[A[A

embedding all sents with BERT: 68it [02:16,  1.22s/it][A[A
cleaning sentances: 68it [02:19,  1.22s/it][ATokenizing tekst text into sentances: 68it [02:19,  1.22s/it]


reducing sentences to a single doc embeding: 69it [02:18,  1.32s/it][A[A[A

embedding all sents with BERT: 69it [02:18,  1.32s/it][A[A
cleaning sentances: 69it [02:20,  1.32s/it][ATokenizing tekst text into sentances: 69it [02:20,  1.32s/it]


reducing sentences to a single doc embeding: 70it [02:19,  1.20s/it][A[A[A

embedding all sents with BERT: 70it [02:19,  1.20s/it][A[A
cleaning sentances: 70it [02:21,  1.20s/it][ATokenizing tekst text into sentances: 70it [02:21,  1.20s/it]


reducing sentences to a single doc embeding: 71it [02:20,  1.13s/it][A[A[A

embedding all sents with BERT: 71it [02:20,  1.13s/it][A[A
cleaning sentances: 71it [02:22,  1.13s/it][ATokenizing tekst text into sentances: 71it [02:22,  1.13s/it]


reducing sentences to a single doc embeding: 72it [02:21,  1.13s/it][A[A[A

embedding all sents with BERT: 72it [02:21,  1.13s/it][A[A
cleaning sentances: 72it [02:23,  1.13s/it][ATokenizing tekst text into sentances: 72it [02:23,  1.13s/it]


reducing sentences to a single doc embeding: 73it [02:23,  1.44s/it][A[A[A

embedding all sents with BERT: 73it [02:23,  1.44s/it][A[A
cleaning sentances: 73it [02:25,  1.44s/it][ATokenizing tekst text into sentances: 73it [02:25,  1.44s/it]


reducing sentences to a single doc embeding: 74it [02:24,  1.43s/it][A[A[A

embedding all sents with BERT: 74it [02:24,  1.43s/it][A[A
cleaning sentances: 74it [02:27,  1.43s/it][ATokenizing tekst text into sentances: 74it [02:27,  1.43s/it]


reducing sentences to a single doc embeding: 75it [02:26,  1.37s/it][A[A[A

embedding all sents with BERT: 75it [02:26,  1.37s/it][A[A
cleaning sentances: 75it [02:28,  1.37s/it][ATokenizing tekst text into sentances: 75it [02:28,  1.37s/it]


reducing sentences to a single doc embeding: 76it [02:46,  6.96s/it][A[A[A

embedding all sents with BERT: 76it [02:46,  6.96s/it][A[A
cleaning sentances: 76it [02:48,  6.96s/it][ATokenizing tekst text into sentances: 76it [02:48,  6.96s/it]


reducing sentences to a single doc embeding: 77it [02:47,  5.13s/it][A[A[A

embedding all sents with BERT: 77it [02:47,  5.13s/it][A[A
cleaning sentances: 77it [02:49,  5.13s/it][ATokenizing tekst text into sentances: 77it [02:49,  5.13s/it]


reducing sentences to a single doc embeding: 78it [02:48,  4.11s/it][A[A[A

embedding all sents with BERT: 78it [02:48,  4.11s/it][A[A
cleaning sentances: 78it [02:51,  4.11s/it][ATokenizing tekst text into sentances: 78it [02:51,  4.11s/it]


reducing sentences to a single doc embeding: 79it [02:50,  3.30s/it][A[A[A

embedding all sents with BERT: 79it [02:50,  3.30s/it][A[A
cleaning sentances: 79it [02:52,  3.30s/it][ATokenizing tekst text into sentances: 79it [02:52,  3.30s/it]


reducing sentences to a single doc embeding: 80it [02:51,  2.69s/it][A[A[A

embedding all sents with BERT: 80it [02:51,  2.69s/it][A[A
cleaning sentances: 80it [02:53,  2.69s/it][ATokenizing tekst text into sentances: 80it [02:53,  2.69s/it]


reducing sentences to a single doc embeding: 81it [02:53,  2.39s/it][A[A[A

embedding all sents with BERT: 81it [02:53,  2.39s/it][A[A
cleaning sentances: 81it [02:55,  2.39s/it][ATokenizing tekst text into sentances: 81it [02:55,  2.39s/it]


reducing sentences to a single doc embeding: 82it [02:55,  2.33s/it][A[A[A

embedding all sents with BERT: 82it [02:55,  2.33s/it][A[A
cleaning sentances: 82it [02:57,  2.33s/it][ATokenizing tekst text into sentances: 82it [02:57,  2.33s/it]


reducing sentences to a single doc embeding: 83it [02:57,  2.30s/it][A[A[A

embedding all sents with BERT: 83it [02:57,  2.30s/it][A[A
cleaning sentances: 83it [02:59,  2.30s/it][ATokenizing tekst text into sentances: 83it [02:59,  2.30s/it]


reducing sentences to a single doc embeding: 84it [02:59,  2.05s/it][A[A[A

embedding all sents with BERT: 84it [02:59,  2.05s/it][A[A
cleaning sentances: 84it [03:01,  2.05s/it][ATokenizing tekst text into sentances: 84it [03:01,  2.05s/it]


reducing sentences to a single doc embeding: 85it [03:00,  1.91s/it][A[A[A

embedding all sents with BERT: 85it [03:00,  1.91s/it][A[A
cleaning sentances: 85it [03:03,  1.91s/it][ATokenizing tekst text into sentances: 85it [03:03,  1.91s/it]


reducing sentences to a single doc embeding: 86it [03:02,  1.91s/it][A[A[A

embedding all sents with BERT: 86it [03:02,  1.91s/it][A[A
cleaning sentances: 86it [03:04,  1.91s/it][ATokenizing tekst text into sentances: 86it [03:04,  1.91s/it]


reducing sentences to a single doc embeding: 87it [03:03,  1.75s/it][A[A[A

embedding all sents with BERT: 87it [03:03,  1.75s/it][A[A
cleaning sentances: 87it [03:06,  1.75s/it][ATokenizing tekst text into sentances: 87it [03:06,  1.75s/it]


reducing sentences to a single doc embeding: 88it [03:05,  1.83s/it][A[A[A

embedding all sents with BERT: 88it [03:05,  1.83s/it][A[A
cleaning sentances: 88it [03:08,  1.83s/it][ATokenizing tekst text into sentances: 88it [03:08,  1.83s/it]


reducing sentences to a single doc embeding: 89it [03:08,  1.97s/it][A[A[A

embedding all sents with BERT: 89it [03:08,  1.97s/it][A[A
cleaning sentances: 89it [03:10,  1.97s/it][ATokenizing tekst text into sentances: 89it [03:10,  1.97s/it]


reducing sentences to a single doc embeding: 90it [03:10,  1.99s/it][A[A[A

embedding all sents with BERT: 90it [03:10,  1.99s/it][A[A
cleaning sentances: 90it [03:12,  1.99s/it][ATokenizing tekst text into sentances: 90it [03:12,  1.99s/it]


reducing sentences to a single doc embeding: 91it [03:11,  1.66s/it][A[A[A

embedding all sents with BERT: 91it [03:11,  1.66s/it][A[A
cleaning sentances: 91it [03:13,  1.66s/it][ATokenizing tekst text into sentances: 91it [03:13,  1.66s/it]


reducing sentences to a single doc embeding: 92it [03:12,  1.57s/it][A[A[A

embedding all sents with BERT: 92it [03:12,  1.57s/it][A[A
cleaning sentances: 92it [03:14,  1.57s/it][ATokenizing tekst text into sentances: 92it [03:14,  1.57s/it]


reducing sentences to a single doc embeding: 93it [03:13,  1.48s/it][A[A[A

embedding all sents with BERT: 93it [03:13,  1.48s/it][A[A
cleaning sentances: 93it [03:16,  1.48s/it][ATokenizing tekst text into sentances: 93it [03:16,  1.48s/it]


reducing sentences to a single doc embeding: 94it [03:15,  1.61s/it][A[A[A

embedding all sents with BERT: 94it [03:15,  1.61s/it][A[A
cleaning sentances: 94it [03:18,  1.61s/it][ATokenizing tekst text into sentances: 94it [03:18,  1.61s/it]


reducing sentences to a single doc embeding: 95it [03:17,  1.66s/it][A[A[A

embedding all sents with BERT: 95it [03:17,  1.66s/it][A[A
cleaning sentances: 95it [03:19,  1.66s/it][ATokenizing tekst text into sentances: 95it [03:19,  1.66s/it]


reducing sentences to a single doc embeding: 96it [03:18,  1.40s/it][A[A[A

embedding all sents with BERT: 96it [03:18,  1.40s/it][A[A
cleaning sentances: 96it [03:20,  1.40s/it][ATokenizing tekst text into sentances: 96it [03:20,  1.40s/it]


reducing sentences to a single doc embeding: 97it [03:20,  1.63s/it][A[A[A

embedding all sents with BERT: 97it [03:20,  1.63s/it][A[A
cleaning sentances: 97it [03:22,  1.63s/it][ATokenizing tekst text into sentances: 97it [03:22,  1.63s/it]


reducing sentences to a single doc embeding: 98it [03:22,  1.65s/it][A[A[A

embedding all sents with BERT: 98it [03:22,  1.65s/it][A[A
cleaning sentances: 98it [03:24,  1.65s/it][ATokenizing tekst text into sentances: 98it [03:24,  1.65s/it]


reducing sentences to a single doc embeding: 99it [03:24,  1.80s/it][A[A[A

embedding all sents with BERT: 99it [03:24,  1.80s/it][A[A
cleaning sentances: 99it [03:26,  1.80s/it][ATokenizing tekst text into sentances: 99it [03:26,  1.80s/it]


reducing sentences to a single doc embeding: 100it [03:26,  1.98s/it][A[A[A

embedding all sents with BERT: 100it [03:26,  1.98s/it][A[A
cleaning sentances: 100it [03:29,  1.98s/it][ATokenizing tekst text into sentances: 100it [03:29,  1.98s/it]


reducing sentences to a single doc embeding: 101it [03:29,  2.08s/it][A[A[A

embedding all sents with BERT: 101it [03:29,  2.08s/it][A[A
cleaning sentances: 101it [03:31,  2.08s/it][ATokenizing tekst text into sentances: 101it [03:31,  2.08s/it]


reducing sentences to a single doc embeding: 102it [03:30,  1.88s/it][A[A[A

embedding all sents with BERT: 102it [03:30,  1.88s/it][A[A
cleaning sentances: 102it [03:32,  1.88s/it][ATokenizing tekst text into sentances: 102it [03:32,  1.88s/it]


reducing sentences to a single doc embeding: 103it [03:33,  2.10s/it][A[A[A

embedding all sents with BERT: 103it [03:33,  2.10s/it][A[A
cleaning sentances: 103it [03:35,  2.10s/it][ATokenizing tekst text into sentances: 103it [03:35,  2.10s/it]


reducing sentences to a single doc embeding: 104it [03:34,  1.90s/it][A[A[A

embedding all sents with BERT: 104it [03:34,  1.90s/it][A[A
cleaning sentances: 104it [03:36,  1.90s/it][ATokenizing tekst text into sentances: 104it [03:36,  1.90s/it]


reducing sentences to a single doc embeding: 105it [03:36,  1.97s/it][A[A[A

embedding all sents with BERT: 105it [03:36,  1.97s/it][A[A
cleaning sentances: 105it [03:38,  1.97s/it][ATokenizing tekst text into sentances: 105it [03:38,  1.97s/it]


reducing sentences to a single doc embeding: 106it [03:38,  2.09s/it][A[A[A

embedding all sents with BERT: 106it [03:38,  2.09s/it][A[A
cleaning sentances: 106it [03:41,  2.09s/it][ATokenizing tekst text into sentances: 106it [03:41,  2.09s/it]


reducing sentences to a single doc embeding: 107it [03:41,  2.13s/it][A[A[A

embedding all sents with BERT: 107it [03:41,  2.13s/it][A[A
cleaning sentances: 107it [03:43,  2.13s/it][ATokenizing tekst text into sentances: 107it [03:43,  2.13s/it]


reducing sentences to a single doc embeding: 108it [03:41,  1.71s/it][A[A[A

embedding all sents with BERT: 108it [03:41,  1.71s/it][A[A
cleaning sentances: 108it [03:44,  1.71s/it][ATokenizing tekst text into sentances: 108it [03:44,  1.71s/it]


reducing sentences to a single doc embeding: 109it [03:43,  1.55s/it][A[A[A

embedding all sents with BERT: 109it [03:43,  1.55s/it][A[A
cleaning sentances: 109it [03:45,  1.55s/it][ATokenizing tekst text into sentances: 109it [03:45,  1.55s/it]


reducing sentences to a single doc embeding: 110it [03:44,  1.54s/it][A[A[A

embedding all sents with BERT: 110it [03:44,  1.54s/it][A[A
cleaning sentances: 110it [03:46,  1.54s/it][ATokenizing tekst text into sentances: 110it [03:46,  1.54s/it]


reducing sentences to a single doc embeding: 111it [03:48,  2.17s/it][A[A[A

embedding all sents with BERT: 111it [03:48,  2.17s/it][A[A
cleaning sentances: 111it [03:50,  2.17s/it][ATokenizing tekst text into sentances: 111it [03:50,  2.17s/it]


reducing sentences to a single doc embeding: 112it [03:50,  2.17s/it][A[A[A

embedding all sents with BERT: 112it [03:50,  2.17s/it][A[A
cleaning sentances: 112it [03:52,  2.17s/it][ATokenizing tekst text into sentances: 112it [03:52,  2.17s/it]


reducing sentences to a single doc embeding: 113it [03:51,  1.78s/it][A[A[A

embedding all sents with BERT: 113it [03:51,  1.78s/it][A[A
cleaning sentances: 113it [03:53,  1.78s/it][ATokenizing tekst text into sentances: 113it [03:53,  1.78s/it]


reducing sentences to a single doc embeding: 114it [03:51,  1.42s/it][A[A[A

embedding all sents with BERT: 114it [03:51,  1.42s/it][A[A
cleaning sentances: 114it [03:54,  1.42s/it][ATokenizing tekst text into sentances: 114it [03:54,  1.42s/it]


reducing sentences to a single doc embeding: 115it [03:52,  1.26s/it][A[A[A

embedding all sents with BERT: 115it [03:52,  1.26s/it][A[A
cleaning sentances: 115it [03:55,  1.26s/it][ATokenizing tekst text into sentances: 115it [03:55,  1.26s/it]


reducing sentences to a single doc embeding: 116it [03:54,  1.42s/it][A[A[A

embedding all sents with BERT: 116it [03:54,  1.42s/it][A[A
cleaning sentances: 116it [03:56,  1.42s/it][ATokenizing tekst text into sentances: 116it [03:56,  1.42s/it]


reducing sentences to a single doc embeding: 117it [03:56,  1.60s/it][A[A[A

embedding all sents with BERT: 117it [03:56,  1.60s/it][A[A
cleaning sentances: 117it [03:58,  1.60s/it][ATokenizing tekst text into sentances: 117it [03:58,  1.60s/it]


reducing sentences to a single doc embeding: 118it [03:57,  1.48s/it][A[A[A

embedding all sents with BERT: 118it [03:57,  1.48s/it][A[A
cleaning sentances: 118it [04:00,  1.48s/it][ATokenizing tekst text into sentances: 118it [04:00,  1.48s/it]


reducing sentences to a single doc embeding: 119it [03:59,  1.51s/it][A[A[A

embedding all sents with BERT: 119it [03:59,  1.51s/it][A[A
cleaning sentances: 119it [04:01,  1.51s/it][ATokenizing tekst text into sentances: 119it [04:01,  1.51s/it]


reducing sentences to a single doc embeding: 120it [04:00,  1.54s/it][A[A[A

embedding all sents with BERT: 120it [04:00,  1.54s/it][A[A
cleaning sentances: 120it [04:03,  1.54s/it][ATokenizing tekst text into sentances: 120it [04:03,  1.54s/it]


reducing sentences to a single doc embeding: 121it [04:03,  1.80s/it][A[A[A

embedding all sents with BERT: 121it [04:03,  1.80s/it][A[A
cleaning sentances: 121it [04:05,  1.80s/it][ATokenizing tekst text into sentances: 121it [04:05,  1.80s/it]


reducing sentences to a single doc embeding: 122it [04:04,  1.55s/it][A[A[A

embedding all sents with BERT: 122it [04:04,  1.55s/it][A[A
cleaning sentances: 122it [04:06,  1.55s/it][ATokenizing tekst text into sentances: 122it [04:06,  1.55s/it]


reducing sentences to a single doc embeding: 123it [04:09,  2.53s/it][A[A[A

embedding all sents with BERT: 123it [04:09,  2.53s/it][A[A
cleaning sentances: 123it [04:11,  2.53s/it][ATokenizing tekst text into sentances: 123it [04:11,  2.53s/it]


reducing sentences to a single doc embeding: 124it [04:11,  2.46s/it][A[A[A

embedding all sents with BERT: 124it [04:11,  2.46s/it][A[A
cleaning sentances: 124it [04:13,  2.46s/it][ATokenizing tekst text into sentances: 124it [04:13,  2.46s/it]


reducing sentences to a single doc embeding: 125it [04:12,  2.15s/it][A[A[A

embedding all sents with BERT: 125it [04:12,  2.15s/it][A[A
cleaning sentances: 125it [04:15,  2.15s/it][ATokenizing tekst text into sentances: 125it [04:15,  2.15s/it]


reducing sentences to a single doc embeding: 126it [04:15,  2.17s/it][A[A[A

embedding all sents with BERT: 126it [04:15,  2.17s/it][A[A
cleaning sentances: 126it [04:17,  2.17s/it][ATokenizing tekst text into sentances: 126it [04:17,  2.17s/it]


reducing sentences to a single doc embeding: 127it [04:16,  2.06s/it][A[A[A

embedding all sents with BERT: 127it [04:16,  2.06s/it][A[A
cleaning sentances: 127it [04:19,  2.06s/it][ATokenizing tekst text into sentances: 127it [04:19,  2.06s/it]


reducing sentences to a single doc embeding: 128it [04:18,  1.98s/it][A[A[A

embedding all sents with BERT: 128it [04:18,  1.98s/it][A[A
cleaning sentances: 128it [04:21,  1.98s/it][ATokenizing tekst text into sentances: 128it [04:21,  1.98s/it]


reducing sentences to a single doc embeding: 129it [04:20,  1.91s/it][A[A[A

embedding all sents with BERT: 129it [04:20,  1.91s/it][A[A
cleaning sentances: 129it [04:22,  1.91s/it][ATokenizing tekst text into sentances: 129it [04:22,  1.91s/it]


reducing sentences to a single doc embeding: 130it [04:22,  1.95s/it][A[A[A

embedding all sents with BERT: 130it [04:22,  1.95s/it][A[A
cleaning sentances: 130it [04:24,  1.95s/it][ATokenizing tekst text into sentances: 130it [04:24,  1.95s/it]


reducing sentences to a single doc embeding: 131it [04:23,  1.74s/it][A[A[A

embedding all sents with BERT: 131it [04:23,  1.74s/it][A[A
cleaning sentances: 131it [04:26,  1.74s/it][ATokenizing tekst text into sentances: 131it [04:26,  1.74s/it]


reducing sentences to a single doc embeding: 132it [04:26,  1.91s/it][A[A[A

embedding all sents with BERT: 132it [04:26,  1.91s/it][A[A
cleaning sentances: 132it [04:28,  1.91s/it][ATokenizing tekst text into sentances: 132it [04:28,  1.91s/it]


reducing sentences to a single doc embeding: 133it [04:27,  1.87s/it][A[A[A

embedding all sents with BERT: 133it [04:27,  1.87s/it][A[A
cleaning sentances: 133it [04:30,  1.87s/it][ATokenizing tekst text into sentances: 133it [04:30,  1.87s/it]


reducing sentences to a single doc embeding: 134it [04:29,  1.73s/it][A[A[A

embedding all sents with BERT: 134it [04:29,  1.73s/it][A[A
cleaning sentances: 134it [04:31,  1.73s/it][ATokenizing tekst text into sentances: 134it [04:31,  1.73s/it]


reducing sentences to a single doc embeding: 135it [04:30,  1.66s/it][A[A[A

embedding all sents with BERT: 135it [04:30,  1.66s/it][A[A
cleaning sentances: 135it [04:33,  1.66s/it][ATokenizing tekst text into sentances: 135it [04:33,  1.66s/it]


reducing sentences to a single doc embeding: 136it [04:31,  1.44s/it][A[A[A

embedding all sents with BERT: 136it [04:31,  1.44s/it][A[A
cleaning sentances: 136it [04:33,  1.44s/it][ATokenizing tekst text into sentances: 136it [04:33,  1.44s/it]


reducing sentences to a single doc embeding: 137it [04:33,  1.58s/it][A[A[A

embedding all sents with BERT: 137it [04:33,  1.58s/it][A[A
cleaning sentances: 137it [04:35,  1.58s/it][ATokenizing tekst text into sentances: 137it [04:35,  1.58s/it]


reducing sentences to a single doc embeding: 138it [04:35,  1.79s/it][A[A[A

embedding all sents with BERT: 138it [04:35,  1.79s/it][A[A
cleaning sentances: 138it [04:38,  1.79s/it][ATokenizing tekst text into sentances: 138it [04:38,  1.79s/it]


reducing sentences to a single doc embeding: 139it [04:36,  1.59s/it][A[A[A

embedding all sents with BERT: 139it [04:36,  1.59s/it][A[A
cleaning sentances: 139it [04:39,  1.59s/it][ATokenizing tekst text into sentances: 139it [04:39,  1.59s/it]


reducing sentences to a single doc embeding: 140it [04:38,  1.55s/it][A[A[A

embedding all sents with BERT: 140it [04:38,  1.55s/it][A[A
cleaning sentances: 140it [04:40,  1.55s/it][ATokenizing tekst text into sentances: 140it [04:40,  1.55s/it]


reducing sentences to a single doc embeding: 141it [04:39,  1.33s/it][A[A[A

embedding all sents with BERT: 141it [04:39,  1.33s/it][A[A
cleaning sentances: 141it [04:41,  1.33s/it][ATokenizing tekst text into sentances: 141it [04:41,  1.33s/it]


reducing sentences to a single doc embeding: 142it [04:40,  1.24s/it][A[A[A

embedding all sents with BERT: 142it [04:40,  1.24s/it][A[A
cleaning sentances: 142it [04:42,  1.24s/it][ATokenizing tekst text into sentances: 142it [04:42,  1.24s/it]


reducing sentences to a single doc embeding: 143it [04:42,  1.41s/it][A[A[A

embedding all sents with BERT: 143it [04:42,  1.41s/it][A[A
cleaning sentances: 143it [04:44,  1.41s/it][ATokenizing tekst text into sentances: 143it [04:44,  1.41s/it]


reducing sentences to a single doc embeding: 144it [04:43,  1.34s/it][A[A[A

embedding all sents with BERT: 144it [04:43,  1.34s/it][A[A
cleaning sentances: 144it [04:45,  1.34s/it][ATokenizing tekst text into sentances: 144it [04:45,  1.34s/it]


reducing sentences to a single doc embeding: 145it [04:45,  1.60s/it][A[A[A

embedding all sents with BERT: 145it [04:45,  1.60s/it][A[A
cleaning sentances: 145it [04:47,  1.60s/it][ATokenizing tekst text into sentances: 145it [04:47,  1.60s/it]


reducing sentences to a single doc embeding: 146it [04:47,  1.80s/it][A[A[A

embedding all sents with BERT: 146it [04:47,  1.80s/it][A[A
cleaning sentances: 146it [04:50,  1.80s/it][ATokenizing tekst text into sentances: 146it [04:50,  1.80s/it]


reducing sentences to a single doc embeding: 147it [04:49,  1.92s/it][A[A[A

embedding all sents with BERT: 147it [04:49,  1.92s/it][A[A
cleaning sentances: 147it [04:52,  1.92s/it][ATokenizing tekst text into sentances: 147it [04:52,  1.92s/it]


reducing sentences to a single doc embeding: 148it [04:51,  1.89s/it][A[A[A

embedding all sents with BERT: 148it [04:51,  1.89s/it][A[A
cleaning sentances: 148it [04:54,  1.89s/it][ATokenizing tekst text into sentances: 148it [04:54,  1.89s/it]


reducing sentences to a single doc embeding: 149it [04:53,  1.85s/it][A[A[A

embedding all sents with BERT: 149it [04:53,  1.85s/it][A[A
cleaning sentances: 149it [04:55,  1.85s/it][ATokenizing tekst text into sentances: 149it [04:55,  1.85s/it]


reducing sentences to a single doc embeding: 150it [04:54,  1.68s/it][A[A[A

embedding all sents with BERT: 150it [04:54,  1.68s/it][A[A
cleaning sentances: 150it [04:57,  1.68s/it][ATokenizing tekst text into sentances: 150it [04:57,  1.68s/it]


reducing sentences to a single doc embeding: 151it [04:56,  1.69s/it][A[A[A

embedding all sents with BERT: 151it [04:56,  1.69s/it][A[A
cleaning sentances: 151it [04:58,  1.69s/it][ATokenizing tekst text into sentances: 151it [04:58,  1.69s/it]


reducing sentences to a single doc embeding: 152it [04:58,  1.85s/it][A[A[A

embedding all sents with BERT: 152it [04:58,  1.85s/it][A[A
cleaning sentances: 152it [05:01,  1.85s/it][ATokenizing tekst text into sentances: 152it [05:01,  1.85s/it]


reducing sentences to a single doc embeding: 153it [04:59,  1.46s/it][A[A[A

embedding all sents with BERT: 153it [04:59,  1.46s/it][A[A
cleaning sentances: 153it [05:01,  1.46s/it][ATokenizing tekst text into sentances: 153it [05:01,  1.46s/it]


reducing sentences to a single doc embeding: 154it [05:00,  1.30s/it][A[A[A

embedding all sents with BERT: 154it [05:00,  1.30s/it][A[A
cleaning sentances: 154it [05:02,  1.30s/it][ATokenizing tekst text into sentances: 154it [05:02,  1.30s/it]


reducing sentences to a single doc embeding: 155it [05:02,  1.63s/it][A[A[A

embedding all sents with BERT: 155it [05:02,  1.63s/it][A[A
cleaning sentances: 155it [05:04,  1.63s/it][ATokenizing tekst text into sentances: 155it [05:04,  1.63s/it]


reducing sentences to a single doc embeding: 156it [05:04,  1.67s/it][A[A[A

embedding all sents with BERT: 156it [05:04,  1.67s/it][A[A
cleaning sentances: 156it [05:06,  1.67s/it][ATokenizing tekst text into sentances: 156it [05:06,  1.67s/it]


reducing sentences to a single doc embeding: 157it [05:05,  1.55s/it][A[A[A

embedding all sents with BERT: 157it [05:05,  1.55s/it][A[A
cleaning sentances: 157it [05:07,  1.55s/it][ATokenizing tekst text into sentances: 157it [05:07,  1.55s/it]


reducing sentences to a single doc embeding: 158it [05:07,  1.59s/it][A[A[A

embedding all sents with BERT: 158it [05:07,  1.59s/it][A[A
cleaning sentances: 158it [05:09,  1.59s/it][ATokenizing tekst text into sentances: 158it [05:09,  1.59s/it]


reducing sentences to a single doc embeding: 159it [05:08,  1.47s/it][A[A[A

embedding all sents with BERT: 159it [05:08,  1.47s/it][A[A
cleaning sentances: 159it [05:10,  1.47s/it][ATokenizing tekst text into sentances: 159it [05:10,  1.47s/it]


reducing sentences to a single doc embeding: 160it [05:10,  1.52s/it][A[A[A

embedding all sents with BERT: 160it [05:10,  1.52s/it][A[A
cleaning sentances: 160it [05:12,  1.52s/it][ATokenizing tekst text into sentances: 160it [05:12,  1.52s/it]


reducing sentences to a single doc embeding: 161it [05:11,  1.62s/it][A[A[A

embedding all sents with BERT: 161it [05:11,  1.62s/it][A[A
cleaning sentances: 161it [05:14,  1.62s/it][ATokenizing tekst text into sentances: 161it [05:14,  1.62s/it]


reducing sentences to a single doc embeding: 162it [05:13,  1.48s/it][A[A[A

embedding all sents with BERT: 162it [05:13,  1.48s/it][A[A
cleaning sentances: 162it [05:15,  1.48s/it][ATokenizing tekst text into sentances: 162it [05:15,  1.48s/it]


reducing sentences to a single doc embeding: 163it [05:14,  1.56s/it][A[A[A

embedding all sents with BERT: 163it [05:14,  1.56s/it][A[A
cleaning sentances: 163it [05:17,  1.56s/it][ATokenizing tekst text into sentances: 163it [05:17,  1.56s/it]


reducing sentences to a single doc embeding: 164it [05:16,  1.44s/it][A[A[A

embedding all sents with BERT: 164it [05:16,  1.44s/it][A[A
cleaning sentances: 164it [05:18,  1.44s/it][ATokenizing tekst text into sentances: 164it [05:18,  1.44s/it]


reducing sentences to a single doc embeding: 165it [05:17,  1.50s/it][A[A[A

embedding all sents with BERT: 165it [05:17,  1.50s/it][A[A
cleaning sentances: 165it [05:20,  1.50s/it][ATokenizing tekst text into sentances: 165it [05:20,  1.50s/it]


reducing sentences to a single doc embeding: 166it [05:18,  1.30s/it][A[A[A

embedding all sents with BERT: 166it [05:18,  1.30s/it][A[A
cleaning sentances: 166it [05:20,  1.30s/it][ATokenizing tekst text into sentances: 166it [05:20,  1.30s/it]


reducing sentences to a single doc embeding: 167it [05:20,  1.37s/it][A[A[A

embedding all sents with BERT: 167it [05:20,  1.37s/it][A[A
cleaning sentances: 167it [05:22,  1.37s/it][ATokenizing tekst text into sentances: 167it [05:22,  1.37s/it]


reducing sentences to a single doc embeding: 168it [05:25,  2.72s/it][A[A[A

embedding all sents with BERT: 168it [05:25,  2.72s/it][A[A
cleaning sentances: 168it [05:28,  2.72s/it][ATokenizing tekst text into sentances: 168it [05:28,  2.72s/it]


reducing sentences to a single doc embeding: 169it [05:27,  2.30s/it][A[A[A

embedding all sents with BERT: 169it [05:27,  2.30s/it][A[A
cleaning sentances: 169it [05:29,  2.30s/it][ATokenizing tekst text into sentances: 169it [05:29,  2.30s/it]


reducing sentences to a single doc embeding: 170it [05:37,  4.74s/it][A[A[A

embedding all sents with BERT: 170it [05:37,  4.74s/it][A[A
cleaning sentances: 170it [05:39,  4.74s/it][ATokenizing tekst text into sentances: 170it [05:39,  4.74s/it]


reducing sentences to a single doc embeding: 171it [05:40,  4.12s/it][A[A[A

embedding all sents with BERT: 171it [05:40,  4.12s/it][A[A
cleaning sentances: 171it [05:42,  4.12s/it][ATokenizing tekst text into sentances: 171it [05:42,  4.12s/it]


reducing sentences to a single doc embeding: 172it [05:41,  3.32s/it][A[A[A

embedding all sents with BERT: 172it [05:41,  3.32s/it][A[A
cleaning sentances: 172it [05:44,  3.32s/it][ATokenizing tekst text into sentances: 172it [05:44,  3.32s/it]


reducing sentences to a single doc embeding: 173it [05:44,  3.03s/it][A[A[A

embedding all sents with BERT: 173it [05:44,  3.03s/it][A[A
cleaning sentances: 173it [05:46,  3.03s/it][ATokenizing tekst text into sentances: 173it [05:46,  3.03s/it]


reducing sentences to a single doc embeding: 174it [05:45,  2.40s/it][A[A[A

embedding all sents with BERT: 174it [05:45,  2.40s/it][A[A
cleaning sentances: 174it [05:47,  2.40s/it][ATokenizing tekst text into sentances: 174it [05:47,  2.40s/it]


reducing sentences to a single doc embeding: 175it [05:56,  5.14s/it][A[A[A

embedding all sents with BERT: 175it [05:56,  5.14s/it][A[A
cleaning sentances: 175it [05:58,  5.14s/it][ATokenizing tekst text into sentances: 175it [05:58,  5.14s/it]


reducing sentences to a single doc embeding: 176it [05:58,  4.17s/it][A[A[A

embedding all sents with BERT: 176it [05:58,  4.17s/it][A[A
cleaning sentances: 176it [06:00,  4.17s/it][ATokenizing tekst text into sentances: 176it [06:00,  4.17s/it]


reducing sentences to a single doc embeding: 177it [05:59,  3.24s/it][A[A[A

embedding all sents with BERT: 177it [05:59,  3.24s/it][A[A
cleaning sentances: 177it [06:01,  3.24s/it][ATokenizing tekst text into sentances: 177it [06:01,  3.24s/it]


reducing sentences to a single doc embeding: 178it [06:01,  2.90s/it][A[A[A

embedding all sents with BERT: 178it [06:01,  2.90s/it][A[A
cleaning sentances: 178it [06:04,  2.90s/it][ATokenizing tekst text into sentances: 178it [06:04,  2.90s/it]


reducing sentences to a single doc embeding: 179it [06:03,  2.51s/it][A[A[A

embedding all sents with BERT: 179it [06:03,  2.51s/it][A[A
cleaning sentances: 179it [06:05,  2.51s/it][ATokenizing tekst text into sentances: 179it [06:05,  2.51s/it]


reducing sentences to a single doc embeding: 180it [06:04,  2.17s/it][A[A[A

embedding all sents with BERT: 180it [06:04,  2.17s/it][A[A
cleaning sentances: 180it [06:06,  2.17s/it][ATokenizing tekst text into sentances: 180it [06:06,  2.17s/it]


reducing sentences to a single doc embeding: 181it [06:05,  1.92s/it][A[A[A

embedding all sents with BERT: 181it [06:05,  1.92s/it][A[A
cleaning sentances: 181it [06:08,  1.92s/it][ATokenizing tekst text into sentances: 181it [06:08,  1.92s/it]


reducing sentences to a single doc embeding: 182it [06:08,  2.19s/it][A[A[A

embedding all sents with BERT: 182it [06:08,  2.19s/it][A[A
cleaning sentances: 182it [06:11,  2.19s/it][ATokenizing tekst text into sentances: 182it [06:11,  2.19s/it]


reducing sentences to a single doc embeding: 183it [06:10,  1.96s/it][A[A[A

embedding all sents with BERT: 183it [06:10,  1.96s/it][A[A
cleaning sentances: 183it [06:12,  1.96s/it][ATokenizing tekst text into sentances: 183it [06:12,  1.96s/it]


reducing sentences to a single doc embeding: 184it [06:12,  2.06s/it][A[A[A

embedding all sents with BERT: 184it [06:12,  2.06s/it][A[A
cleaning sentances: 184it [06:14,  2.06s/it][ATokenizing tekst text into sentances: 184it [06:14,  2.06s/it]


reducing sentences to a single doc embeding: 185it [06:13,  1.72s/it][A[A[A

embedding all sents with BERT: 185it [06:13,  1.72s/it][A[A
cleaning sentances: 185it [06:15,  1.72s/it][ATokenizing tekst text into sentances: 185it [06:15,  1.72s/it]


reducing sentences to a single doc embeding: 186it [06:15,  1.74s/it][A[A[A

embedding all sents with BERT: 186it [06:15,  1.74s/it][A[A
cleaning sentances: 186it [06:17,  1.74s/it][ATokenizing tekst text into sentances: 186it [06:17,  1.74s/it]


reducing sentences to a single doc embeding: 187it [06:16,  1.69s/it][A[A[A

embedding all sents with BERT: 187it [06:16,  1.69s/it][A[A
cleaning sentances: 187it [06:19,  1.69s/it][ATokenizing tekst text into sentances: 187it [06:19,  1.69s/it]


reducing sentences to a single doc embeding: 188it [06:20,  2.23s/it][A[A[A

embedding all sents with BERT: 188it [06:20,  2.23s/it][A[A
cleaning sentances: 188it [06:22,  2.23s/it][ATokenizing tekst text into sentances: 188it [06:22,  2.23s/it]


reducing sentences to a single doc embeding: 189it [06:30,  4.52s/it][A[A[A

embedding all sents with BERT: 189it [06:30,  4.52s/it][A[A
cleaning sentances: 189it [06:32,  4.52s/it][ATokenizing tekst text into sentances: 189it [06:32,  4.52s/it]


reducing sentences to a single doc embeding: 190it [06:32,  3.76s/it][A[A[A

embedding all sents with BERT: 190it [06:32,  3.76s/it][A[A
cleaning sentances: 190it [06:34,  3.76s/it][ATokenizing tekst text into sentances: 190it [06:34,  3.76s/it]


reducing sentences to a single doc embeding: 191it [06:36,  3.88s/it][A[A[A

embedding all sents with BERT: 191it [06:36,  3.88s/it][A[A
cleaning sentances: 191it [06:38,  3.88s/it][ATokenizing tekst text into sentances: 191it [06:38,  3.88s/it]


reducing sentences to a single doc embeding: 192it [06:46,  5.65s/it][A[A[A

embedding all sents with BERT: 192it [06:46,  5.65s/it][A[A
cleaning sentances: 192it [06:48,  5.65s/it][ATokenizing tekst text into sentances: 192it [06:48,  5.65s/it]


reducing sentences to a single doc embeding: 193it [06:47,  4.51s/it][A[A[A

embedding all sents with BERT: 193it [06:47,  4.51s/it][A[A
cleaning sentances: 193it [06:50,  4.51s/it][ATokenizing tekst text into sentances: 193it [06:50,  4.51s/it]


reducing sentences to a single doc embeding: 194it [06:49,  3.55s/it][A[A[A

embedding all sents with BERT: 194it [06:49,  3.55s/it][A[A
cleaning sentances: 194it [06:51,  3.55s/it][ATokenizing tekst text into sentances: 194it [06:51,  3.55s/it]


reducing sentences to a single doc embeding: 195it [06:50,  2.97s/it][A[A[A

embedding all sents with BERT: 195it [06:50,  2.97s/it][A[A
cleaning sentances: 195it [06:53,  2.97s/it][ATokenizing tekst text into sentances: 195it [06:53,  2.97s/it]


reducing sentences to a single doc embeding: 196it [06:57,  4.04s/it][A[A[A

embedding all sents with BERT: 196it [06:57,  4.04s/it][A[A
cleaning sentances: 196it [06:59,  4.04s/it][ATokenizing tekst text into sentances: 196it [06:59,  4.04s/it]


reducing sentences to a single doc embeding: 197it [06:58,  3.25s/it][A[A[A

embedding all sents with BERT: 197it [06:58,  3.25s/it][A[A
cleaning sentances: 197it [07:01,  3.25s/it][ATokenizing tekst text into sentances: 197it [07:01,  3.25s/it]


reducing sentences to a single doc embeding: 198it [07:06,  4.59s/it][A[A[A

embedding all sents with BERT: 198it [07:06,  4.59s/it][A[A
cleaning sentances: 198it [07:08,  4.59s/it][ATokenizing tekst text into sentances: 198it [07:08,  4.59s/it]


reducing sentences to a single doc embeding: 199it [07:08,  3.92s/it][A[A[A

embedding all sents with BERT: 199it [07:08,  3.92s/it][A[A
cleaning sentances: 199it [07:11,  3.92s/it][ATokenizing tekst text into sentances: 199it [07:11,  3.92s/it]


reducing sentences to a single doc embeding: 200it [07:10,  3.23s/it][A[A[A

embedding all sents with BERT: 200it [07:10,  3.23s/it][A[A
cleaning sentances: 200it [07:12,  3.23s/it][ATokenizing tekst text into sentances: 200it [07:12,  3.23s/it]


reducing sentences to a single doc embeding: 201it [07:11,  2.57s/it][A[A[A

embedding all sents with BERT: 201it [07:11,  2.57s/it][A[A
cleaning sentances: 201it [07:13,  2.57s/it][ATokenizing tekst text into sentances: 201it [07:13,  2.57s/it]


reducing sentences to a single doc embeding: 202it [07:13,  2.44s/it][A[A[A

embedding all sents with BERT: 202it [07:13,  2.44s/it][A[A
cleaning sentances: 202it [07:16,  2.44s/it][ATokenizing tekst text into sentances: 202it [07:16,  2.44s/it]


reducing sentences to a single doc embeding: 203it [07:15,  2.12s/it][A[A[A

embedding all sents with BERT: 203it [07:15,  2.12s/it][A[A
cleaning sentances: 203it [07:17,  2.12s/it][ATokenizing tekst text into sentances: 203it [07:17,  2.12s/it]


reducing sentences to a single doc embeding: 204it [07:17,  2.36s/it][A[A[A

embedding all sents with BERT: 204it [07:17,  2.36s/it][A[A
cleaning sentances: 204it [07:20,  2.36s/it][ATokenizing tekst text into sentances: 204it [07:20,  2.36s/it]


reducing sentences to a single doc embeding: 205it [07:24,  3.65s/it][A[A[A

embedding all sents with BERT: 205it [07:24,  3.65s/it][A[A
cleaning sentances: 205it [07:26,  3.65s/it][ATokenizing tekst text into sentances: 205it [07:26,  3.65s/it]


reducing sentences to a single doc embeding: 206it [07:25,  2.90s/it][A[A[A

embedding all sents with BERT: 206it [07:25,  2.90s/it][A[A
cleaning sentances: 206it [07:28,  2.90s/it][ATokenizing tekst text into sentances: 206it [07:28,  2.90s/it]


reducing sentences to a single doc embeding: 207it [07:34,  4.78s/it][A[A[A

embedding all sents with BERT: 207it [07:34,  4.78s/it][A[A
cleaning sentances: 207it [07:37,  4.78s/it][ATokenizing tekst text into sentances: 207it [07:37,  4.78s/it]


reducing sentences to a single doc embeding: 208it [07:36,  3.72s/it][A[A[A

embedding all sents with BERT: 208it [07:36,  3.72s/it][A[A
cleaning sentances: 208it [07:38,  3.72s/it][ATokenizing tekst text into sentances: 208it [07:38,  3.72s/it]


reducing sentences to a single doc embeding: 209it [07:38,  3.26s/it][A[A[A

embedding all sents with BERT: 209it [07:38,  3.26s/it][A[A
cleaning sentances: 209it [07:40,  3.26s/it][ATokenizing tekst text into sentances: 209it [07:40,  3.26s/it]


reducing sentences to a single doc embeding: 210it [07:40,  2.93s/it][A[A[A

embedding all sents with BERT: 210it [07:40,  2.93s/it][A[A
cleaning sentances: 210it [07:42,  2.93s/it][ATokenizing tekst text into sentances: 210it [07:42,  2.93s/it]


reducing sentences to a single doc embeding: 211it [07:42,  2.66s/it][A[A[A

embedding all sents with BERT: 211it [07:42,  2.66s/it][A[A
cleaning sentances: 211it [07:44,  2.66s/it][ATokenizing tekst text into sentances: 211it [07:44,  2.66s/it]


reducing sentences to a single doc embeding: 212it [07:43,  2.21s/it][A[A[A

embedding all sents with BERT: 212it [07:43,  2.21s/it][A[A
cleaning sentances: 212it [07:46,  2.21s/it][ATokenizing tekst text into sentances: 212it [07:46,  2.21s/it]


reducing sentences to a single doc embeding: 213it [07:44,  1.67s/it][A[A[A

embedding all sents with BERT: 213it [07:44,  1.67s/it][A[A
cleaning sentances: 213it [07:46,  1.67s/it][ATokenizing tekst text into sentances: 213it [07:46,  1.67s/it]


reducing sentences to a single doc embeding: 214it [07:45,  1.61s/it][A[A[A

embedding all sents with BERT: 214it [07:45,  1.61s/it][A[A
cleaning sentances: 214it [07:47,  1.61s/it][ATokenizing tekst text into sentances: 214it [07:47,  1.61s/it]


reducing sentences to a single doc embeding: 215it [07:47,  1.67s/it][A[A[A

embedding all sents with BERT: 215it [07:47,  1.67s/it][A[A
cleaning sentances: 215it [07:49,  1.67s/it][ATokenizing tekst text into sentances: 215it [07:49,  1.67s/it]


reducing sentences to a single doc embeding: 216it [07:49,  1.76s/it][A[A[A

embedding all sents with BERT: 216it [07:49,  1.76s/it][A[A
cleaning sentances: 216it [07:51,  1.76s/it][ATokenizing tekst text into sentances: 216it [07:51,  1.76s/it]


reducing sentences to a single doc embeding: 217it [07:51,  1.91s/it][A[A[A

embedding all sents with BERT: 217it [07:51,  1.91s/it][A[A
cleaning sentances: 217it [07:53,  1.91s/it][ATokenizing tekst text into sentances: 217it [07:53,  1.91s/it]


reducing sentences to a single doc embeding: 218it [07:53,  2.00s/it][A[A[A

embedding all sents with BERT: 218it [07:53,  2.00s/it][A[A
cleaning sentances: 218it [07:56,  2.00s/it][ATokenizing tekst text into sentances: 218it [07:56,  2.00s/it]


reducing sentences to a single doc embeding: 219it [07:54,  1.69s/it][A[A[A

embedding all sents with BERT: 219it [07:54,  1.69s/it][A[A
cleaning sentances: 219it [07:57,  1.69s/it][ATokenizing tekst text into sentances: 219it [07:57,  1.69s/it]


reducing sentences to a single doc embeding: 220it [07:56,  1.54s/it][A[A[A

embedding all sents with BERT: 220it [07:56,  1.54s/it][A[A
cleaning sentances: 220it [07:58,  1.54s/it][ATokenizing tekst text into sentances: 220it [07:58,  1.54s/it]


reducing sentences to a single doc embeding: 221it [07:56,  1.36s/it][A[A[A

embedding all sents with BERT: 221it [07:56,  1.36s/it][A[A
cleaning sentances: 221it [07:59,  1.36s/it][ATokenizing tekst text into sentances: 221it [07:59,  1.36s/it]


reducing sentences to a single doc embeding: 222it [07:58,  1.47s/it][A[A[A

embedding all sents with BERT: 222it [07:58,  1.47s/it][A[A
cleaning sentances: 222it [08:01,  1.47s/it][ATokenizing tekst text into sentances: 222it [08:01,  1.47s/it]


reducing sentences to a single doc embeding: 223it [08:01,  1.94s/it][A[A[A

embedding all sents with BERT: 223it [08:01,  1.94s/it][A[A
cleaning sentances: 223it [08:04,  1.94s/it][ATokenizing tekst text into sentances: 223it [08:04,  1.94s/it]


reducing sentences to a single doc embeding: 224it [08:02,  1.74s/it][A[A[A

embedding all sents with BERT: 224it [08:02,  1.74s/it][A[A
cleaning sentances: 224it [08:05,  1.74s/it][ATokenizing tekst text into sentances: 224it [08:05,  1.74s/it]


reducing sentences to a single doc embeding: 225it [08:04,  1.63s/it][A[A[A

embedding all sents with BERT: 225it [08:04,  1.63s/it][A[A
cleaning sentances: 225it [08:06,  1.63s/it][ATokenizing tekst text into sentances: 225it [08:06,  1.63s/it]


reducing sentences to a single doc embeding: 226it [08:05,  1.49s/it][A[A[A

embedding all sents with BERT: 226it [08:05,  1.49s/it][A[A
cleaning sentances: 226it [08:07,  1.49s/it][ATokenizing tekst text into sentances: 226it [08:07,  1.49s/it]


reducing sentences to a single doc embeding: 227it [08:08,  1.81s/it][A[A[A

embedding all sents with BERT: 227it [08:08,  1.81s/it][A[A
cleaning sentances: 227it [08:10,  1.81s/it][ATokenizing tekst text into sentances: 227it [08:10,  1.81s/it]


reducing sentences to a single doc embeding: 228it [08:10,  1.88s/it][A[A[A

embedding all sents with BERT: 228it [08:10,  1.88s/it][A[A
cleaning sentances: 228it [08:12,  1.88s/it][ATokenizing tekst text into sentances: 228it [08:12,  1.88s/it]


reducing sentences to a single doc embeding: 229it [08:11,  1.75s/it][A[A[A

embedding all sents with BERT: 229it [08:11,  1.75s/it][A[A
cleaning sentances: 229it [08:13,  1.75s/it][ATokenizing tekst text into sentances: 229it [08:13,  1.75s/it]


reducing sentences to a single doc embeding: 230it [08:23,  4.79s/it][A[A[A

embedding all sents with BERT: 230it [08:23,  4.79s/it][A[A
cleaning sentances: 230it [08:25,  4.79s/it][ATokenizing tekst text into sentances: 230it [08:25,  4.79s/it]


reducing sentences to a single doc embeding: 231it [08:25,  3.89s/it][A[A[A

embedding all sents with BERT: 231it [08:25,  3.89s/it][A[A
cleaning sentances: 231it [08:27,  3.89s/it][ATokenizing tekst text into sentances: 231it [08:27,  3.89s/it]


reducing sentences to a single doc embeding: 232it [08:26,  3.13s/it][A[A[A

embedding all sents with BERT: 232it [08:26,  3.13s/it][A[A
cleaning sentances: 232it [08:28,  3.13s/it][ATokenizing tekst text into sentances: 232it [08:28,  3.13s/it]


reducing sentences to a single doc embeding: 233it [08:28,  2.67s/it][A[A[A

embedding all sents with BERT: 233it [08:28,  2.67s/it][A[A
cleaning sentances: 233it [08:30,  2.67s/it][ATokenizing tekst text into sentances: 233it [08:30,  2.67s/it]


reducing sentences to a single doc embeding: 234it [08:32,  3.12s/it][A[A[A

embedding all sents with BERT: 234it [08:32,  3.12s/it][A[A
cleaning sentances: 234it [08:34,  3.12s/it][ATokenizing tekst text into sentances: 234it [08:34,  3.12s/it]


reducing sentences to a single doc embeding: 235it [08:32,  2.35s/it][A[A[A

embedding all sents with BERT: 235it [08:32,  2.35s/it][A[A
cleaning sentances: 235it [08:35,  2.35s/it][ATokenizing tekst text into sentances: 235it [08:35,  2.35s/it]


reducing sentences to a single doc embeding: 236it [08:34,  2.01s/it][A[A[A

embedding all sents with BERT: 236it [08:34,  2.01s/it][A[A
cleaning sentances: 236it [08:36,  2.01s/it][ATokenizing tekst text into sentances: 236it [08:36,  2.01s/it]


reducing sentences to a single doc embeding: 237it [08:35,  1.85s/it][A[A[A

embedding all sents with BERT: 237it [08:35,  1.85s/it][A[A
cleaning sentances: 237it [08:37,  1.85s/it][ATokenizing tekst text into sentances: 237it [08:37,  1.85s/it]


reducing sentences to a single doc embeding: 238it [08:36,  1.69s/it][A[A[A

embedding all sents with BERT: 238it [08:36,  1.69s/it][A[A
cleaning sentances: 238it [08:39,  1.69s/it][ATokenizing tekst text into sentances: 238it [08:39,  1.69s/it]


reducing sentences to a single doc embeding: 239it [08:37,  1.49s/it][A[A[A

embedding all sents with BERT: 239it [08:37,  1.49s/it][A[A
cleaning sentances: 239it [08:40,  1.49s/it][ATokenizing tekst text into sentances: 239it [08:40,  1.49s/it]


reducing sentences to a single doc embeding: 240it [08:39,  1.63s/it][A[A[A

embedding all sents with BERT: 240it [08:39,  1.63s/it][A[A
cleaning sentances: 240it [08:42,  1.63s/it][ATokenizing tekst text into sentances: 240it [08:42,  1.63s/it]


reducing sentences to a single doc embeding: 241it [08:40,  1.43s/it][A[A[A

embedding all sents with BERT: 241it [08:40,  1.43s/it][A[A
cleaning sentances: 241it [08:43,  1.43s/it][ATokenizing tekst text into sentances: 241it [08:43,  1.43s/it]


reducing sentences to a single doc embeding: 242it [08:43,  1.75s/it][A[A[A

embedding all sents with BERT: 242it [08:43,  1.75s/it][A[A
cleaning sentances: 242it [08:45,  1.75s/it][ATokenizing tekst text into sentances: 242it [08:45,  1.75s/it]


reducing sentences to a single doc embeding: 243it [08:45,  1.88s/it][A[A[A

embedding all sents with BERT: 243it [08:45,  1.88s/it][A[A
cleaning sentances: 243it [08:47,  1.88s/it][ATokenizing tekst text into sentances: 243it [08:47,  1.88s/it]


reducing sentences to a single doc embeding: 244it [08:47,  1.79s/it][A[A[A

embedding all sents with BERT: 244it [08:47,  1.79s/it][A[A
cleaning sentances: 244it [08:49,  1.79s/it][ATokenizing tekst text into sentances: 244it [08:49,  1.79s/it]


reducing sentences to a single doc embeding: 245it [08:48,  1.60s/it][A[A[A

embedding all sents with BERT: 245it [08:48,  1.60s/it][A[A
cleaning sentances: 245it [08:50,  1.60s/it][ATokenizing tekst text into sentances: 245it [08:50,  1.60s/it]


reducing sentences to a single doc embeding: 246it [08:50,  1.65s/it][A[A[A

embedding all sents with BERT: 246it [08:50,  1.65s/it][A[A
cleaning sentances: 246it [08:52,  1.65s/it][ATokenizing tekst text into sentances: 246it [08:52,  1.65s/it]


reducing sentences to a single doc embeding: 247it [09:06,  5.97s/it][A[A[A

embedding all sents with BERT: 247it [09:06,  5.97s/it][A[A
cleaning sentances: 247it [09:08,  5.97s/it][ATokenizing tekst text into sentances: 247it [09:08,  5.97s/it]


reducing sentences to a single doc embeding: 248it [09:07,  4.60s/it][A[A[A

embedding all sents with BERT: 248it [09:07,  4.60s/it][A[A
cleaning sentances: 248it [09:09,  4.60s/it][ATokenizing tekst text into sentances: 248it [09:09,  4.60s/it]


reducing sentences to a single doc embeding: 249it [09:20,  7.27s/it][A[A[A

embedding all sents with BERT: 249it [09:20,  7.27s/it][A[A
cleaning sentances: 249it [09:23,  7.27s/it][ATokenizing tekst text into sentances: 249it [09:23,  7.27s/it]


reducing sentences to a single doc embeding: 250it [09:23,  5.71s/it][A[A[A

embedding all sents with BERT: 250it [09:23,  5.71s/it][A[A
cleaning sentances: 250it [09:25,  5.71s/it][ATokenizing tekst text into sentances: 250it [09:25,  5.71s/it]


reducing sentences to a single doc embeding: 251it [09:25,  4.65s/it][A[A[A

embedding all sents with BERT: 251it [09:25,  4.65s/it][A[A
cleaning sentances: 251it [09:27,  4.65s/it][ATokenizing tekst text into sentances: 251it [09:27,  4.65s/it]


reducing sentences to a single doc embeding: 252it [09:28,  4.25s/it][A[A[A

embedding all sents with BERT: 252it [09:28,  4.25s/it][A[A
cleaning sentances: 252it [09:30,  4.25s/it][ATokenizing tekst text into sentances: 252it [09:30,  4.25s/it]


reducing sentences to a single doc embeding: 253it [09:29,  3.37s/it][A[A[A

embedding all sents with BERT: 253it [09:29,  3.37s/it][A[A
cleaning sentances: 253it [09:32,  3.37s/it][ATokenizing tekst text into sentances: 253it [09:32,  3.37s/it]


reducing sentences to a single doc embeding: 254it [09:31,  2.79s/it][A[A[A

embedding all sents with BERT: 254it [09:31,  2.79s/it][A[A
cleaning sentances: 254it [09:33,  2.79s/it][ATokenizing tekst text into sentances: 254it [09:33,  2.79s/it]


reducing sentences to a single doc embeding: 255it [09:32,  2.38s/it][A[A[A

embedding all sents with BERT: 255it [09:32,  2.38s/it][A[A
cleaning sentances: 255it [09:35,  2.38s/it][ATokenizing tekst text into sentances: 255it [09:35,  2.38s/it]


reducing sentences to a single doc embeding: 256it [09:36,  2.71s/it][A[A[A

embedding all sents with BERT: 256it [09:36,  2.71s/it][A[A
cleaning sentances: 256it [09:38,  2.71s/it][ATokenizing tekst text into sentances: 256it [09:38,  2.71s/it]


reducing sentences to a single doc embeding: 257it [09:38,  2.60s/it][A[A[A

embedding all sents with BERT: 257it [09:38,  2.60s/it][A[A
cleaning sentances: 257it [09:40,  2.60s/it][ATokenizing tekst text into sentances: 257it [09:40,  2.60s/it]


reducing sentences to a single doc embeding: 258it [09:41,  2.57s/it][A[A[A

embedding all sents with BERT: 258it [09:41,  2.57s/it][A[A
cleaning sentances: 258it [09:43,  2.57s/it][ATokenizing tekst text into sentances: 258it [09:43,  2.57s/it]


reducing sentences to a single doc embeding: 259it [09:42,  2.36s/it][A[A[A

embedding all sents with BERT: 259it [09:42,  2.36s/it][A[A
cleaning sentances: 259it [09:45,  2.36s/it][ATokenizing tekst text into sentances: 259it [09:45,  2.36s/it]


reducing sentences to a single doc embeding: 260it [09:44,  2.13s/it][A[A[A

embedding all sents with BERT: 260it [09:44,  2.13s/it][A[A
cleaning sentances: 260it [09:46,  2.13s/it][ATokenizing tekst text into sentances: 260it [09:46,  2.13s/it]


reducing sentences to a single doc embeding: 261it [09:46,  1.98s/it][A[A[A

embedding all sents with BERT: 261it [09:46,  1.98s/it][A[A
cleaning sentances: 261it [09:48,  1.98s/it][ATokenizing tekst text into sentances: 261it [09:48,  1.98s/it]


reducing sentences to a single doc embeding: 262it [09:50,  2.77s/it][A[A[A

embedding all sents with BERT: 262it [09:50,  2.77s/it][A[A
cleaning sentances: 262it [09:53,  2.77s/it][ATokenizing tekst text into sentances: 262it [09:53,  2.77s/it]


reducing sentences to a single doc embeding: 263it [09:54,  3.04s/it][A[A[A

embedding all sents with BERT: 263it [09:54,  3.04s/it][A[A
cleaning sentances: 263it [09:56,  3.04s/it][ATokenizing tekst text into sentances: 263it [09:56,  3.04s/it]


reducing sentences to a single doc embeding: 264it [09:55,  2.32s/it][A[A[A

embedding all sents with BERT: 264it [09:55,  2.32s/it][A[A
cleaning sentances: 264it [09:57,  2.32s/it][ATokenizing tekst text into sentances: 264it [09:57,  2.32s/it]


reducing sentences to a single doc embeding: 265it [09:57,  2.33s/it][A[A[A

embedding all sents with BERT: 265it [09:57,  2.33s/it][A[A
cleaning sentances: 265it [09:59,  2.33s/it][ATokenizing tekst text into sentances: 265it [09:59,  2.33s/it]


reducing sentences to a single doc embeding: 266it [10:03,  3.45s/it][A[A[A

embedding all sents with BERT: 266it [10:03,  3.45s/it][A[A
cleaning sentances: 266it [10:05,  3.45s/it][ATokenizing tekst text into sentances: 266it [10:05,  3.45s/it]


reducing sentences to a single doc embeding: 267it [10:03,  2.51s/it][A[A[A

embedding all sents with BERT: 267it [10:03,  2.51s/it][A[A
cleaning sentances: 267it [10:06,  2.51s/it][ATokenizing tekst text into sentances: 267it [10:06,  2.51s/it]


reducing sentences to a single doc embeding: 268it [10:04,  2.05s/it][A[A[A

embedding all sents with BERT: 268it [10:04,  2.05s/it][A[A
cleaning sentances: 268it [10:07,  2.05s/it][ATokenizing tekst text into sentances: 268it [10:07,  2.05s/it]


reducing sentences to a single doc embeding: 269it [10:06,  1.98s/it][A[A[A

embedding all sents with BERT: 269it [10:06,  1.98s/it][A[A
cleaning sentances: 269it [10:08,  1.98s/it][ATokenizing tekst text into sentances: 269it [10:08,  1.98s/it]


reducing sentences to a single doc embeding: 270it [10:14,  3.84s/it][A[A[A

embedding all sents with BERT: 270it [10:14,  3.84s/it][A[A
cleaning sentances: 270it [10:17,  3.84s/it][ATokenizing tekst text into sentances: 270it [10:17,  3.84s/it]


reducing sentences to a single doc embeding: 271it [10:16,  3.08s/it][A[A[A

embedding all sents with BERT: 271it [10:16,  3.08s/it][A[A
cleaning sentances: 271it [10:18,  3.08s/it][ATokenizing tekst text into sentances: 271it [10:18,  3.08s/it]


reducing sentences to a single doc embeding: 272it [10:18,  2.77s/it][A[A[A

embedding all sents with BERT: 272it [10:18,  2.77s/it][A[A
cleaning sentances: 272it [10:20,  2.77s/it][ATokenizing tekst text into sentances: 272it [10:20,  2.77s/it]


reducing sentences to a single doc embeding: 273it [10:20,  2.76s/it][A[A[A

embedding all sents with BERT: 273it [10:20,  2.76s/it][A[A
cleaning sentances: 273it [10:23,  2.76s/it][ATokenizing tekst text into sentances: 273it [10:23,  2.76s/it]


reducing sentences to a single doc embeding: 274it [10:21,  2.20s/it][A[A[A

embedding all sents with BERT: 274it [10:21,  2.20s/it][A[A
cleaning sentances: 274it [10:24,  2.20s/it][ATokenizing tekst text into sentances: 274it [10:24,  2.20s/it]


reducing sentences to a single doc embeding: 275it [11:13, 17.04s/it][A[A[A

embedding all sents with BERT: 275it [11:13, 17.04s/it][A[A
cleaning sentances: 275it [11:15, 17.04s/it][ATokenizing tekst text into sentances: 275it [11:15, 17.04s/it]


reducing sentences to a single doc embeding: 276it [11:23, 14.81s/it][A[A[A

embedding all sents with BERT: 276it [11:23, 14.81s/it][A[A
cleaning sentances: 276it [11:25, 14.81s/it][ATokenizing tekst text into sentances: 276it [11:25, 14.81s/it]


reducing sentences to a single doc embeding: 277it [11:24, 10.84s/it][A[A[A

embedding all sents with BERT: 277it [11:24, 10.84s/it][A[A
cleaning sentances: 277it [11:26, 10.84s/it][ATokenizing tekst text into sentances: 277it [11:26, 10.84s/it]


reducing sentences to a single doc embeding: 278it [11:26,  8.03s/it][A[A[A

embedding all sents with BERT: 278it [11:26,  8.03s/it][A[A
cleaning sentances: 278it [11:28,  8.03s/it][ATokenizing tekst text into sentances: 278it [11:28,  8.03s/it]


reducing sentences to a single doc embeding: 279it [11:28,  6.29s/it][A[A[A

embedding all sents with BERT: 279it [11:28,  6.29s/it][A[A
cleaning sentances: 279it [11:30,  6.29s/it][ATokenizing tekst text into sentances: 279it [11:30,  6.29s/it]


reducing sentences to a single doc embeding: 280it [11:29,  4.84s/it][A[A[A

embedding all sents with BERT: 280it [11:29,  4.84s/it][A[A
cleaning sentances: 280it [11:32,  4.84s/it][ATokenizing tekst text into sentances: 280it [11:32,  4.84s/it]


reducing sentences to a single doc embeding: 281it [11:31,  3.98s/it][A[A[A

embedding all sents with BERT: 281it [11:31,  3.98s/it][A[A
cleaning sentances: 281it [11:34,  3.98s/it][ATokenizing tekst text into sentances: 281it [11:34,  3.98s/it]


reducing sentences to a single doc embeding: 282it [11:33,  3.18s/it][A[A[A

embedding all sents with BERT: 282it [11:33,  3.18s/it][A[A
cleaning sentances: 282it [11:35,  3.18s/it][ATokenizing tekst text into sentances: 282it [11:35,  3.18s/it]


reducing sentences to a single doc embeding: 283it [11:35,  2.97s/it][A[A[A

embedding all sents with BERT: 283it [11:35,  2.97s/it][A[A
cleaning sentances: 283it [11:37,  2.97s/it][ATokenizing tekst text into sentances: 283it [11:37,  2.97s/it]


reducing sentences to a single doc embeding: 284it [11:37,  2.64s/it][A[A[A

embedding all sents with BERT: 284it [11:37,  2.64s/it][A[A
cleaning sentances: 284it [11:39,  2.64s/it][ATokenizing tekst text into sentances: 284it [11:39,  2.64s/it]


reducing sentences to a single doc embeding: 285it [11:38,  2.23s/it][A[A[A

embedding all sents with BERT: 285it [11:38,  2.23s/it][A[A
cleaning sentances: 285it [11:41,  2.23s/it][ATokenizing tekst text into sentances: 285it [11:41,  2.23s/it]


reducing sentences to a single doc embeding: 286it [11:46,  3.99s/it][A[A[A

embedding all sents with BERT: 286it [11:46,  3.99s/it][A[A
cleaning sentances: 286it [11:49,  3.99s/it][ATokenizing tekst text into sentances: 286it [11:49,  3.99s/it]


reducing sentences to a single doc embeding: 287it [11:48,  3.28s/it][A[A[A

embedding all sents with BERT: 287it [11:48,  3.28s/it][A[A
cleaning sentances: 287it [11:50,  3.28s/it][ATokenizing tekst text into sentances: 287it [11:50,  3.28s/it]


reducing sentences to a single doc embeding: 288it [11:50,  2.84s/it][A[A[A

embedding all sents with BERT: 288it [11:50,  2.84s/it][A[A
cleaning sentances: 288it [11:52,  2.84s/it][ATokenizing tekst text into sentances: 288it [11:52,  2.84s/it]


reducing sentences to a single doc embeding: 289it [11:51,  2.40s/it][A[A[A

embedding all sents with BERT: 289it [11:51,  2.40s/it][A[A
cleaning sentances: 289it [11:53,  2.40s/it][ATokenizing tekst text into sentances: 289it [11:53,  2.40s/it]


reducing sentences to a single doc embeding: 290it [11:52,  2.00s/it][A[A[A

embedding all sents with BERT: 290it [11:52,  2.00s/it][A[A
cleaning sentances: 290it [11:54,  2.00s/it][ATokenizing tekst text into sentances: 290it [11:54,  2.00s/it]


reducing sentences to a single doc embeding: 291it [11:54,  1.93s/it][A[A[A

embedding all sents with BERT: 291it [11:54,  1.93s/it][A[A
cleaning sentances: 291it [11:56,  1.93s/it][ATokenizing tekst text into sentances: 291it [11:56,  1.93s/it]


reducing sentences to a single doc embeding: 292it [11:55,  1.79s/it][A[A[A

embedding all sents with BERT: 292it [11:55,  1.79s/it][A[A
cleaning sentances: 292it [11:58,  1.79s/it][ATokenizing tekst text into sentances: 292it [11:58,  1.79s/it]


reducing sentences to a single doc embeding: 293it [11:57,  1.74s/it][A[A[A

embedding all sents with BERT: 293it [11:57,  1.74s/it][A[A
cleaning sentances: 293it [11:59,  1.74s/it][ATokenizing tekst text into sentances: 293it [11:59,  1.74s/it]


reducing sentences to a single doc embeding: 294it [11:59,  1.72s/it][A[A[A

embedding all sents with BERT: 294it [11:59,  1.72s/it][A[A
cleaning sentances: 294it [12:01,  1.72s/it][ATokenizing tekst text into sentances: 294it [12:01,  1.72s/it]


reducing sentences to a single doc embeding: 295it [12:00,  1.63s/it][A[A[A

embedding all sents with BERT: 295it [12:00,  1.63s/it][A[A
cleaning sentances: 295it [12:02,  1.63s/it][ATokenizing tekst text into sentances: 295it [12:02,  1.63s/it]


reducing sentences to a single doc embeding: 296it [12:03,  1.88s/it][A[A[A

embedding all sents with BERT: 296it [12:03,  1.88s/it][A[A
cleaning sentances: 296it [12:05,  1.88s/it][ATokenizing tekst text into sentances: 296it [12:05,  1.88s/it]


reducing sentences to a single doc embeding: 297it [12:12,  4.16s/it][A[A[A

embedding all sents with BERT: 297it [12:12,  4.16s/it][A[A
cleaning sentances: 297it [12:14,  4.16s/it][ATokenizing tekst text into sentances: 297it [12:14,  4.16s/it]


reducing sentences to a single doc embeding: 298it [12:20,  5.42s/it][A[A[A

embedding all sents with BERT: 298it [12:20,  5.42s/it][A[A
cleaning sentances: 298it [12:23,  5.42s/it][ATokenizing tekst text into sentances: 298it [12:23,  5.42s/it]


reducing sentences to a single doc embeding: 299it [12:37,  8.71s/it][A[A[A

embedding all sents with BERT: 299it [12:37,  8.71s/it][A[A
cleaning sentances: 299it [12:39,  8.71s/it][ATokenizing tekst text into sentances: 299it [12:39,  8.71s/it]


reducing sentences to a single doc embeding: 300it [12:38,  6.58s/it][A[A[A

embedding all sents with BERT: 300it [12:38,  6.58s/it][A[A
cleaning sentances: 300it [12:41,  6.58s/it][ATokenizing tekst text into sentances: 300it [12:41,  6.58s/it]


reducing sentences to a single doc embeding: 301it [12:39,  4.69s/it][A[A[A

embedding all sents with BERT: 301it [12:39,  4.69s/it][A[A
cleaning sentances: 301it [12:41,  4.69s/it][ATokenizing tekst text into sentances: 301it [12:41,  4.69s/it]


reducing sentences to a single doc embeding: 302it [12:39,  3.35s/it][A[A[A

embedding all sents with BERT: 302it [12:39,  3.35s/it][A[A
cleaning sentances: 302it [12:41,  3.35s/it][ATokenizing tekst text into sentances: 302it [12:41,  3.35s/it]


reducing sentences to a single doc embeding: 303it [12:40,  2.69s/it][A[A[A

embedding all sents with BERT: 303it [12:40,  2.69s/it][A[A
cleaning sentances: 303it [12:42,  2.69s/it][ATokenizing tekst text into sentances: 303it [12:42,  2.69s/it]


reducing sentences to a single doc embeding: 304it [12:40,  1.94s/it][A[A[A

embedding all sents with BERT: 304it [12:40,  1.94s/it][A[A
cleaning sentances: 304it [12:43,  1.94s/it][ATokenizing tekst text into sentances: 304it [12:43,  1.94s/it]


reducing sentences to a single doc embeding: 305it [12:42,  1.92s/it][A[A[A

embedding all sents with BERT: 305it [12:42,  1.92s/it][A[A
cleaning sentances: 305it [12:44,  1.92s/it][ATokenizing tekst text into sentances: 305it [12:44,  1.92s/it]


reducing sentences to a single doc embeding: 306it [12:43,  1.57s/it][A[A[A

embedding all sents with BERT: 306it [12:43,  1.57s/it][A[A
cleaning sentances: 306it [12:45,  1.57s/it][ATokenizing tekst text into sentances: 306it [12:45,  1.57s/it]


reducing sentences to a single doc embeding: 307it [12:53,  4.17s/it][A[A[A

embedding all sents with BERT: 307it [12:53,  4.17s/it][A[A
cleaning sentances: 307it [12:55,  4.17s/it][ATokenizing tekst text into sentances: 307it [12:55,  4.17s/it]


reducing sentences to a single doc embeding: 308it [12:56,  3.79s/it][A[A[A

embedding all sents with BERT: 308it [12:56,  3.79s/it][A[A
cleaning sentances: 308it [12:58,  3.79s/it][ATokenizing tekst text into sentances: 308it [12:58,  3.79s/it]


reducing sentences to a single doc embeding: 309it [12:56,  2.71s/it][A[A[A

embedding all sents with BERT: 309it [12:56,  2.71s/it][A[A
cleaning sentances: 309it [12:59,  2.71s/it][ATokenizing tekst text into sentances: 309it [12:59,  2.71s/it]


reducing sentences to a single doc embeding: 310it [13:12,  6.75s/it][A[A[A

embedding all sents with BERT: 310it [13:12,  6.75s/it][A[A
cleaning sentances: 310it [13:15,  6.75s/it][ATokenizing tekst text into sentances: 310it [13:15,  6.75s/it]


reducing sentences to a single doc embeding: 311it [13:36, 11.95s/it][A[A[A

embedding all sents with BERT: 311it [13:36, 11.95s/it][A[A
cleaning sentances: 311it [13:39, 11.95s/it][ATokenizing tekst text into sentances: 311it [13:39, 11.95s/it]


reducing sentences to a single doc embeding: 312it [13:57, 14.62s/it][A[A[A

embedding all sents with BERT: 312it [13:57, 14.62s/it][A[A
cleaning sentances: 312it [14:00, 14.62s/it][ATokenizing tekst text into sentances: 312it [14:00, 14.62s/it]


reducing sentences to a single doc embeding: 313it [14:05, 12.56s/it][A[A[A

embedding all sents with BERT: 313it [14:05, 12.56s/it][A[A
cleaning sentances: 313it [14:07, 12.56s/it][ATokenizing tekst text into sentances: 313it [14:07, 12.56s/it]


reducing sentences to a single doc embeding: 314it [14:35, 17.87s/it][A[A[A

embedding all sents with BERT: 314it [14:35, 17.87s/it][A[A
cleaning sentances: 314it [14:38, 17.87s/it][ATokenizing tekst text into sentances: 314it [14:38, 17.87s/it]


reducing sentences to a single doc embeding: 315it [15:21, 26.18s/it][A[A[A

embedding all sents with BERT: 315it [15:21, 26.18s/it][A[A
cleaning sentances: 315it [15:23, 26.18s/it][ATokenizing tekst text into sentances: 315it [15:23, 26.18s/it]


reducing sentences to a single doc embeding: 316it [15:25, 19.54s/it][A[A[A

embedding all sents with BERT: 316it [15:25, 19.54s/it][A[A
cleaning sentances: 316it [15:27, 19.54s/it][ATokenizing tekst text into sentances: 316it [15:27, 19.54s/it]


reducing sentences to a single doc embeding: 317it [15:38, 17.68s/it][A[A[A

embedding all sents with BERT: 317it [15:38, 17.68s/it][A[A
cleaning sentances: 317it [15:41, 17.68s/it][ATokenizing tekst text into sentances: 317it [15:41, 17.68s/it]


reducing sentences to a single doc embeding: 318it [15:39, 12.75s/it][A[A[A

embedding all sents with BERT: 318it [15:39, 12.75s/it][A[A
cleaning sentances: 318it [15:42, 12.75s/it][ATokenizing tekst text into sentances: 318it [15:42, 12.75s/it]


reducing sentences to a single doc embeding: 319it [15:40,  9.06s/it][A[A[A

embedding all sents with BERT: 319it [15:40,  9.06s/it][A[A
cleaning sentances: 319it [15:42,  9.06s/it][ATokenizing tekst text into sentances: 319it [15:42,  9.06s/it]


reducing sentences to a single doc embeding: 320it [15:46,  8.18s/it][A[A[A

embedding all sents with BERT: 320it [15:46,  8.18s/it][A[A
cleaning sentances: 320it [15:48,  8.18s/it][ATokenizing tekst text into sentances: 320it [15:48,  8.18s/it]


reducing sentences to a single doc embeding: 321it [15:47,  5.87s/it][A[A[A

embedding all sents with BERT: 321it [15:47,  5.87s/it][A[A
cleaning sentances: 321it [15:49,  5.87s/it][ATokenizing tekst text into sentances: 321it [15:49,  5.87s/it]


reducing sentences to a single doc embeding: 322it [15:47,  4.34s/it][A[A[A

embedding all sents with BERT: 322it [15:47,  4.34s/it][A[A
cleaning sentances: 322it [15:50,  4.34s/it][ATokenizing tekst text into sentances: 322it [15:50,  4.34s/it]


reducing sentences to a single doc embeding: 323it [15:49,  3.46s/it][A[A[A

embedding all sents with BERT: 323it [15:49,  3.46s/it][A[A
cleaning sentances: 323it [15:51,  3.46s/it][ATokenizing tekst text into sentances: 323it [15:51,  3.46s/it]


reducing sentences to a single doc embeding: 324it [15:52,  3.31s/it][A[A[A

embedding all sents with BERT: 324it [15:52,  3.31s/it][A[A
cleaning sentances: 324it [15:54,  3.31s/it][ATokenizing tekst text into sentances: 324it [15:54,  3.31s/it]


reducing sentences to a single doc embeding: 325it [15:55,  3.21s/it][A[A[A

embedding all sents with BERT: 325it [15:55,  3.21s/it][A[A
cleaning sentances: 325it [15:57,  3.21s/it][ATokenizing tekst text into sentances: 325it [15:57,  3.21s/it]


reducing sentences to a single doc embeding: 326it [16:10,  6.83s/it][A[A[A

embedding all sents with BERT: 326it [16:10,  6.83s/it][A[A
cleaning sentances: 326it [16:12,  6.83s/it][ATokenizing tekst text into sentances: 326it [16:12,  6.83s/it]


reducing sentences to a single doc embeding: 327it [16:13,  5.76s/it][A[A[A

embedding all sents with BERT: 327it [16:13,  5.76s/it][A[A
cleaning sentances: 327it [16:16,  5.76s/it][ATokenizing tekst text into sentances: 327it [16:16,  5.76s/it]


reducing sentences to a single doc embeding: 328it [16:16,  4.86s/it][A[A[A

embedding all sents with BERT: 328it [16:16,  4.86s/it][A[A
cleaning sentances: 328it [16:18,  4.86s/it][ATokenizing tekst text into sentances: 328it [16:18,  4.86s/it]


reducing sentences to a single doc embeding: 329it [16:17,  3.78s/it][A[A[A

embedding all sents with BERT: 329it [16:17,  3.78s/it][A[A
cleaning sentances: 329it [16:20,  3.78s/it][ATokenizing tekst text into sentances: 329it [16:20,  3.78s/it]


reducing sentences to a single doc embeding: 330it [16:18,  2.87s/it][A[A[A

embedding all sents with BERT: 330it [16:18,  2.87s/it][A[A
cleaning sentances: 330it [16:20,  2.87s/it][ATokenizing tekst text into sentances: 330it [16:20,  2.87s/it]


reducing sentences to a single doc embeding: 331it [16:20,  2.47s/it][A[A[A

embedding all sents with BERT: 331it [16:20,  2.47s/it][A[A
cleaning sentances: 331it [16:22,  2.47s/it][ATokenizing tekst text into sentances: 331it [16:22,  2.47s/it]


reducing sentences to a single doc embeding: 332it [16:21,  2.32s/it][A[A[A

embedding all sents with BERT: 332it [16:21,  2.32s/it][A[A
cleaning sentances: 332it [16:24,  2.32s/it][ATokenizing tekst text into sentances: 332it [16:24,  2.32s/it]


reducing sentences to a single doc embeding: 333it [16:25,  2.71s/it][A[A[A

embedding all sents with BERT: 333it [16:25,  2.71s/it][A[A
cleaning sentances: 333it [16:27,  2.71s/it][ATokenizing tekst text into sentances: 333it [16:27,  2.71s/it]


reducing sentences to a single doc embeding: 334it [16:35,  4.83s/it][A[A[A

embedding all sents with BERT: 334it [16:35,  4.83s/it][A[A
cleaning sentances: 334it [16:37,  4.83s/it][ATokenizing tekst text into sentances: 334it [16:37,  4.83s/it]


reducing sentences to a single doc embeding: 335it [16:48,  7.22s/it][A[A[A

embedding all sents with BERT: 335it [16:48,  7.22s/it][A[A
cleaning sentances: 335it [16:50,  7.22s/it][ATokenizing tekst text into sentances: 335it [16:50,  7.22s/it]


reducing sentences to a single doc embeding: 336it [16:48,  5.19s/it][A[A[A

embedding all sents with BERT: 336it [16:48,  5.19s/it][A[A
cleaning sentances: 336it [16:50,  5.19s/it][ATokenizing tekst text into sentances: 336it [16:50,  5.19s/it]


reducing sentences to a single doc embeding: 337it [16:50,  4.27s/it][A[A[A

embedding all sents with BERT: 337it [16:50,  4.27s/it][A[A
cleaning sentances: 337it [16:53,  4.27s/it][ATokenizing tekst text into sentances: 337it [16:53,  4.27s/it]


reducing sentences to a single doc embeding: 338it [16:51,  3.29s/it][A[A[A

embedding all sents with BERT: 338it [16:51,  3.29s/it][A[A
cleaning sentances: 338it [16:54,  3.29s/it][ATokenizing tekst text into sentances: 338it [16:54,  3.29s/it]


reducing sentences to a single doc embeding: 339it [16:53,  2.97s/it][A[A[A

embedding all sents with BERT: 339it [16:53,  2.97s/it][A[A
cleaning sentances: 339it [16:56,  2.97s/it][ATokenizing tekst text into sentances: 339it [16:56,  2.97s/it]


reducing sentences to a single doc embeding: 340it [16:54,  2.16s/it][A[A[A

embedding all sents with BERT: 340it [16:54,  2.16s/it][A[A
cleaning sentances: 340it [16:56,  2.16s/it][ATokenizing tekst text into sentances: 340it [16:56,  2.16s/it]


reducing sentences to a single doc embeding: 341it [16:54,  1.56s/it][A[A[A

embedding all sents with BERT: 341it [16:54,  1.56s/it][A[A
cleaning sentances: 341it [16:56,  1.56s/it][ATokenizing tekst text into sentances: 341it [16:56,  1.56s/it]


reducing sentences to a single doc embeding: 342it [16:56,  1.60s/it][A[A[A

embedding all sents with BERT: 342it [16:56,  1.60s/it][A[A
cleaning sentances: 342it [16:58,  1.60s/it][ATokenizing tekst text into sentances: 342it [16:58,  1.60s/it]


reducing sentences to a single doc embeding: 343it [17:02,  3.11s/it][A[A[A

embedding all sents with BERT: 343it [17:02,  3.11s/it][A[A
cleaning sentances: 343it [17:05,  3.11s/it][ATokenizing tekst text into sentances: 343it [17:05,  3.11s/it]


reducing sentences to a single doc embeding: 344it [17:04,  2.69s/it][A[A[A

embedding all sents with BERT: 344it [17:04,  2.69s/it][A[A
cleaning sentances: 344it [17:06,  2.69s/it][ATokenizing tekst text into sentances: 344it [17:06,  2.69s/it]


reducing sentences to a single doc embeding: 345it [17:08,  3.11s/it][A[A[A

embedding all sents with BERT: 345it [17:08,  3.11s/it][A[A
cleaning sentances: 345it [17:10,  3.11s/it][ATokenizing tekst text into sentances: 345it [17:10,  3.11s/it]


reducing sentences to a single doc embeding: 346it [17:25,  7.14s/it][A[A[A

embedding all sents with BERT: 346it [17:25,  7.14s/it][A[A
cleaning sentances: 346it [17:27,  7.14s/it][ATokenizing tekst text into sentances: 346it [17:27,  7.14s/it]


reducing sentences to a single doc embeding: 347it [17:39,  9.47s/it][A[A[A

embedding all sents with BERT: 347it [17:39,  9.47s/it][A[A
cleaning sentances: 347it [17:42,  9.47s/it][ATokenizing tekst text into sentances: 347it [17:42,  9.47s/it]


reducing sentences to a single doc embeding: 348it [17:53, 10.71s/it][A[A[A

embedding all sents with BERT: 348it [17:53, 10.71s/it][A[A
cleaning sentances: 348it [17:55, 10.71s/it][ATokenizing tekst text into sentances: 348it [17:55, 10.71s/it]


reducing sentences to a single doc embeding: 349it [17:53,  7.57s/it][A[A[A

embedding all sents with BERT: 349it [17:53,  7.57s/it][A[A
cleaning sentances: 349it [17:56,  7.57s/it][ATokenizing tekst text into sentances: 349it [17:56,  7.57s/it]


reducing sentences to a single doc embeding: 350it [17:54,  5.47s/it][A[A[A

embedding all sents with BERT: 350it [17:54,  5.47s/it][A[A
cleaning sentances: 350it [17:56,  5.47s/it][ATokenizing tekst text into sentances: 350it [17:56,  5.47s/it]


reducing sentences to a single doc embeding: 351it [17:58,  4.96s/it][A[A[A

embedding all sents with BERT: 351it [17:58,  4.96s/it][A[A
cleaning sentances: 351it [18:00,  4.96s/it][ATokenizing tekst text into sentances: 351it [18:00,  4.96s/it]


reducing sentences to a single doc embeding: 352it [17:58,  3.56s/it][A[A[A

embedding all sents with BERT: 352it [17:58,  3.56s/it][A[A
cleaning sentances: 352it [18:00,  3.56s/it][ATokenizing tekst text into sentances: 352it [18:00,  3.56s/it]


reducing sentences to a single doc embeding: 353it [18:24, 10.19s/it][A[A[A

embedding all sents with BERT: 353it [18:24, 10.19s/it][A[A
cleaning sentances: 353it [18:26, 10.19s/it][ATokenizing tekst text into sentances: 353it [18:26, 10.19s/it]


reducing sentences to a single doc embeding: 354it [18:25,  7.52s/it][A[A[A

embedding all sents with BERT: 354it [18:25,  7.52s/it][A[A
cleaning sentances: 354it [18:27,  7.52s/it][ATokenizing tekst text into sentances: 354it [18:27,  7.52s/it]


reducing sentences to a single doc embeding: 355it [18:25,  5.35s/it][A[A[A

embedding all sents with BERT: 355it [18:25,  5.35s/it][A[A
cleaning sentances: 355it [18:28,  5.35s/it][ATokenizing tekst text into sentances: 355it [18:28,  5.35s/it]


reducing sentences to a single doc embeding: 356it [18:27,  4.16s/it][A[A[A

embedding all sents with BERT: 356it [18:27,  4.16s/it][A[A
cleaning sentances: 356it [18:29,  4.16s/it][ATokenizing tekst text into sentances: 356it [18:29,  4.16s/it]


reducing sentences to a single doc embeding: 357it [18:29,  3.61s/it][A[A[A

embedding all sents with BERT: 357it [18:29,  3.61s/it][A[A
cleaning sentances: 357it [18:31,  3.61s/it][ATokenizing tekst text into sentances: 357it [18:31,  3.61s/it]


reducing sentences to a single doc embeding: 358it [18:36,  4.77s/it][A[A[A

embedding all sents with BERT: 358it [18:36,  4.77s/it][A[A
cleaning sentances: 358it [18:39,  4.77s/it][ATokenizing tekst text into sentances: 358it [18:39,  4.77s/it]


reducing sentences to a single doc embeding: 359it [18:42,  4.96s/it][A[A[A

embedding all sents with BERT: 359it [18:42,  4.96s/it][A[A
cleaning sentances: 359it [18:44,  4.96s/it][ATokenizing tekst text into sentances: 359it [18:44,  4.96s/it]


reducing sentences to a single doc embeding: 360it [18:42,  3.60s/it][A[A[A

embedding all sents with BERT: 360it [18:42,  3.60s/it][A[A
cleaning sentances: 360it [18:45,  3.60s/it][ATokenizing tekst text into sentances: 360it [18:45,  3.60s/it]


reducing sentences to a single doc embeding: 361it [18:47,  4.05s/it][A[A[A

embedding all sents with BERT: 361it [18:47,  4.05s/it][A[A
cleaning sentances: 361it [18:50,  4.05s/it][ATokenizing tekst text into sentances: 361it [18:50,  4.05s/it]


reducing sentences to a single doc embeding: 362it [18:50,  3.52s/it][A[A[A

embedding all sents with BERT: 362it [18:50,  3.52s/it][A[A
cleaning sentances: 362it [18:52,  3.52s/it][ATokenizing tekst text into sentances: 362it [18:52,  3.52s/it]


reducing sentences to a single doc embeding: 363it [18:50,  2.59s/it][A[A[A

embedding all sents with BERT: 363it [18:50,  2.59s/it][A[A
cleaning sentances: 363it [18:52,  2.59s/it][ATokenizing tekst text into sentances: 363it [18:52,  2.59s/it]


reducing sentences to a single doc embeding: 364it [18:51,  1.98s/it][A[A[A

embedding all sents with BERT: 364it [18:51,  1.98s/it][A[A
cleaning sentances: 364it [18:53,  1.98s/it][ATokenizing tekst text into sentances: 364it [18:53,  1.98s/it]


reducing sentences to a single doc embeding: 365it [18:53,  2.10s/it][A[A[A

embedding all sents with BERT: 365it [18:53,  2.10s/it][A[A
cleaning sentances: 365it [18:55,  2.10s/it][ATokenizing tekst text into sentances: 365it [18:55,  2.10s/it]


reducing sentences to a single doc embeding: 366it [18:54,  1.91s/it][A[A[A

embedding all sents with BERT: 366it [18:54,  1.91s/it][A[A
cleaning sentances: 366it [18:57,  1.91s/it][ATokenizing tekst text into sentances: 366it [18:57,  1.91s/it]


reducing sentences to a single doc embeding: 367it [18:56,  1.74s/it][A[A[A

embedding all sents with BERT: 367it [18:56,  1.74s/it][A[A
cleaning sentances: 367it [18:58,  1.74s/it][ATokenizing tekst text into sentances: 367it [18:58,  1.74s/it]


reducing sentences to a single doc embeding: 368it [19:02,  2.96s/it][A[A[A

embedding all sents with BERT: 368it [19:02,  2.96s/it][A[A
cleaning sentances: 368it [19:04,  2.96s/it][ATokenizing tekst text into sentances: 368it [19:04,  2.96s/it]


reducing sentences to a single doc embeding: 369it [19:04,  2.75s/it][A[A[A

embedding all sents with BERT: 369it [19:04,  2.75s/it][A[A
cleaning sentances: 369it [19:06,  2.75s/it][ATokenizing tekst text into sentances: 369it [19:06,  2.75s/it]


reducing sentences to a single doc embeding: 370it [19:20,  6.63s/it][A[A[A

embedding all sents with BERT: 370it [19:20,  6.63s/it][A[A
cleaning sentances: 370it [19:22,  6.63s/it][ATokenizing tekst text into sentances: 370it [19:22,  6.63s/it]


reducing sentences to a single doc embeding: 371it [19:33,  8.83s/it][A[A[A

embedding all sents with BERT: 371it [19:33,  8.83s/it][A[A
cleaning sentances: 371it [19:36,  8.83s/it][ATokenizing tekst text into sentances: 371it [19:36,  8.83s/it]


reducing sentences to a single doc embeding: 372it [19:34,  6.41s/it][A[A[A

embedding all sents with BERT: 372it [19:34,  6.41s/it][A[A
cleaning sentances: 372it [19:37,  6.41s/it][ATokenizing tekst text into sentances: 372it [19:37,  6.41s/it]


reducing sentences to a single doc embeding: 373it [19:35,  4.87s/it][A[A[A

embedding all sents with BERT: 373it [19:35,  4.87s/it][A[A
cleaning sentances: 373it [19:38,  4.87s/it][ATokenizing tekst text into sentances: 373it [19:38,  4.87s/it]


reducing sentences to a single doc embeding: 374it [19:36,  3.65s/it][A[A[A

embedding all sents with BERT: 374it [19:36,  3.65s/it][A[A
cleaning sentances: 374it [19:39,  3.65s/it][ATokenizing tekst text into sentances: 374it [19:39,  3.65s/it]


reducing sentences to a single doc embeding: 375it [19:39,  3.23s/it][A[A[A

embedding all sents with BERT: 375it [19:39,  3.23s/it][A[A
cleaning sentances: 375it [19:41,  3.23s/it][ATokenizing tekst text into sentances: 375it [19:41,  3.23s/it]


reducing sentences to a single doc embeding: 376it [19:55,  7.34s/it][A[A[A

embedding all sents with BERT: 376it [19:55,  7.34s/it][A[A
cleaning sentances: 376it [19:58,  7.34s/it][ATokenizing tekst text into sentances: 376it [19:58,  7.34s/it]


reducing sentences to a single doc embeding: 377it [19:56,  5.35s/it][A[A[A

embedding all sents with BERT: 377it [19:56,  5.35s/it][A[A
cleaning sentances: 377it [19:59,  5.35s/it][ATokenizing tekst text into sentances: 377it [19:59,  5.35s/it]


reducing sentences to a single doc embeding: 378it [19:57,  3.87s/it][A[A[A

embedding all sents with BERT: 378it [19:57,  3.87s/it][A[A
cleaning sentances: 378it [19:59,  3.87s/it][ATokenizing tekst text into sentances: 378it [19:59,  3.87s/it]


reducing sentences to a single doc embeding: 379it [19:57,  2.78s/it][A[A[A

embedding all sents with BERT: 379it [19:57,  2.78s/it][A[A
cleaning sentances: 379it [19:59,  2.78s/it][ATokenizing tekst text into sentances: 379it [19:59,  2.78s/it]


reducing sentences to a single doc embeding: 380it [19:58,  2.42s/it][A[A[A

embedding all sents with BERT: 380it [19:58,  2.42s/it][A[A
cleaning sentances: 380it [20:01,  2.42s/it][ATokenizing tekst text into sentances: 380it [20:01,  2.42s/it]


reducing sentences to a single doc embeding: 381it [19:59,  1.79s/it][A[A[A

embedding all sents with BERT: 381it [19:59,  1.79s/it][A[A
cleaning sentances: 381it [20:01,  1.79s/it][ATokenizing tekst text into sentances: 381it [20:01,  1.79s/it]


reducing sentences to a single doc embeding: 382it [20:00,  1.50s/it][A[A[A

embedding all sents with BERT: 382it [20:00,  1.50s/it][A[A
cleaning sentances: 382it [20:02,  1.50s/it][ATokenizing tekst text into sentances: 382it [20:02,  1.50s/it]


reducing sentences to a single doc embeding: 383it [20:03,  1.94s/it][A[A[A

embedding all sents with BERT: 383it [20:03,  1.94s/it][A[A
cleaning sentances: 383it [20:05,  1.94s/it][ATokenizing tekst text into sentances: 383it [20:05,  1.94s/it]


reducing sentences to a single doc embeding: 384it [20:11,  3.80s/it][A[A[A

embedding all sents with BERT: 384it [20:11,  3.80s/it][A[A
cleaning sentances: 384it [20:13,  3.80s/it][ATokenizing tekst text into sentances: 384it [20:13,  3.80s/it]


reducing sentences to a single doc embeding: 385it [20:11,  2.78s/it][A[A[A

embedding all sents with BERT: 385it [20:11,  2.78s/it][A[A
cleaning sentances: 385it [20:13,  2.78s/it][ATokenizing tekst text into sentances: 385it [20:13,  2.78s/it]


reducing sentences to a single doc embeding: 386it [20:12,  2.25s/it][A[A[A

embedding all sents with BERT: 386it [20:12,  2.25s/it][A[A
cleaning sentances: 386it [20:14,  2.25s/it][ATokenizing tekst text into sentances: 386it [20:14,  2.25s/it]


reducing sentences to a single doc embeding: 387it [20:13,  1.84s/it][A[A[A

embedding all sents with BERT: 387it [20:13,  1.84s/it][A[A
cleaning sentances: 387it [20:15,  1.84s/it][ATokenizing tekst text into sentances: 387it [20:15,  1.84s/it]


reducing sentences to a single doc embeding: 388it [20:14,  1.48s/it][A[A[A

embedding all sents with BERT: 388it [20:14,  1.48s/it][A[A
cleaning sentances: 388it [20:16,  1.48s/it][ATokenizing tekst text into sentances: 388it [20:16,  1.48s/it]


reducing sentences to a single doc embeding: 389it [20:14,  1.22s/it][A[A[A

embedding all sents with BERT: 389it [20:14,  1.22s/it][A[A
cleaning sentances: 389it [20:17,  1.22s/it][ATokenizing tekst text into sentances: 389it [20:17,  1.22s/it]


reducing sentences to a single doc embeding: 390it [20:18,  2.12s/it][A[A[A

embedding all sents with BERT: 390it [20:18,  2.12s/it][A[A
cleaning sentances: 390it [20:21,  2.12s/it][ATokenizing tekst text into sentances: 390it [20:21,  2.12s/it]


reducing sentences to a single doc embeding: 391it [20:22,  2.66s/it][A[A[A

embedding all sents with BERT: 391it [20:22,  2.66s/it][A[A
cleaning sentances: 391it [20:25,  2.66s/it][ATokenizing tekst text into sentances: 391it [20:25,  2.66s/it]


reducing sentences to a single doc embeding: 392it [20:34,  5.24s/it][A[A[A

embedding all sents with BERT: 392it [20:34,  5.24s/it][A[A
cleaning sentances: 392it [20:36,  5.24s/it][ATokenizing tekst text into sentances: 392it [20:36,  5.24s/it]


reducing sentences to a single doc embeding: 393it [20:36,  4.51s/it][A[A[A

embedding all sents with BERT: 393it [20:36,  4.51s/it][A[A
cleaning sentances: 393it [20:39,  4.51s/it][ATokenizing tekst text into sentances: 393it [20:39,  4.51s/it]


reducing sentences to a single doc embeding: 394it [20:46,  5.89s/it][A[A[A

embedding all sents with BERT: 394it [20:46,  5.89s/it][A[A
cleaning sentances: 394it [20:48,  5.89s/it][ATokenizing tekst text into sentances: 394it [20:48,  5.89s/it]


reducing sentences to a single doc embeding: 395it [21:01,  8.68s/it][A[A[A

embedding all sents with BERT: 395it [21:01,  8.68s/it][A[A
cleaning sentances: 395it [21:03,  8.68s/it][ATokenizing tekst text into sentances: 395it [21:03,  8.68s/it]


reducing sentences to a single doc embeding: 396it [21:03,  6.85s/it][A[A[A

embedding all sents with BERT: 396it [21:03,  6.85s/it][A[A
cleaning sentances: 396it [21:06,  6.85s/it][ATokenizing tekst text into sentances: 396it [21:06,  6.85s/it]


reducing sentences to a single doc embeding: 397it [21:47, 17.90s/it][A[A[A

embedding all sents with BERT: 397it [21:47, 17.90s/it][A[A
cleaning sentances: 397it [21:49, 17.90s/it][ATokenizing tekst text into sentances: 397it [21:49, 17.90s/it]


reducing sentences to a single doc embeding: 398it [21:49, 13.05s/it][A[A[A

embedding all sents with BERT: 398it [21:49, 13.05s/it][A[A
cleaning sentances: 398it [21:51, 13.05s/it][ATokenizing tekst text into sentances: 398it [21:51, 13.05s/it]


reducing sentences to a single doc embeding: 399it [21:54, 10.81s/it][A[A[A

embedding all sents with BERT: 399it [21:54, 10.81s/it][A[A
cleaning sentances: 399it [21:57, 10.81s/it][ATokenizing tekst text into sentances: 399it [21:57, 10.81s/it]


reducing sentences to a single doc embeding: 400it [21:55,  7.74s/it][A[A[A

embedding all sents with BERT: 400it [21:55,  7.74s/it][A[A
cleaning sentances: 400it [21:57,  7.74s/it][ATokenizing tekst text into sentances: 400it [21:57,  7.74s/it]


reducing sentences to a single doc embeding: 401it [21:55,  5.54s/it][A[A[A

embedding all sents with BERT: 401it [21:55,  5.54s/it][A[A
cleaning sentances: 401it [21:58,  5.54s/it][ATokenizing tekst text into sentances: 401it [21:58,  5.54s/it]


reducing sentences to a single doc embeding: 402it [22:05,  6.67s/it][A[A[A

embedding all sents with BERT: 402it [22:05,  6.67s/it][A[A
cleaning sentances: 402it [22:07,  6.67s/it][ATokenizing tekst text into sentances: 402it [22:07,  6.67s/it]


reducing sentences to a single doc embeding: 403it [22:05,  4.81s/it][A[A[A

embedding all sents with BERT: 403it [22:05,  4.81s/it][A[A
cleaning sentances: 403it [22:07,  4.81s/it][ATokenizing tekst text into sentances: 403it [22:07,  4.81s/it]


reducing sentences to a single doc embeding: 404it [22:08,  4.31s/it][A[A[A

embedding all sents with BERT: 404it [22:08,  4.31s/it][A[A
cleaning sentances: 404it [22:11,  4.31s/it][ATokenizing tekst text into sentances: 404it [22:11,  4.31s/it]


reducing sentences to a single doc embeding: 405it [22:08,  3.07s/it][A[A[A

embedding all sents with BERT: 405it [22:08,  3.07s/it][A[A
cleaning sentances: 405it [22:11,  3.07s/it][ATokenizing tekst text into sentances: 405it [22:11,  3.07s/it]


reducing sentences to a single doc embeding: 406it [22:15,  4.24s/it][A[A[A

embedding all sents with BERT: 406it [22:15,  4.24s/it][A[A
cleaning sentances: 406it [22:18,  4.24s/it][ATokenizing tekst text into sentances: 406it [22:18,  4.24s/it]


reducing sentences to a single doc embeding: 407it [22:19,  3.91s/it][A[A[A

embedding all sents with BERT: 407it [22:19,  3.91s/it][A[A
cleaning sentances: 407it [22:21,  3.91s/it][ATokenizing tekst text into sentances: 407it [22:21,  3.91s/it]


reducing sentences to a single doc embeding: 408it [22:36,  8.04s/it][A[A[A

embedding all sents with BERT: 408it [22:36,  8.04s/it][A[A
cleaning sentances: 408it [22:39,  8.04s/it][ATokenizing tekst text into sentances: 408it [22:39,  8.04s/it]


reducing sentences to a single doc embeding: 409it [22:37,  6.02s/it][A[A[A

embedding all sents with BERT: 409it [22:37,  6.02s/it][A[A
cleaning sentances: 409it [22:40,  6.02s/it][ATokenizing tekst text into sentances: 409it [22:40,  6.02s/it]


reducing sentences to a single doc embeding: 410it [22:41,  5.39s/it][A[A[A

embedding all sents with BERT: 410it [22:41,  5.39s/it][A[A
cleaning sentances: 410it [22:44,  5.39s/it][ATokenizing tekst text into sentances: 410it [22:44,  5.39s/it]


reducing sentences to a single doc embeding: 411it [22:42,  3.94s/it][A[A[A

embedding all sents with BERT: 411it [22:42,  3.94s/it][A[A
cleaning sentances: 411it [22:44,  3.94s/it][ATokenizing tekst text into sentances: 411it [22:44,  3.94s/it]


reducing sentences to a single doc embeding: 412it [22:44,  3.29s/it][A[A[A

embedding all sents with BERT: 412it [22:44,  3.29s/it][A[A
cleaning sentances: 412it [22:46,  3.29s/it][ATokenizing tekst text into sentances: 412it [22:46,  3.29s/it]


reducing sentences to a single doc embeding: 413it [22:51,  4.40s/it][A[A[A

embedding all sents with BERT: 413it [22:51,  4.40s/it][A[A
cleaning sentances: 413it [22:53,  4.40s/it][ATokenizing tekst text into sentances: 413it [22:53,  4.40s/it]


reducing sentences to a single doc embeding: 414it [22:52,  3.50s/it][A[A[A

embedding all sents with BERT: 414it [22:52,  3.50s/it][A[A
cleaning sentances: 414it [22:55,  3.50s/it][ATokenizing tekst text into sentances: 414it [22:55,  3.50s/it]


reducing sentences to a single doc embeding: 415it [22:54,  3.06s/it][A[A[A

embedding all sents with BERT: 415it [22:54,  3.06s/it][A[A
cleaning sentances: 415it [22:57,  3.06s/it][ATokenizing tekst text into sentances: 415it [22:57,  3.06s/it]


reducing sentences to a single doc embeding: 416it [22:57,  2.90s/it][A[A[A

embedding all sents with BERT: 416it [22:57,  2.90s/it][A[A
cleaning sentances: 416it [22:59,  2.90s/it][ATokenizing tekst text into sentances: 416it [22:59,  2.90s/it]


reducing sentences to a single doc embeding: 417it [22:57,  2.11s/it][A[A[A

embedding all sents with BERT: 417it [22:57,  2.11s/it][A[A
cleaning sentances: 417it [22:59,  2.11s/it][ATokenizing tekst text into sentances: 417it [22:59,  2.11s/it]


reducing sentences to a single doc embeding: 418it [22:58,  1.65s/it][A[A[A

embedding all sents with BERT: 418it [22:58,  1.65s/it][A[A
cleaning sentances: 418it [23:00,  1.65s/it][ATokenizing tekst text into sentances: 418it [23:00,  1.65s/it]


reducing sentences to a single doc embeding: 419it [23:02,  2.48s/it][A[A[A

embedding all sents with BERT: 419it [23:02,  2.48s/it][A[A
cleaning sentances: 419it [23:04,  2.48s/it][ATokenizing tekst text into sentances: 419it [23:04,  2.48s/it]


reducing sentences to a single doc embeding: 420it [23:04,  2.24s/it][A[A[A

embedding all sents with BERT: 420it [23:04,  2.24s/it][A[A
cleaning sentances: 420it [23:06,  2.24s/it][ATokenizing tekst text into sentances: 420it [23:06,  2.24s/it]


reducing sentences to a single doc embeding: 421it [23:15,  4.92s/it][A[A[A

embedding all sents with BERT: 421it [23:15,  4.92s/it][A[A
cleaning sentances: 421it [23:17,  4.92s/it][ATokenizing tekst text into sentances: 421it [23:17,  4.92s/it]


reducing sentences to a single doc embeding: 422it [23:16,  3.94s/it][A[A[A

embedding all sents with BERT: 422it [23:16,  3.94s/it][A[A
cleaning sentances: 422it [23:19,  3.94s/it][ATokenizing tekst text into sentances: 422it [23:19,  3.94s/it]


reducing sentences to a single doc embeding: 423it [23:17,  2.82s/it][A[A[A

embedding all sents with BERT: 423it [23:17,  2.82s/it][A[A
cleaning sentances: 423it [23:19,  2.82s/it][ATokenizing tekst text into sentances: 423it [23:19,  2.82s/it]


reducing sentences to a single doc embeding: 424it [23:18,  2.23s/it][A[A[A

embedding all sents with BERT: 424it [23:18,  2.23s/it][A[A
cleaning sentances: 424it [23:20,  2.23s/it][ATokenizing tekst text into sentances: 424it [23:20,  2.23s/it]


reducing sentences to a single doc embeding: 425it [23:20,  2.28s/it][A[A[A

embedding all sents with BERT: 425it [23:20,  2.28s/it][A[A
cleaning sentances: 425it [23:22,  2.28s/it][ATokenizing tekst text into sentances: 425it [23:22,  2.28s/it]


reducing sentences to a single doc embeding: 426it [23:21,  1.79s/it][A[A[A

embedding all sents with BERT: 426it [23:21,  1.79s/it][A[A
cleaning sentances: 426it [23:23,  1.79s/it][ATokenizing tekst text into sentances: 426it [23:23,  1.79s/it]


reducing sentences to a single doc embeding: 427it [23:24,  2.37s/it][A[A[A

embedding all sents with BERT: 427it [23:24,  2.37s/it][A[A
cleaning sentances: 427it [23:27,  2.37s/it][ATokenizing tekst text into sentances: 427it [23:27,  2.37s/it]


reducing sentences to a single doc embeding: 428it [23:26,  2.08s/it][A[A[A

embedding all sents with BERT: 428it [23:26,  2.08s/it][A[A
cleaning sentances: 428it [23:28,  2.08s/it][ATokenizing tekst text into sentances: 428it [23:28,  2.08s/it]


reducing sentences to a single doc embeding: 429it [23:28,  2.00s/it][A[A[A

embedding all sents with BERT: 429it [23:28,  2.00s/it][A[A
cleaning sentances: 429it [23:30,  2.00s/it][ATokenizing tekst text into sentances: 429it [23:30,  2.00s/it]


reducing sentences to a single doc embeding: 430it [24:07, 13.19s/it][A[A[A

embedding all sents with BERT: 430it [24:07, 13.19s/it][A[A
cleaning sentances: 430it [24:09, 13.19s/it][ATokenizing tekst text into sentances: 430it [24:09, 13.19s/it]


reducing sentences to a single doc embeding: 431it [24:08,  9.49s/it][A[A[A

embedding all sents with BERT: 431it [24:08,  9.49s/it][A[A
cleaning sentances: 431it [24:10,  9.49s/it][ATokenizing tekst text into sentances: 431it [24:10,  9.49s/it]


reducing sentences to a single doc embeding: 432it [25:28, 30.86s/it][A[A[A

embedding all sents with BERT: 432it [25:28, 30.86s/it][A[A
cleaning sentances: 432it [25:31, 30.86s/it][ATokenizing tekst text into sentances: 432it [25:31, 30.86s/it]


reducing sentences to a single doc embeding: 433it [26:29, 39.79s/it][A[A[A

embedding all sents with BERT: 433it [26:29, 39.79s/it][A[A
cleaning sentances: 433it [26:31, 39.79s/it][ATokenizing tekst text into sentances: 433it [26:31, 39.79s/it]


reducing sentences to a single doc embeding: 434it [26:56, 36.09s/it][A[A[A

embedding all sents with BERT: 434it [26:56, 36.09s/it][A[A
cleaning sentances: 434it [26:59, 36.09s/it][ATokenizing tekst text into sentances: 434it [26:59, 36.09s/it]


reducing sentences to a single doc embeding: 435it [27:22, 32.96s/it][A[A[A

embedding all sents with BERT: 435it [27:22, 32.96s/it][A[A
cleaning sentances: 435it [27:24, 32.96s/it][ATokenizing tekst text into sentances: 435it [27:24, 32.96s/it]


reducing sentences to a single doc embeding: 436it [27:22, 23.17s/it][A[A[A

embedding all sents with BERT: 436it [27:22, 23.17s/it][A[A
cleaning sentances: 436it [27:25, 23.17s/it][ATokenizing tekst text into sentances: 436it [27:25, 23.17s/it]


reducing sentences to a single doc embeding: 437it [27:25, 16.90s/it][A[A[A

embedding all sents with BERT: 437it [27:25, 16.90s/it][A[A
cleaning sentances: 437it [27:27, 16.90s/it][ATokenizing tekst text into sentances: 437it [27:27, 16.90s/it]


reducing sentences to a single doc embeding: 438it [27:25, 12.05s/it][A[A[A

embedding all sents with BERT: 438it [27:25, 12.05s/it][A[A
cleaning sentances: 438it [27:28, 12.05s/it][ATokenizing tekst text into sentances: 438it [27:28, 12.05s/it]


reducing sentences to a single doc embeding: 439it [27:33, 10.74s/it][A[A[A

embedding all sents with BERT: 439it [27:33, 10.74s/it][A[A
cleaning sentances: 439it [27:36, 10.74s/it][ATokenizing tekst text into sentances: 439it [27:36, 10.74s/it]


reducing sentences to a single doc embeding: 440it [27:33,  7.56s/it][A[A[A

embedding all sents with BERT: 440it [27:33,  7.56s/it][A[A
cleaning sentances: 440it [27:36,  7.56s/it][ATokenizing tekst text into sentances: 440it [27:36,  7.56s/it]


reducing sentences to a single doc embeding: 441it [27:37,  6.50s/it][A[A[A

embedding all sents with BERT: 441it [27:37,  6.50s/it][A[A
cleaning sentances: 441it [27:40,  6.50s/it][ATokenizing tekst text into sentances: 441it [27:40,  6.50s/it]


reducing sentences to a single doc embeding: 442it [28:02, 12.00s/it][A[A[A

embedding all sents with BERT: 442it [28:02, 12.00s/it][A[A
cleaning sentances: 442it [28:04, 12.00s/it][ATokenizing tekst text into sentances: 442it [28:04, 12.00s/it]


reducing sentences to a single doc embeding: 443it [28:03,  8.61s/it][A[A[A

embedding all sents with BERT: 443it [28:03,  8.61s/it][A[A
cleaning sentances: 443it [28:05,  8.61s/it][ATokenizing tekst text into sentances: 443it [28:05,  8.61s/it]


reducing sentences to a single doc embeding: 444it [28:10,  8.06s/it][A[A[A

embedding all sents with BERT: 444it [28:10,  8.06s/it][A[A
cleaning sentances: 444it [28:12,  8.06s/it][ATokenizing tekst text into sentances: 444it [28:12,  8.06s/it]


reducing sentences to a single doc embeding: 445it [28:10,  5.89s/it][A[A[A

embedding all sents with BERT: 445it [28:10,  5.89s/it][A[A
cleaning sentances: 445it [28:13,  5.89s/it][ATokenizing tekst text into sentances: 445it [28:13,  5.89s/it]


reducing sentences to a single doc embeding: 446it [28:21,  7.17s/it][A[A[A

embedding all sents with BERT: 446it [28:21,  7.17s/it][A[A
cleaning sentances: 446it [28:23,  7.17s/it][ATokenizing tekst text into sentances: 446it [28:23,  7.17s/it]


reducing sentences to a single doc embeding: 447it [28:21,  5.21s/it][A[A[A

embedding all sents with BERT: 447it [28:21,  5.21s/it][A[A
cleaning sentances: 447it [28:24,  5.21s/it][ATokenizing tekst text into sentances: 447it [28:24,  5.21s/it]


reducing sentences to a single doc embeding: 448it [28:25,  4.65s/it][A[A[A

embedding all sents with BERT: 448it [28:25,  4.65s/it][A[A
cleaning sentances: 448it [28:27,  4.65s/it][ATokenizing tekst text into sentances: 448it [28:27,  4.65s/it]


reducing sentences to a single doc embeding: 449it [28:26,  3.72s/it][A[A[A

embedding all sents with BERT: 449it [28:26,  3.72s/it][A[A
cleaning sentances: 449it [28:28,  3.72s/it][ATokenizing tekst text into sentances: 449it [28:28,  3.72s/it]


reducing sentences to a single doc embeding: 450it [28:26,  2.68s/it][A[A[A

embedding all sents with BERT: 450it [28:26,  2.68s/it][A[A
cleaning sentances: 450it [28:29,  2.68s/it][ATokenizing tekst text into sentances: 450it [28:29,  2.68s/it]


reducing sentences to a single doc embeding: 451it [28:28,  2.30s/it][A[A[A

embedding all sents with BERT: 451it [28:28,  2.30s/it][A[A
cleaning sentances: 451it [28:30,  2.30s/it][ATokenizing tekst text into sentances: 451it [28:30,  2.30s/it]


reducing sentences to a single doc embeding: 452it [28:44,  6.45s/it][A[A[A

embedding all sents with BERT: 452it [28:44,  6.45s/it][A[A
cleaning sentances: 452it [28:46,  6.45s/it][ATokenizing tekst text into sentances: 452it [28:46,  6.45s/it]


reducing sentences to a single doc embeding: 453it [28:45,  4.76s/it][A[A[A

embedding all sents with BERT: 453it [28:45,  4.76s/it][A[A
cleaning sentances: 453it [28:47,  4.76s/it][ATokenizing tekst text into sentances: 453it [28:47,  4.76s/it]


reducing sentences to a single doc embeding: 454it [29:07,  9.99s/it][A[A[A

embedding all sents with BERT: 454it [29:07,  9.99s/it][A[A
cleaning sentances: 454it [29:09,  9.99s/it][ATokenizing tekst text into sentances: 454it [29:09,  9.99s/it]


reducing sentences to a single doc embeding: 455it [29:16,  9.86s/it][A[A[A

embedding all sents with BERT: 455it [29:16,  9.86s/it][A[A
cleaning sentances: 455it [29:19,  9.86s/it][ATokenizing tekst text into sentances: 455it [29:19,  9.86s/it]


reducing sentences to a single doc embeding: 456it [29:19,  7.69s/it][A[A[A

embedding all sents with BERT: 456it [29:19,  7.69s/it][A[A
cleaning sentances: 456it [29:21,  7.69s/it][ATokenizing tekst text into sentances: 456it [29:21,  7.69s/it]


reducing sentences to a single doc embeding: 457it [29:25,  7.26s/it][A[A[A

embedding all sents with BERT: 457it [29:25,  7.26s/it][A[A
cleaning sentances: 457it [29:28,  7.26s/it][ATokenizing tekst text into sentances: 457it [29:28,  7.26s/it]


reducing sentences to a single doc embeding: 458it [29:27,  5.49s/it][A[A[A

embedding all sents with BERT: 458it [29:27,  5.49s/it][A[A
cleaning sentances: 458it [29:29,  5.49s/it][ATokenizing tekst text into sentances: 458it [29:29,  5.49s/it]


reducing sentences to a single doc embeding: 459it [29:27,  3.96s/it][A[A[A

embedding all sents with BERT: 459it [29:27,  3.96s/it][A[A
cleaning sentances: 459it [29:29,  3.96s/it][ATokenizing tekst text into sentances: 459it [29:29,  3.96s/it]


reducing sentences to a single doc embeding: 460it [29:32,  4.22s/it][A[A[A

embedding all sents with BERT: 460it [29:32,  4.22s/it][A[A
cleaning sentances: 460it [29:34,  4.22s/it][ATokenizing tekst text into sentances: 460it [29:34,  4.22s/it]


reducing sentences to a single doc embeding: 461it [30:05, 12.97s/it][A[A[A

embedding all sents with BERT: 461it [30:05, 12.97s/it][A[A
cleaning sentances: 461it [30:08, 12.97s/it][ATokenizing tekst text into sentances: 461it [30:08, 12.97s/it]


reducing sentences to a single doc embeding: 462it [30:06,  9.42s/it][A[A[A

embedding all sents with BERT: 462it [30:06,  9.42s/it][A[A
cleaning sentances: 462it [30:09,  9.42s/it][ATokenizing tekst text into sentances: 462it [30:09,  9.42s/it]


reducing sentences to a single doc embeding: 463it [30:07,  6.80s/it][A[A[A

embedding all sents with BERT: 463it [30:07,  6.80s/it][A[A
cleaning sentances: 463it [30:10,  6.80s/it][ATokenizing tekst text into sentances: 463it [30:10,  6.80s/it]


reducing sentences to a single doc embeding: 464it [30:09,  5.20s/it][A[A[A

embedding all sents with BERT: 464it [30:09,  5.20s/it][A[A
cleaning sentances: 464it [30:11,  5.20s/it][ATokenizing tekst text into sentances: 464it [30:11,  5.20s/it]


reducing sentences to a single doc embeding: 465it [30:10,  4.04s/it][A[A[A

embedding all sents with BERT: 465it [30:10,  4.04s/it][A[A
cleaning sentances: 465it [30:12,  4.04s/it][ATokenizing tekst text into sentances: 465it [30:12,  4.04s/it]


reducing sentences to a single doc embeding: 466it [30:15,  4.35s/it][A[A[A

embedding all sents with BERT: 466it [30:15,  4.35s/it][A[A
cleaning sentances: 466it [30:17,  4.35s/it][ATokenizing tekst text into sentances: 466it [30:17,  4.35s/it]


reducing sentences to a single doc embeding: 467it [30:17,  3.56s/it][A[A[A

embedding all sents with BERT: 467it [30:17,  3.56s/it][A[A
cleaning sentances: 467it [30:19,  3.56s/it][ATokenizing tekst text into sentances: 467it [30:19,  3.56s/it]


reducing sentences to a single doc embeding: 468it [30:19,  3.17s/it][A[A[A

embedding all sents with BERT: 468it [30:19,  3.17s/it][A[A
cleaning sentances: 468it [30:21,  3.17s/it][ATokenizing tekst text into sentances: 468it [30:21,  3.17s/it]


reducing sentences to a single doc embeding: 469it [30:28,  5.06s/it][A[A[A

embedding all sents with BERT: 469it [30:28,  5.06s/it][A[A
cleaning sentances: 469it [30:31,  5.06s/it][ATokenizing tekst text into sentances: 469it [30:31,  5.06s/it]


reducing sentences to a single doc embeding: 470it [30:30,  4.00s/it][A[A[A

embedding all sents with BERT: 470it [30:30,  4.00s/it][A[A
cleaning sentances: 470it [30:32,  4.00s/it][ATokenizing tekst text into sentances: 470it [30:32,  4.00s/it]


reducing sentences to a single doc embeding: 471it [30:31,  3.12s/it][A[A[A

embedding all sents with BERT: 471it [30:31,  3.12s/it][A[A
cleaning sentances: 471it [30:33,  3.12s/it][ATokenizing tekst text into sentances: 471it [30:33,  3.12s/it]


reducing sentences to a single doc embeding: 472it [30:33,  2.64s/it][A[A[A

embedding all sents with BERT: 472it [30:33,  2.64s/it][A[A
cleaning sentances: 472it [30:35,  2.64s/it][ATokenizing tekst text into sentances: 472it [30:35,  2.64s/it]


reducing sentences to a single doc embeding: 473it [30:34,  2.34s/it][A[A[A

embedding all sents with BERT: 473it [30:34,  2.34s/it][A[A
cleaning sentances: 473it [30:37,  2.34s/it][ATokenizing tekst text into sentances: 473it [30:37,  2.34s/it]


reducing sentences to a single doc embeding: 474it [30:36,  2.12s/it][A[A[A

embedding all sents with BERT: 474it [30:36,  2.12s/it][A[A
cleaning sentances: 474it [30:38,  2.12s/it][ATokenizing tekst text into sentances: 474it [30:38,  2.12s/it]


reducing sentences to a single doc embeding: 475it [30:41,  3.00s/it][A[A[A

embedding all sents with BERT: 475it [30:41,  3.00s/it][A[A
cleaning sentances: 475it [30:43,  3.00s/it][ATokenizing tekst text into sentances: 475it [30:43,  3.00s/it]


reducing sentences to a single doc embeding: 476it [30:46,  3.54s/it][A[A[A

embedding all sents with BERT: 476it [30:46,  3.54s/it][A[A
cleaning sentances: 476it [30:48,  3.54s/it][ATokenizing tekst text into sentances: 476it [30:48,  3.54s/it]


reducing sentences to a single doc embeding: 477it [30:47,  2.85s/it][A[A[A

embedding all sents with BERT: 477it [30:47,  2.85s/it][A[A
cleaning sentances: 477it [30:49,  2.85s/it][ATokenizing tekst text into sentances: 477it [30:49,  2.85s/it]


reducing sentences to a single doc embeding: 478it [30:48,  2.23s/it][A[A[A

embedding all sents with BERT: 478it [30:48,  2.23s/it][A[A
cleaning sentances: 478it [30:50,  2.23s/it][ATokenizing tekst text into sentances: 478it [30:50,  2.23s/it]


reducing sentences to a single doc embeding: 479it [30:48,  1.76s/it][A[A[A

embedding all sents with BERT: 479it [30:48,  1.76s/it][A[A
cleaning sentances: 479it [30:51,  1.76s/it][ATokenizing tekst text into sentances: 479it [30:51,  1.76s/it]


reducing sentences to a single doc embeding: 480it [30:49,  1.35s/it][A[A[A

embedding all sents with BERT: 480it [30:49,  1.35s/it][A[A
cleaning sentances: 480it [30:51,  1.35s/it][ATokenizing tekst text into sentances: 480it [30:51,  1.35s/it]


reducing sentences to a single doc embeding: 481it [30:49,  1.04s/it][A[A[A

embedding all sents with BERT: 481it [30:49,  1.04s/it][A[A
cleaning sentances: 481it [30:51,  1.04s/it][ATokenizing tekst text into sentances: 481it [30:51,  1.04s/it]


reducing sentences to a single doc embeding: 482it [30:53,  2.01s/it][A[A[A

embedding all sents with BERT: 482it [30:53,  2.01s/it][A[A
cleaning sentances: 482it [30:56,  2.01s/it][ATokenizing tekst text into sentances: 482it [30:56,  2.01s/it]


reducing sentences to a single doc embeding: 483it [30:54,  1.52s/it][A[A[A

embedding all sents with BERT: 483it [30:54,  1.52s/it][A[A
cleaning sentances: 483it [30:56,  1.52s/it][ATokenizing tekst text into sentances: 483it [30:56,  1.52s/it]


reducing sentences to a single doc embeding: 484it [31:06,  4.63s/it][A[A[A

embedding all sents with BERT: 484it [31:06,  4.63s/it][A[A
cleaning sentances: 484it [31:08,  4.63s/it][ATokenizing tekst text into sentances: 484it [31:08,  4.63s/it]


reducing sentences to a single doc embeding: 485it [31:07,  3.53s/it][A[A[A

embedding all sents with BERT: 485it [31:07,  3.53s/it][A[A
cleaning sentances: 485it [31:09,  3.53s/it][ATokenizing tekst text into sentances: 485it [31:09,  3.53s/it]


reducing sentences to a single doc embeding: 486it [31:18,  6.04s/it][A[A[A

embedding all sents with BERT: 486it [31:18,  6.04s/it][A[A
cleaning sentances: 486it [31:21,  6.04s/it][ATokenizing tekst text into sentances: 486it [31:21,  6.04s/it]


reducing sentences to a single doc embeding: 487it [31:19,  4.39s/it][A[A[A

embedding all sents with BERT: 487it [31:19,  4.39s/it][A[A
cleaning sentances: 487it [31:21,  4.39s/it][ATokenizing tekst text into sentances: 487it [31:21,  4.39s/it]


reducing sentences to a single doc embeding: 488it [31:20,  3.38s/it][A[A[A

embedding all sents with BERT: 488it [31:20,  3.38s/it][A[A
cleaning sentances: 488it [31:22,  3.38s/it][ATokenizing tekst text into sentances: 488it [31:22,  3.38s/it]


reducing sentences to a single doc embeding: 489it [31:20,  2.48s/it][A[A[A

embedding all sents with BERT: 489it [31:20,  2.48s/it][A[A
cleaning sentances: 489it [31:23,  2.48s/it][ATokenizing tekst text into sentances: 489it [31:23,  2.48s/it]


reducing sentences to a single doc embeding: 490it [31:22,  2.32s/it][A[A[A

embedding all sents with BERT: 490it [31:22,  2.32s/it][A[A
cleaning sentances: 490it [31:25,  2.32s/it][ATokenizing tekst text into sentances: 490it [31:25,  2.32s/it]


reducing sentences to a single doc embeding: 491it [31:22,  1.66s/it][A[A[A

embedding all sents with BERT: 491it [31:22,  1.66s/it][A[A
cleaning sentances: 491it [31:25,  1.66s/it][ATokenizing tekst text into sentances: 491it [31:25,  1.66s/it]


reducing sentences to a single doc embeding: 492it [31:23,  1.33s/it][A[A[A

embedding all sents with BERT: 492it [31:23,  1.33s/it][A[A
cleaning sentances: 492it [31:25,  1.33s/it][ATokenizing tekst text into sentances: 492it [31:25,  1.33s/it]


reducing sentences to a single doc embeding: 493it [31:23,  1.02it/s][A[A[A

embedding all sents with BERT: 493it [31:23,  1.02it/s][A[A
cleaning sentances: 493it [31:26,  1.02it/s][ATokenizing tekst text into sentances: 493it [31:26,  1.02it/s]


reducing sentences to a single doc embeding: 494it [31:27,  1.87s/it][A[A[A

embedding all sents with BERT: 494it [31:27,  1.87s/it][A[A
cleaning sentances: 494it [31:30,  1.87s/it][ATokenizing tekst text into sentances: 494it [31:30,  1.87s/it]


reducing sentences to a single doc embeding: 495it [31:28,  1.51s/it][A[A[A

embedding all sents with BERT: 495it [31:28,  1.51s/it][A[A
cleaning sentances: 495it [31:30,  1.51s/it][ATokenizing tekst text into sentances: 495it [31:30,  1.51s/it]


reducing sentences to a single doc embeding: 496it [31:29,  1.30s/it][A[A[A

embedding all sents with BERT: 496it [31:29,  1.30s/it][A[A
cleaning sentances: 496it [31:31,  1.30s/it][ATokenizing tekst text into sentances: 496it [31:31,  1.30s/it]


reducing sentences to a single doc embeding: 497it [31:30,  1.23s/it][A[A[A

embedding all sents with BERT: 497it [31:30,  1.23s/it][A[A
cleaning sentances: 497it [31:32,  1.23s/it][ATokenizing tekst text into sentances: 497it [31:32,  1.23s/it]


reducing sentences to a single doc embeding: 498it [32:00,  9.92s/it][A[A[A

embedding all sents with BERT: 498it [32:00,  9.92s/it][A[A
cleaning sentances: 498it [32:02,  9.92s/it][ATokenizing tekst text into sentances: 498it [32:02,  9.92s/it]


reducing sentences to a single doc embeding: 499it [32:02,  7.48s/it][A[A[A

embedding all sents with BERT: 499it [32:02,  7.48s/it][A[A
cleaning sentances: 499it [32:04,  7.48s/it][ATokenizing tekst text into sentances: 499it [32:04,  7.48s/it]


reducing sentences to a single doc embeding: 500it [32:03,  5.52s/it][A[A[A

embedding all sents with BERT: 500it [32:03,  5.52s/it][A[A
cleaning sentances: 500it [32:05,  5.52s/it][ATokenizing tekst text into sentances: 500it [32:05,  5.52s/it]


reducing sentences to a single doc embeding: 501it [32:03,  4.06s/it][A[A[A

embedding all sents with BERT: 501it [32:03,  4.06s/it][A[A
cleaning sentances: 501it [32:06,  4.06s/it][ATokenizing tekst text into sentances: 501it [32:06,  4.06s/it]


reducing sentences to a single doc embeding: 502it [32:16,  6.72s/it][A[A[A

embedding all sents with BERT: 502it [32:16,  6.72s/it][A[A
cleaning sentances: 502it [32:19,  6.72s/it][ATokenizing tekst text into sentances: 502it [32:19,  6.72s/it]


reducing sentences to a single doc embeding: 503it [32:17,  4.84s/it][A[A[A

embedding all sents with BERT: 503it [32:17,  4.84s/it][A[A
cleaning sentances: 503it [32:19,  4.84s/it][ATokenizing tekst text into sentances: 503it [32:19,  4.84s/it]


reducing sentences to a single doc embeding: 504it [32:18,  3.77s/it][A[A[A

embedding all sents with BERT: 504it [32:18,  3.77s/it][A[A
cleaning sentances: 504it [32:20,  3.77s/it][ATokenizing tekst text into sentances: 504it [32:20,  3.77s/it]


reducing sentences to a single doc embeding: 505it [32:18,  2.80s/it][A[A[A

embedding all sents with BERT: 505it [32:18,  2.80s/it][A[A
cleaning sentances: 505it [32:21,  2.80s/it][ATokenizing tekst text into sentances: 505it [32:21,  2.80s/it]


reducing sentences to a single doc embeding: 506it [32:21,  2.58s/it][A[A[A

embedding all sents with BERT: 506it [32:21,  2.58s/it][A[A
cleaning sentances: 506it [32:23,  2.58s/it][ATokenizing tekst text into sentances: 506it [32:23,  2.58s/it]


reducing sentences to a single doc embeding: 507it [32:21,  1.89s/it][A[A[A

embedding all sents with BERT: 507it [32:21,  1.89s/it][A[A
cleaning sentances: 507it [32:23,  1.89s/it][ATokenizing tekst text into sentances: 507it [32:23,  1.89s/it]


reducing sentences to a single doc embeding: 508it [32:32,  4.71s/it][A[A[A

embedding all sents with BERT: 508it [32:32,  4.71s/it][A[A
cleaning sentances: 508it [32:34,  4.71s/it][ATokenizing tekst text into sentances: 508it [32:34,  4.71s/it]


reducing sentences to a single doc embeding: 509it [32:33,  3.50s/it][A[A[A

embedding all sents with BERT: 509it [32:33,  3.50s/it][A[A
cleaning sentances: 509it [32:35,  3.50s/it][ATokenizing tekst text into sentances: 509it [32:35,  3.50s/it]


reducing sentences to a single doc embeding: 510it [32:34,  2.89s/it][A[A[A

embedding all sents with BERT: 510it [32:34,  2.89s/it][A[A
cleaning sentances: 510it [32:37,  2.89s/it][ATokenizing tekst text into sentances: 510it [32:37,  2.89s/it]


reducing sentences to a single doc embeding: 511it [32:35,  2.22s/it][A[A[A

embedding all sents with BERT: 511it [32:35,  2.22s/it][A[A
cleaning sentances: 511it [32:37,  2.22s/it][ATokenizing tekst text into sentances: 511it [32:37,  2.22s/it]


reducing sentences to a single doc embeding: 512it [32:35,  1.67s/it][A[A[A

embedding all sents with BERT: 512it [32:35,  1.67s/it][A[A
cleaning sentances: 512it [32:38,  1.67s/it][ATokenizing tekst text into sentances: 512it [32:38,  1.67s/it]


reducing sentences to a single doc embeding: 513it [32:36,  1.27s/it][A[A[A

embedding all sents with BERT: 513it [32:36,  1.27s/it][A[A
cleaning sentances: 513it [32:38,  1.27s/it][ATokenizing tekst text into sentances: 513it [32:38,  1.27s/it]


reducing sentences to a single doc embeding: 514it [33:27, 16.25s/it][A[A[A

embedding all sents with BERT: 514it [33:27, 16.25s/it][A[A
cleaning sentances: 514it [33:29, 16.25s/it][ATokenizing tekst text into sentances: 514it [33:29, 16.25s/it]


reducing sentences to a single doc embeding: 515it [33:41, 15.61s/it][A[A[A

embedding all sents with BERT: 515it [33:41, 15.61s/it][A[A
cleaning sentances: 515it [33:43, 15.61s/it][ATokenizing tekst text into sentances: 515it [33:43, 15.61s/it]


reducing sentences to a single doc embeding: 516it [33:42, 11.16s/it][A[A[A

embedding all sents with BERT: 516it [33:42, 11.16s/it][A[A
cleaning sentances: 516it [33:44, 11.16s/it][ATokenizing tekst text into sentances: 516it [33:44, 11.16s/it]


reducing sentences to a single doc embeding: 517it [33:52, 10.97s/it][A[A[A

embedding all sents with BERT: 517it [33:52, 10.97s/it][A[A
cleaning sentances: 517it [33:55, 10.97s/it][ATokenizing tekst text into sentances: 517it [33:55, 10.97s/it]


reducing sentences to a single doc embeding: 518it [34:00, 10.12s/it][A[A[A

embedding all sents with BERT: 518it [34:00, 10.12s/it][A[A
cleaning sentances: 518it [34:03, 10.12s/it][ATokenizing tekst text into sentances: 518it [34:03, 10.12s/it]


reducing sentences to a single doc embeding: 519it [34:01,  7.26s/it][A[A[A

embedding all sents with BERT: 519it [34:01,  7.26s/it][A[A
cleaning sentances: 519it [34:03,  7.26s/it][ATokenizing tekst text into sentances: 519it [34:03,  7.26s/it]


reducing sentences to a single doc embeding: 520it [34:03,  5.67s/it][A[A[A

embedding all sents with BERT: 520it [34:03,  5.67s/it][A[A
cleaning sentances: 520it [34:05,  5.67s/it][ATokenizing tekst text into sentances: 520it [34:05,  5.67s/it]


reducing sentences to a single doc embeding: 521it [34:04,  4.32s/it][A[A[A

embedding all sents with BERT: 521it [34:04,  4.32s/it][A[A
cleaning sentances: 521it [34:06,  4.32s/it][ATokenizing tekst text into sentances: 521it [34:06,  4.32s/it]


reducing sentences to a single doc embeding: 522it [34:07,  4.02s/it][A[A[A

embedding all sents with BERT: 522it [34:07,  4.02s/it][A[A
cleaning sentances: 522it [34:10,  4.02s/it][ATokenizing tekst text into sentances: 522it [34:10,  4.02s/it]


reducing sentences to a single doc embeding: 523it [34:08,  2.96s/it][A[A[A

embedding all sents with BERT: 523it [34:08,  2.96s/it][A[A
cleaning sentances: 523it [34:10,  2.96s/it][ATokenizing tekst text into sentances: 523it [34:10,  2.96s/it]


reducing sentences to a single doc embeding: 524it [34:11,  2.90s/it][A[A[A

embedding all sents with BERT: 524it [34:11,  2.90s/it][A[A
cleaning sentances: 524it [34:13,  2.90s/it][ATokenizing tekst text into sentances: 524it [34:13,  2.90s/it]


reducing sentences to a single doc embeding: 525it [34:12,  2.47s/it][A[A[A

embedding all sents with BERT: 525it [34:12,  2.47s/it][A[A
cleaning sentances: 525it [34:15,  2.47s/it][ATokenizing tekst text into sentances: 525it [34:15,  2.47s/it]


reducing sentences to a single doc embeding: 526it [34:15,  2.64s/it][A[A[A

embedding all sents with BERT: 526it [34:15,  2.64s/it][A[A
cleaning sentances: 526it [34:18,  2.64s/it][ATokenizing tekst text into sentances: 526it [34:18,  2.64s/it]


reducing sentences to a single doc embeding: 527it [34:17,  2.39s/it][A[A[A

embedding all sents with BERT: 527it [34:17,  2.39s/it][A[A
cleaning sentances: 527it [34:19,  2.39s/it][ATokenizing tekst text into sentances: 527it [34:19,  2.39s/it]


reducing sentences to a single doc embeding: 528it [34:18,  1.98s/it][A[A[A

embedding all sents with BERT: 528it [34:18,  1.98s/it][A[A
cleaning sentances: 528it [34:20,  1.98s/it][ATokenizing tekst text into sentances: 528it [34:20,  1.98s/it]


reducing sentences to a single doc embeding: 529it [34:18,  1.53s/it][A[A[A

embedding all sents with BERT: 529it [34:18,  1.53s/it][A[A
cleaning sentances: 529it [34:21,  1.53s/it][ATokenizing tekst text into sentances: 529it [34:21,  1.53s/it]


reducing sentences to a single doc embeding: 530it [34:20,  1.42s/it][A[A[A

embedding all sents with BERT: 530it [34:20,  1.42s/it][A[A
cleaning sentances: 530it [34:22,  1.42s/it][ATokenizing tekst text into sentances: 530it [34:22,  1.42s/it]


reducing sentences to a single doc embeding: 531it [34:32,  4.81s/it][A[A[A

embedding all sents with BERT: 531it [34:32,  4.81s/it][A[A
cleaning sentances: 531it [34:35,  4.81s/it][ATokenizing tekst text into sentances: 531it [34:35,  4.81s/it]


reducing sentences to a single doc embeding: 532it [34:46,  7.45s/it][A[A[A

embedding all sents with BERT: 532it [34:46,  7.45s/it][A[A
cleaning sentances: 532it [34:48,  7.45s/it][ATokenizing tekst text into sentances: 532it [34:48,  7.45s/it]


reducing sentences to a single doc embeding: 533it [34:50,  6.56s/it][A[A[A

embedding all sents with BERT: 533it [34:50,  6.56s/it][A[A
cleaning sentances: 533it [34:53,  6.56s/it][ATokenizing tekst text into sentances: 533it [34:53,  6.56s/it]


reducing sentences to a single doc embeding: 534it [34:51,  4.86s/it][A[A[A

embedding all sents with BERT: 534it [34:51,  4.86s/it][A[A
cleaning sentances: 534it [34:54,  4.86s/it][ATokenizing tekst text into sentances: 534it [34:54,  4.86s/it]


reducing sentences to a single doc embeding: 535it [34:54,  4.27s/it][A[A[A

embedding all sents with BERT: 535it [34:54,  4.27s/it][A[A
cleaning sentances: 535it [34:57,  4.27s/it][ATokenizing tekst text into sentances: 535it [34:57,  4.27s/it]


reducing sentences to a single doc embeding: 536it [34:56,  3.48s/it][A[A[A

embedding all sents with BERT: 536it [34:56,  3.48s/it][A[A
cleaning sentances: 536it [34:58,  3.48s/it][ATokenizing tekst text into sentances: 536it [34:58,  3.48s/it]


reducing sentences to a single doc embeding: 537it [34:57,  2.91s/it][A[A[A

embedding all sents with BERT: 537it [34:57,  2.91s/it][A[A
cleaning sentances: 537it [35:00,  2.91s/it][ATokenizing tekst text into sentances: 537it [35:00,  2.91s/it]


reducing sentences to a single doc embeding: 538it [34:59,  2.45s/it][A[A[A

embedding all sents with BERT: 538it [34:59,  2.45s/it][A[A
cleaning sentances: 538it [35:01,  2.45s/it][ATokenizing tekst text into sentances: 538it [35:01,  2.45s/it]


reducing sentences to a single doc embeding: 539it [35:00,  2.09s/it][A[A[A

embedding all sents with BERT: 539it [35:00,  2.09s/it][A[A
cleaning sentances: 539it [35:02,  2.09s/it][ATokenizing tekst text into sentances: 539it [35:02,  2.09s/it]


reducing sentences to a single doc embeding: 540it [35:02,  1.94s/it][A[A[A

embedding all sents with BERT: 540it [35:02,  1.94s/it][A[A
cleaning sentances: 540it [35:04,  1.94s/it][ATokenizing tekst text into sentances: 540it [35:04,  1.94s/it]


reducing sentences to a single doc embeding: 541it [35:03,  1.72s/it][A[A[A

embedding all sents with BERT: 541it [35:03,  1.72s/it][A[A
cleaning sentances: 541it [35:05,  1.72s/it][ATokenizing tekst text into sentances: 541it [35:05,  1.72s/it]


reducing sentences to a single doc embeding: 542it [35:04,  1.40s/it][A[A[A

embedding all sents with BERT: 542it [35:04,  1.40s/it][A[A
cleaning sentances: 542it [35:06,  1.40s/it][ATokenizing tekst text into sentances: 542it [35:06,  1.40s/it]


reducing sentences to a single doc embeding: 543it [35:22,  6.43s/it][A[A[A

embedding all sents with BERT: 543it [35:22,  6.43s/it][A[A
cleaning sentances: 543it [35:24,  6.43s/it][ATokenizing tekst text into sentances: 543it [35:24,  6.43s/it]


reducing sentences to a single doc embeding: 544it [35:22,  4.69s/it][A[A[A

embedding all sents with BERT: 544it [35:22,  4.69s/it][A[A
cleaning sentances: 544it [35:25,  4.69s/it][ATokenizing tekst text into sentances: 544it [35:25,  4.69s/it]


reducing sentences to a single doc embeding: 545it [35:23,  3.39s/it][A[A[A

embedding all sents with BERT: 545it [35:23,  3.39s/it][A[A
cleaning sentances: 545it [35:25,  3.39s/it][ATokenizing tekst text into sentances: 545it [35:25,  3.39s/it]


reducing sentences to a single doc embeding: 546it [35:23,  2.54s/it][A[A[A

embedding all sents with BERT: 546it [35:23,  2.54s/it][A[A
cleaning sentances: 546it [35:26,  2.54s/it][ATokenizing tekst text into sentances: 546it [35:26,  2.54s/it]


reducing sentences to a single doc embeding: 547it [35:25,  2.44s/it][A[A[A

embedding all sents with BERT: 547it [35:25,  2.44s/it][A[A
cleaning sentances: 547it [35:28,  2.44s/it][ATokenizing tekst text into sentances: 547it [35:28,  2.44s/it]


reducing sentences to a single doc embeding: 548it [35:33,  3.99s/it][A[A[A

embedding all sents with BERT: 548it [35:33,  3.99s/it][A[A
cleaning sentances: 548it [35:35,  3.99s/it][ATokenizing tekst text into sentances: 548it [35:35,  3.99s/it]


reducing sentences to a single doc embeding: 549it [35:34,  2.98s/it][A[A[A

embedding all sents with BERT: 549it [35:34,  2.98s/it][A[A
cleaning sentances: 549it [35:36,  2.98s/it][ATokenizing tekst text into sentances: 549it [35:36,  2.98s/it]


reducing sentences to a single doc embeding: 550it [35:34,  2.30s/it][A[A[A

embedding all sents with BERT: 550it [35:34,  2.30s/it][A[A
cleaning sentances: 550it [35:37,  2.30s/it][ATokenizing tekst text into sentances: 550it [35:37,  2.30s/it]


reducing sentences to a single doc embeding: 551it [35:58,  8.54s/it][A[A[A

embedding all sents with BERT: 551it [35:58,  8.54s/it][A[A
cleaning sentances: 551it [36:00,  8.54s/it][ATokenizing tekst text into sentances: 551it [36:00,  8.54s/it]


reducing sentences to a single doc embeding: 552it [35:58,  6.19s/it][A[A[A

embedding all sents with BERT: 552it [35:58,  6.19s/it][A[A
cleaning sentances: 552it [36:01,  6.19s/it][ATokenizing tekst text into sentances: 552it [36:01,  6.19s/it]


reducing sentences to a single doc embeding: 553it [36:00,  4.80s/it][A[A[A

embedding all sents with BERT: 553it [36:00,  4.80s/it][A[A
cleaning sentances: 553it [36:02,  4.80s/it][ATokenizing tekst text into sentances: 553it [36:02,  4.80s/it]


reducing sentences to a single doc embeding: 554it [36:04,  4.63s/it][A[A[A

embedding all sents with BERT: 554it [36:04,  4.63s/it][A[A
cleaning sentances: 554it [36:06,  4.63s/it][ATokenizing tekst text into sentances: 554it [36:06,  4.63s/it]


reducing sentences to a single doc embeding: 555it [36:06,  3.80s/it][A[A[A

embedding all sents with BERT: 555it [36:06,  3.80s/it][A[A
cleaning sentances: 555it [36:08,  3.80s/it][ATokenizing tekst text into sentances: 555it [36:08,  3.80s/it]


reducing sentences to a single doc embeding: 556it [36:22,  7.49s/it][A[A[A

embedding all sents with BERT: 556it [36:22,  7.49s/it][A[A
cleaning sentances: 556it [36:24,  7.49s/it][ATokenizing tekst text into sentances: 556it [36:24,  7.49s/it]


reducing sentences to a single doc embeding: 557it [36:35,  9.11s/it][A[A[A

embedding all sents with BERT: 557it [36:35,  9.11s/it][A[A
cleaning sentances: 557it [36:37,  9.11s/it][ATokenizing tekst text into sentances: 557it [36:37,  9.11s/it]


reducing sentences to a single doc embeding: 558it [36:35,  6.53s/it][A[A[A

embedding all sents with BERT: 558it [36:35,  6.53s/it][A[A
cleaning sentances: 558it [36:38,  6.53s/it][ATokenizing tekst text into sentances: 558it [36:38,  6.53s/it]


reducing sentences to a single doc embeding: 559it [36:37,  5.18s/it][A[A[A

embedding all sents with BERT: 559it [36:37,  5.18s/it][A[A
cleaning sentances: 559it [36:40,  5.18s/it][ATokenizing tekst text into sentances: 559it [36:40,  5.18s/it]


reducing sentences to a single doc embeding: 560it [36:39,  4.15s/it][A[A[A

embedding all sents with BERT: 560it [36:39,  4.15s/it][A[A
cleaning sentances: 560it [36:42,  4.15s/it][ATokenizing tekst text into sentances: 560it [36:42,  4.15s/it]


reducing sentences to a single doc embeding: 561it [36:41,  3.33s/it][A[A[A

embedding all sents with BERT: 561it [36:41,  3.33s/it][A[A
cleaning sentances: 561it [36:43,  3.33s/it][ATokenizing tekst text into sentances: 561it [36:43,  3.33s/it]


reducing sentences to a single doc embeding: 562it [36:41,  2.52s/it][A[A[A

embedding all sents with BERT: 562it [36:41,  2.52s/it][A[A
cleaning sentances: 562it [36:44,  2.52s/it][ATokenizing tekst text into sentances: 562it [36:44,  2.52s/it]


reducing sentences to a single doc embeding: 563it [37:04,  8.49s/it][A[A[A

embedding all sents with BERT: 563it [37:04,  8.49s/it][A[A
cleaning sentances: 563it [37:06,  8.49s/it][ATokenizing tekst text into sentances: 563it [37:06,  8.49s/it]


reducing sentences to a single doc embeding: 564it [37:04,  6.09s/it][A[A[A

embedding all sents with BERT: 564it [37:04,  6.09s/it][A[A
cleaning sentances: 564it [37:06,  6.09s/it][ATokenizing tekst text into sentances: 564it [37:06,  6.09s/it]


reducing sentences to a single doc embeding: 565it [37:05,  4.64s/it][A[A[A

embedding all sents with BERT: 565it [37:05,  4.64s/it][A[A
cleaning sentances: 565it [37:08,  4.64s/it][ATokenizing tekst text into sentances: 565it [37:08,  4.64s/it]


reducing sentences to a single doc embeding: 566it [37:08,  4.07s/it][A[A[A

embedding all sents with BERT: 566it [37:08,  4.07s/it][A[A
cleaning sentances: 566it [37:10,  4.07s/it][ATokenizing tekst text into sentances: 566it [37:10,  4.07s/it]


reducing sentences to a single doc embeding: 567it [37:17,  5.49s/it][A[A[A

embedding all sents with BERT: 567it [37:17,  5.49s/it][A[A
cleaning sentances: 567it [37:19,  5.49s/it][ATokenizing tekst text into sentances: 567it [37:19,  5.49s/it]


reducing sentences to a single doc embeding: 568it [37:22,  5.22s/it][A[A[A

embedding all sents with BERT: 568it [37:22,  5.22s/it][A[A
cleaning sentances: 568it [37:24,  5.22s/it][ATokenizing tekst text into sentances: 568it [37:24,  5.22s/it]


reducing sentences to a single doc embeding: 569it [37:28,  5.61s/it][A[A[A

embedding all sents with BERT: 569it [37:28,  5.61s/it][A[A
cleaning sentances: 569it [37:30,  5.61s/it][ATokenizing tekst text into sentances: 569it [37:30,  5.61s/it]


reducing sentences to a single doc embeding: 570it [37:30,  4.54s/it][A[A[A

embedding all sents with BERT: 570it [37:30,  4.54s/it][A[A
cleaning sentances: 570it [37:32,  4.54s/it][ATokenizing tekst text into sentances: 570it [37:32,  4.54s/it]


reducing sentences to a single doc embeding: 571it [37:31,  3.43s/it][A[A[A

embedding all sents with BERT: 571it [37:31,  3.43s/it][A[A
cleaning sentances: 571it [37:33,  3.43s/it][ATokenizing tekst text into sentances: 571it [37:33,  3.43s/it]


reducing sentences to a single doc embeding: 572it [37:39,  4.73s/it][A[A[A

embedding all sents with BERT: 572it [37:39,  4.73s/it][A[A
cleaning sentances: 572it [37:41,  4.73s/it][ATokenizing tekst text into sentances: 572it [37:41,  4.73s/it]


reducing sentences to a single doc embeding: 573it [37:43,  4.74s/it][A[A[A

embedding all sents with BERT: 573it [37:43,  4.74s/it][A[A
cleaning sentances: 573it [37:46,  4.74s/it][ATokenizing tekst text into sentances: 573it [37:46,  4.74s/it]


reducing sentences to a single doc embeding: 574it [37:57,  7.43s/it][A[A[A

embedding all sents with BERT: 574it [37:57,  7.43s/it][A[A
cleaning sentances: 574it [38:00,  7.43s/it][ATokenizing tekst text into sentances: 574it [38:00,  7.43s/it]


reducing sentences to a single doc embeding: 575it [37:58,  5.58s/it][A[A[A

embedding all sents with BERT: 575it [37:58,  5.58s/it][A[A
cleaning sentances: 575it [38:01,  5.58s/it][ATokenizing tekst text into sentances: 575it [38:01,  5.58s/it]


reducing sentences to a single doc embeding: 576it [38:00,  4.26s/it][A[A[A

embedding all sents with BERT: 576it [38:00,  4.26s/it][A[A
cleaning sentances: 576it [38:02,  4.26s/it][ATokenizing tekst text into sentances: 576it [38:02,  4.26s/it]


reducing sentences to a single doc embeding: 577it [38:00,  3.15s/it][A[A[A

embedding all sents with BERT: 577it [38:00,  3.15s/it][A[A
cleaning sentances: 577it [38:02,  3.15s/it][ATokenizing tekst text into sentances: 577it [38:02,  3.15s/it]


reducing sentences to a single doc embeding: 578it [38:01,  2.39s/it][A[A[A

embedding all sents with BERT: 578it [38:01,  2.39s/it][A[A
cleaning sentances: 578it [38:03,  2.39s/it][ATokenizing tekst text into sentances: 578it [38:03,  2.39s/it]


reducing sentences to a single doc embeding: 579it [38:02,  2.00s/it][A[A[A

embedding all sents with BERT: 579it [38:02,  2.00s/it][A[A
cleaning sentances: 579it [38:04,  2.00s/it][ATokenizing tekst text into sentances: 579it [38:04,  2.00s/it]


reducing sentences to a single doc embeding: 580it [38:03,  1.63s/it][A[A[A

embedding all sents with BERT: 580it [38:03,  1.63s/it][A[A
cleaning sentances: 580it [38:05,  1.63s/it][ATokenizing tekst text into sentances: 580it [38:05,  1.63s/it]


reducing sentences to a single doc embeding: 581it [38:04,  1.62s/it][A[A[A

embedding all sents with BERT: 581it [38:04,  1.62s/it][A[A
cleaning sentances: 581it [38:07,  1.62s/it][ATokenizing tekst text into sentances: 581it [38:07,  1.62s/it]


reducing sentences to a single doc embeding: 582it [38:07,  2.03s/it][A[A[A

embedding all sents with BERT: 582it [38:07,  2.03s/it][A[A
cleaning sentances: 582it [38:10,  2.03s/it][ATokenizing tekst text into sentances: 582it [38:10,  2.03s/it]


reducing sentences to a single doc embeding: 583it [38:08,  1.70s/it][A[A[A

embedding all sents with BERT: 583it [38:08,  1.70s/it][A[A
cleaning sentances: 583it [38:10,  1.70s/it][ATokenizing tekst text into sentances: 583it [38:10,  1.70s/it]


reducing sentences to a single doc embeding: 584it [38:09,  1.52s/it][A[A[A

embedding all sents with BERT: 584it [38:09,  1.52s/it][A[A
cleaning sentances: 584it [38:12,  1.52s/it][ATokenizing tekst text into sentances: 584it [38:12,  1.52s/it]


reducing sentences to a single doc embeding: 585it [38:10,  1.36s/it][A[A[A

embedding all sents with BERT: 585it [38:10,  1.36s/it][A[A
cleaning sentances: 585it [38:13,  1.36s/it][ATokenizing tekst text into sentances: 585it [38:13,  1.36s/it]


reducing sentences to a single doc embeding: 586it [38:12,  1.51s/it][A[A[A

embedding all sents with BERT: 586it [38:12,  1.51s/it][A[A
cleaning sentances: 586it [38:14,  1.51s/it][ATokenizing tekst text into sentances: 586it [38:14,  1.51s/it]


reducing sentences to a single doc embeding: 587it [38:16,  2.38s/it][A[A[A

embedding all sents with BERT: 587it [38:16,  2.38s/it][A[A
cleaning sentances: 587it [38:19,  2.38s/it][ATokenizing tekst text into sentances: 587it [38:19,  2.38s/it]


reducing sentences to a single doc embeding: 588it [38:20,  2.64s/it][A[A[A

embedding all sents with BERT: 588it [38:20,  2.64s/it][A[A
cleaning sentances: 588it [38:22,  2.64s/it][ATokenizing tekst text into sentances: 588it [38:22,  2.64s/it]


reducing sentences to a single doc embeding: 589it [38:22,  2.60s/it][A[A[A

embedding all sents with BERT: 589it [38:22,  2.60s/it][A[A
cleaning sentances: 589it [38:25,  2.60s/it][ATokenizing tekst text into sentances: 589it [38:25,  2.60s/it]


reducing sentences to a single doc embeding: 590it [38:23,  2.01s/it][A[A[A

embedding all sents with BERT: 590it [38:23,  2.01s/it][A[A
cleaning sentances: 590it [38:25,  2.01s/it][ATokenizing tekst text into sentances: 590it [38:25,  2.01s/it]


reducing sentences to a single doc embeding: 591it [38:24,  1.87s/it][A[A[A

embedding all sents with BERT: 591it [38:24,  1.87s/it][A[A
cleaning sentances: 591it [38:27,  1.87s/it][ATokenizing tekst text into sentances: 591it [38:27,  1.87s/it]


reducing sentences to a single doc embeding: 592it [38:28,  2.43s/it][A[A[A

embedding all sents with BERT: 592it [38:28,  2.43s/it][A[A
cleaning sentances: 592it [38:31,  2.43s/it][ATokenizing tekst text into sentances: 592it [38:31,  2.43s/it]


reducing sentences to a single doc embeding: 593it [38:31,  2.64s/it][A[A[A

embedding all sents with BERT: 593it [38:31,  2.64s/it][A[A
cleaning sentances: 593it [38:34,  2.64s/it][ATokenizing tekst text into sentances: 593it [38:34,  2.64s/it]


reducing sentences to a single doc embeding: 594it [38:32,  2.16s/it][A[A[A

embedding all sents with BERT: 594it [38:32,  2.16s/it][A[A
cleaning sentances: 594it [38:35,  2.16s/it][ATokenizing tekst text into sentances: 594it [38:35,  2.16s/it]


reducing sentences to a single doc embeding: 595it [38:33,  1.85s/it][A[A[A

embedding all sents with BERT: 595it [38:33,  1.85s/it][A[A
cleaning sentances: 595it [38:36,  1.85s/it][ATokenizing tekst text into sentances: 595it [38:36,  1.85s/it]


reducing sentences to a single doc embeding: 596it [38:35,  1.65s/it][A[A[A

embedding all sents with BERT: 596it [38:35,  1.65s/it][A[A
cleaning sentances: 596it [38:37,  1.65s/it][ATokenizing tekst text into sentances: 596it [38:37,  1.65s/it]


reducing sentences to a single doc embeding: 597it [38:48,  5.12s/it][A[A[A

embedding all sents with BERT: 597it [38:48,  5.12s/it][A[A
cleaning sentances: 597it [38:50,  5.12s/it][ATokenizing tekst text into sentances: 597it [38:50,  5.12s/it]


reducing sentences to a single doc embeding: 598it [38:48,  3.72s/it][A[A[A

embedding all sents with BERT: 598it [38:48,  3.72s/it][A[A
cleaning sentances: 598it [38:51,  3.72s/it][ATokenizing tekst text into sentances: 598it [38:51,  3.72s/it]


reducing sentences to a single doc embeding: 599it [38:52,  3.70s/it][A[A[A

embedding all sents with BERT: 599it [38:52,  3.70s/it][A[A
cleaning sentances: 599it [38:54,  3.70s/it][ATokenizing tekst text into sentances: 599it [38:54,  3.70s/it]


reducing sentences to a single doc embeding: 600it [38:54,  3.14s/it][A[A[A

embedding all sents with BERT: 600it [38:54,  3.14s/it][A[A
cleaning sentances: 600it [38:56,  3.14s/it][ATokenizing tekst text into sentances: 600it [38:56,  3.14s/it]


reducing sentences to a single doc embeding: 601it [38:56,  2.90s/it][A[A[A

embedding all sents with BERT: 601it [38:56,  2.90s/it][A[A
cleaning sentances: 601it [38:58,  2.90s/it][ATokenizing tekst text into sentances: 601it [38:58,  2.90s/it]


reducing sentences to a single doc embeding: 602it [39:17,  8.30s/it][A[A[A

embedding all sents with BERT: 602it [39:17,  8.30s/it][A[A
cleaning sentances: 602it [39:19,  8.30s/it][ATokenizing tekst text into sentances: 602it [39:19,  8.30s/it]


reducing sentences to a single doc embeding: 603it [39:21,  7.06s/it][A[A[A

embedding all sents with BERT: 603it [39:21,  7.06s/it][A[A
cleaning sentances: 603it [39:24,  7.06s/it][ATokenizing tekst text into sentances: 603it [39:24,  7.06s/it]


reducing sentences to a single doc embeding: 604it [39:32,  8.11s/it][A[A[A

embedding all sents with BERT: 604it [39:32,  8.11s/it][A[A
cleaning sentances: 604it [39:34,  8.11s/it][ATokenizing tekst text into sentances: 604it [39:34,  8.11s/it]


reducing sentences to a single doc embeding: 605it [39:34,  6.30s/it][A[A[A

embedding all sents with BERT: 605it [39:34,  6.30s/it][A[A
cleaning sentances: 605it [39:36,  6.30s/it][ATokenizing tekst text into sentances: 605it [39:36,  6.30s/it]


reducing sentences to a single doc embeding: 606it [39:35,  4.70s/it][A[A[A

embedding all sents with BERT: 606it [39:35,  4.70s/it][A[A
cleaning sentances: 606it [39:37,  4.70s/it][ATokenizing tekst text into sentances: 606it [39:37,  4.70s/it]


reducing sentences to a single doc embeding: 607it [39:36,  3.61s/it][A[A[A

embedding all sents with BERT: 607it [39:36,  3.61s/it][A[A
cleaning sentances: 607it [39:38,  3.61s/it][ATokenizing tekst text into sentances: 607it [39:38,  3.61s/it]


reducing sentences to a single doc embeding: 608it [39:37,  2.81s/it][A[A[A

embedding all sents with BERT: 608it [39:37,  2.81s/it][A[A
cleaning sentances: 608it [39:39,  2.81s/it][ATokenizing tekst text into sentances: 608it [39:39,  2.81s/it]


reducing sentences to a single doc embeding: 609it [40:58, 26.33s/it][A[A[A

embedding all sents with BERT: 609it [40:58, 26.33s/it][A[A
cleaning sentances: 609it [41:00, 26.33s/it][ATokenizing tekst text into sentances: 609it [41:00, 26.33s/it]


reducing sentences to a single doc embeding: 610it [41:06, 20.79s/it][A[A[A

embedding all sents with BERT: 610it [41:06, 20.79s/it][A[A
cleaning sentances: 610it [41:08, 20.79s/it][ATokenizing tekst text into sentances: 610it [41:08, 20.79s/it]


reducing sentences to a single doc embeding: 611it [41:12, 16.29s/it][A[A[A

embedding all sents with BERT: 611it [41:12, 16.29s/it][A[A
cleaning sentances: 611it [41:14, 16.29s/it][ATokenizing tekst text into sentances: 611it [41:14, 16.29s/it]


reducing sentences to a single doc embeding: 612it [41:25, 15.46s/it][A[A[A

embedding all sents with BERT: 612it [41:25, 15.46s/it][A[A
cleaning sentances: 612it [41:28, 15.46s/it][ATokenizing tekst text into sentances: 612it [41:28, 15.46s/it]


reducing sentences to a single doc embeding: 613it [41:27, 11.37s/it][A[A[A

embedding all sents with BERT: 613it [41:27, 11.37s/it][A[A
cleaning sentances: 613it [41:29, 11.37s/it][ATokenizing tekst text into sentances: 613it [41:29, 11.37s/it]


reducing sentences to a single doc embeding: 614it [42:26, 25.79s/it][A[A[A

embedding all sents with BERT: 614it [42:26, 25.79s/it][A[A
cleaning sentances: 614it [42:29, 25.79s/it][ATokenizing tekst text into sentances: 614it [42:29, 25.79s/it]


reducing sentences to a single doc embeding: 615it [42:27, 18.26s/it][A[A[A

embedding all sents with BERT: 615it [42:27, 18.26s/it][A[A
cleaning sentances: 615it [42:30, 18.26s/it][ATokenizing tekst text into sentances: 615it [42:30, 18.26s/it]


reducing sentences to a single doc embeding: 616it [43:00, 22.51s/it][A[A[A

embedding all sents with BERT: 616it [43:00, 22.51s/it][A[A
cleaning sentances: 616it [43:02, 22.51s/it][ATokenizing tekst text into sentances: 616it [43:02, 22.51s/it]


reducing sentences to a single doc embeding: 617it [43:12, 19.58s/it][A[A[A

embedding all sents with BERT: 617it [43:12, 19.58s/it][A[A
cleaning sentances: 617it [43:15, 19.58s/it][ATokenizing tekst text into sentances: 617it [43:15, 19.58s/it]


reducing sentences to a single doc embeding: 618it [43:53, 25.87s/it][A[A[A

embedding all sents with BERT: 618it [43:53, 25.87s/it][A[A
cleaning sentances: 618it [43:55, 25.87s/it][ATokenizing tekst text into sentances: 618it [43:55, 25.87s/it]


reducing sentences to a single doc embeding: 619it [43:54, 18.37s/it][A[A[A

embedding all sents with BERT: 619it [43:54, 18.37s/it][A[A
cleaning sentances: 619it [43:56, 18.37s/it][ATokenizing tekst text into sentances: 619it [43:56, 18.37s/it]


reducing sentences to a single doc embeding: 620it [44:01, 14.90s/it][A[A[A

embedding all sents with BERT: 620it [44:01, 14.90s/it][A[A
cleaning sentances: 620it [44:03, 14.90s/it][ATokenizing tekst text into sentances: 620it [44:03, 14.90s/it]


reducing sentences to a single doc embeding: 621it [44:01, 10.71s/it][A[A[A

embedding all sents with BERT: 621it [44:01, 10.71s/it][A[A
cleaning sentances: 621it [44:04, 10.71s/it][ATokenizing tekst text into sentances: 621it [44:04, 10.71s/it]


reducing sentences to a single doc embeding: 622it [44:02,  7.74s/it][A[A[A

embedding all sents with BERT: 622it [44:02,  7.74s/it][A[A
cleaning sentances: 622it [44:05,  7.74s/it][ATokenizing tekst text into sentances: 622it [44:05,  7.74s/it]


reducing sentences to a single doc embeding: 623it [44:04,  5.80s/it][A[A[A

embedding all sents with BERT: 623it [44:04,  5.80s/it][A[A
cleaning sentances: 623it [44:06,  5.80s/it][ATokenizing tekst text into sentances: 623it [44:06,  5.80s/it]


reducing sentences to a single doc embeding: 624it [44:04,  4.33s/it][A[A[A

embedding all sents with BERT: 624it [44:04,  4.33s/it][A[A
cleaning sentances: 624it [44:07,  4.33s/it][ATokenizing tekst text into sentances: 624it [44:07,  4.33s/it]


reducing sentences to a single doc embeding: 625it [44:09,  4.29s/it][A[A[A

embedding all sents with BERT: 625it [44:09,  4.29s/it][A[A
cleaning sentances: 625it [44:11,  4.29s/it][ATokenizing tekst text into sentances: 625it [44:11,  4.29s/it]


reducing sentences to a single doc embeding: 626it [44:17,  5.52s/it][A[A[A

embedding all sents with BERT: 626it [44:17,  5.52s/it][A[A
cleaning sentances: 626it [44:19,  5.52s/it][ATokenizing tekst text into sentances: 626it [44:19,  5.52s/it]


reducing sentences to a single doc embeding: 627it [44:18,  4.21s/it][A[A[A

embedding all sents with BERT: 627it [44:18,  4.21s/it][A[A
cleaning sentances: 627it [44:21,  4.21s/it][ATokenizing tekst text into sentances: 627it [44:21,  4.21s/it]


reducing sentences to a single doc embeding: 628it [44:20,  3.45s/it][A[A[A

embedding all sents with BERT: 628it [44:20,  3.45s/it][A[A
cleaning sentances: 628it [44:22,  3.45s/it][ATokenizing tekst text into sentances: 628it [44:22,  3.45s/it]


reducing sentences to a single doc embeding: 629it [44:28,  4.71s/it][A[A[A

embedding all sents with BERT: 629it [44:28,  4.71s/it][A[A
cleaning sentances: 629it [44:30,  4.71s/it][ATokenizing tekst text into sentances: 629it [44:30,  4.71s/it]


reducing sentences to a single doc embeding: 630it [44:28,  3.44s/it][A[A[A

embedding all sents with BERT: 630it [44:28,  3.44s/it][A[A
cleaning sentances: 630it [44:30,  3.44s/it][ATokenizing tekst text into sentances: 630it [44:30,  3.44s/it]


reducing sentences to a single doc embeding: 631it [44:30,  3.03s/it][A[A[A

embedding all sents with BERT: 631it [44:30,  3.03s/it][A[A
cleaning sentances: 631it [44:32,  3.03s/it][ATokenizing tekst text into sentances: 631it [44:32,  3.03s/it]


reducing sentences to a single doc embeding: 632it [44:53,  8.98s/it][A[A[A

embedding all sents with BERT: 632it [44:53,  8.98s/it][A[A
cleaning sentances: 632it [44:55,  8.98s/it][ATokenizing tekst text into sentances: 632it [44:55,  8.98s/it]


reducing sentences to a single doc embeding: 633it [44:53,  6.40s/it][A[A[A

embedding all sents with BERT: 633it [44:53,  6.40s/it][A[A
cleaning sentances: 633it [44:56,  6.40s/it][ATokenizing tekst text into sentances: 633it [44:56,  6.40s/it]


reducing sentences to a single doc embeding: 634it [44:55,  4.85s/it][A[A[A

embedding all sents with BERT: 634it [44:55,  4.85s/it][A[A
cleaning sentances: 634it [44:57,  4.85s/it][ATokenizing tekst text into sentances: 634it [44:57,  4.85s/it]


reducing sentences to a single doc embeding: 635it [44:55,  3.60s/it][A[A[A

embedding all sents with BERT: 635it [44:55,  3.60s/it][A[A
cleaning sentances: 635it [44:58,  3.60s/it][ATokenizing tekst text into sentances: 635it [44:58,  3.60s/it]


reducing sentences to a single doc embeding: 636it [45:01,  4.19s/it][A[A[A

embedding all sents with BERT: 636it [45:01,  4.19s/it][A[A
cleaning sentances: 636it [45:03,  4.19s/it][ATokenizing tekst text into sentances: 636it [45:03,  4.19s/it]


reducing sentences to a single doc embeding: 637it [45:07,  4.87s/it][A[A[A

embedding all sents with BERT: 637it [45:07,  4.87s/it][A[A
cleaning sentances: 637it [45:10,  4.87s/it][ATokenizing tekst text into sentances: 637it [45:10,  4.87s/it]


reducing sentences to a single doc embeding: 638it [45:08,  3.77s/it][A[A[A

embedding all sents with BERT: 638it [45:08,  3.77s/it][A[A
cleaning sentances: 638it [45:11,  3.77s/it][ATokenizing tekst text into sentances: 638it [45:11,  3.77s/it]


reducing sentences to a single doc embeding: 639it [45:13,  3.99s/it][A[A[A

embedding all sents with BERT: 639it [45:13,  3.99s/it][A[A
cleaning sentances: 639it [45:15,  3.99s/it][ATokenizing tekst text into sentances: 639it [45:15,  3.99s/it]


reducing sentences to a single doc embeding: 640it [45:15,  3.40s/it][A[A[A

embedding all sents with BERT: 640it [45:15,  3.40s/it][A[A
cleaning sentances: 640it [45:17,  3.40s/it][ATokenizing tekst text into sentances: 640it [45:17,  3.40s/it]


reducing sentences to a single doc embeding: 641it [45:16,  2.61s/it][A[A[A

embedding all sents with BERT: 641it [45:16,  2.61s/it][A[A
cleaning sentances: 641it [45:18,  2.61s/it][ATokenizing tekst text into sentances: 641it [45:18,  2.61s/it]


reducing sentences to a single doc embeding: 642it [45:18,  2.47s/it][A[A[A

embedding all sents with BERT: 642it [45:18,  2.47s/it][A[A
cleaning sentances: 642it [45:20,  2.47s/it][ATokenizing tekst text into sentances: 642it [45:20,  2.47s/it]


reducing sentences to a single doc embeding: 643it [45:26,  4.28s/it][A[A[A

embedding all sents with BERT: 643it [45:26,  4.28s/it][A[A
cleaning sentances: 643it [45:29,  4.28s/it][ATokenizing tekst text into sentances: 643it [45:29,  4.28s/it]


reducing sentences to a single doc embeding: 644it [45:28,  3.36s/it][A[A[A

embedding all sents with BERT: 644it [45:28,  3.36s/it][A[A
cleaning sentances: 644it [45:30,  3.36s/it][ATokenizing tekst text into sentances: 644it [45:30,  3.36s/it]


reducing sentences to a single doc embeding: 645it [45:28,  2.45s/it][A[A[A

embedding all sents with BERT: 645it [45:28,  2.45s/it][A[A
cleaning sentances: 645it [45:30,  2.45s/it][ATokenizing tekst text into sentances: 645it [45:30,  2.45s/it]


reducing sentences to a single doc embeding: 646it [45:28,  1.81s/it][A[A[A

embedding all sents with BERT: 646it [45:28,  1.81s/it][A[A
cleaning sentances: 646it [45:31,  1.81s/it][ATokenizing tekst text into sentances: 646it [45:31,  1.81s/it]


reducing sentences to a single doc embeding: 647it [45:39,  4.50s/it][A[A[A

embedding all sents with BERT: 647it [45:39,  4.50s/it][A[A
cleaning sentances: 647it [45:41,  4.50s/it][ATokenizing tekst text into sentances: 647it [45:41,  4.50s/it]


reducing sentences to a single doc embeding: 648it [45:50,  6.49s/it][A[A[A

embedding all sents with BERT: 648it [45:50,  6.49s/it][A[A
cleaning sentances: 648it [45:53,  6.49s/it][ATokenizing tekst text into sentances: 648it [45:53,  6.49s/it]


reducing sentences to a single doc embeding: 649it [45:52,  5.23s/it][A[A[A

embedding all sents with BERT: 649it [45:52,  5.23s/it][A[A
cleaning sentances: 649it [45:55,  5.23s/it][ATokenizing tekst text into sentances: 649it [45:55,  5.23s/it]


reducing sentences to a single doc embeding: 650it [45:54,  4.25s/it][A[A[A

embedding all sents with BERT: 650it [45:54,  4.25s/it][A[A
cleaning sentances: 650it [45:57,  4.25s/it][ATokenizing tekst text into sentances: 650it [45:57,  4.25s/it]


reducing sentences to a single doc embeding: 651it [46:05,  6.28s/it][A[A[A

embedding all sents with BERT: 651it [46:05,  6.28s/it][A[A
cleaning sentances: 651it [46:08,  6.28s/it][ATokenizing tekst text into sentances: 651it [46:08,  6.28s/it]


reducing sentences to a single doc embeding: 652it [46:06,  4.56s/it][A[A[A

embedding all sents with BERT: 652it [46:06,  4.56s/it][A[A
cleaning sentances: 652it [46:08,  4.56s/it][ATokenizing tekst text into sentances: 652it [46:08,  4.56s/it]


reducing sentences to a single doc embeding: 653it [46:08,  3.84s/it][A[A[A

embedding all sents with BERT: 653it [46:08,  3.84s/it][A[A
cleaning sentances: 653it [46:11,  3.84s/it][ATokenizing tekst text into sentances: 653it [46:11,  3.84s/it]


reducing sentences to a single doc embeding: 654it [46:09,  2.93s/it][A[A[A

embedding all sents with BERT: 654it [46:09,  2.93s/it][A[A
cleaning sentances: 654it [46:11,  2.93s/it][ATokenizing tekst text into sentances: 654it [46:11,  2.93s/it]


reducing sentences to a single doc embeding: 655it [46:10,  2.46s/it][A[A[A

embedding all sents with BERT: 655it [46:10,  2.46s/it][A[A
cleaning sentances: 655it [46:13,  2.46s/it][ATokenizing tekst text into sentances: 655it [46:13,  2.46s/it]


reducing sentences to a single doc embeding: 656it [46:13,  2.45s/it][A[A[A

embedding all sents with BERT: 656it [46:13,  2.45s/it][A[A
cleaning sentances: 656it [46:15,  2.45s/it][ATokenizing tekst text into sentances: 656it [46:15,  2.45s/it]


reducing sentences to a single doc embeding: 657it [46:14,  1.95s/it][A[A[A

embedding all sents with BERT: 657it [46:14,  1.95s/it][A[A
cleaning sentances: 657it [46:16,  1.95s/it][ATokenizing tekst text into sentances: 657it [46:16,  1.95s/it]


reducing sentences to a single doc embeding: 658it [46:20,  3.35s/it][A[A[A

embedding all sents with BERT: 658it [46:20,  3.35s/it][A[A
cleaning sentances: 658it [46:23,  3.35s/it][ATokenizing tekst text into sentances: 658it [46:23,  3.35s/it]


reducing sentences to a single doc embeding: 659it [46:29,  4.98s/it][A[A[A

embedding all sents with BERT: 659it [46:29,  4.98s/it][A[A
cleaning sentances: 659it [46:31,  4.98s/it][ATokenizing tekst text into sentances: 659it [46:31,  4.98s/it]


reducing sentences to a single doc embeding: 660it [47:30, 21.90s/it][A[A[A

embedding all sents with BERT: 660it [47:30, 21.90s/it][A[A
cleaning sentances: 660it [47:33, 21.90s/it][ATokenizing tekst text into sentances: 660it [47:33, 21.90s/it]


reducing sentences to a single doc embeding: 661it [47:36, 16.89s/it][A[A[A

embedding all sents with BERT: 661it [47:36, 16.89s/it][A[A
cleaning sentances: 661it [47:38, 16.89s/it][ATokenizing tekst text into sentances: 661it [47:38, 16.89s/it]


reducing sentences to a single doc embeding: 662it [47:37, 12.36s/it][A[A[A

embedding all sents with BERT: 662it [47:37, 12.36s/it][A[A
cleaning sentances: 662it [47:40, 12.36s/it][ATokenizing tekst text into sentances: 662it [47:40, 12.36s/it]


reducing sentences to a single doc embeding: 663it [47:46, 11.31s/it][A[A[A

embedding all sents with BERT: 663it [47:46, 11.31s/it][A[A
cleaning sentances: 663it [47:49, 11.31s/it][ATokenizing tekst text into sentances: 663it [47:49, 11.31s/it]


reducing sentences to a single doc embeding: 664it [47:54, 10.13s/it][A[A[A

embedding all sents with BERT: 664it [47:54, 10.13s/it][A[A
cleaning sentances: 664it [47:56, 10.13s/it][ATokenizing tekst text into sentances: 664it [47:56, 10.13s/it]


reducing sentences to a single doc embeding: 665it [47:55,  7.41s/it][A[A[A

embedding all sents with BERT: 665it [47:55,  7.41s/it][A[A
cleaning sentances: 665it [47:57,  7.41s/it][ATokenizing tekst text into sentances: 665it [47:57,  7.41s/it]


reducing sentences to a single doc embeding: 666it [48:01,  6.97s/it][A[A[A

embedding all sents with BERT: 666it [48:01,  6.97s/it][A[A
cleaning sentances: 666it [48:03,  6.97s/it][ATokenizing tekst text into sentances: 666it [48:03,  6.97s/it]


reducing sentences to a single doc embeding: 667it [48:03,  5.48s/it][A[A[A

embedding all sents with BERT: 667it [48:03,  5.48s/it][A[A
cleaning sentances: 667it [48:05,  5.48s/it][ATokenizing tekst text into sentances: 667it [48:05,  5.48s/it]


reducing sentences to a single doc embeding: 668it [48:06,  4.79s/it][A[A[A

embedding all sents with BERT: 668it [48:06,  4.79s/it][A[A
cleaning sentances: 668it [48:08,  4.79s/it][ATokenizing tekst text into sentances: 668it [48:08,  4.79s/it]


reducing sentences to a single doc embeding: 669it [48:07,  3.63s/it][A[A[A

embedding all sents with BERT: 669it [48:07,  3.63s/it][A[A
cleaning sentances: 669it [48:09,  3.63s/it][ATokenizing tekst text into sentances: 669it [48:09,  3.63s/it]


reducing sentences to a single doc embeding: 670it [48:12,  4.11s/it][A[A[A

embedding all sents with BERT: 670it [48:12,  4.11s/it][A[A
cleaning sentances: 670it [48:14,  4.11s/it][ATokenizing tekst text into sentances: 670it [48:14,  4.11s/it]


reducing sentences to a single doc embeding: 671it [48:14,  3.47s/it][A[A[A

embedding all sents with BERT: 671it [48:14,  3.47s/it][A[A
cleaning sentances: 671it [48:16,  3.47s/it][ATokenizing tekst text into sentances: 671it [48:16,  3.47s/it]


reducing sentences to a single doc embeding: 672it [48:43, 11.12s/it][A[A[A

embedding all sents with BERT: 672it [48:43, 11.12s/it][A[A
cleaning sentances: 672it [48:45, 11.12s/it][ATokenizing tekst text into sentances: 672it [48:45, 11.12s/it]


reducing sentences to a single doc embeding: 673it [48:46,  8.83s/it][A[A[A

embedding all sents with BERT: 673it [48:46,  8.83s/it][A[A
cleaning sentances: 673it [48:49,  8.83s/it][ATokenizing tekst text into sentances: 673it [48:49,  8.83s/it]


reducing sentences to a single doc embeding: 674it [48:54,  8.63s/it][A[A[A

embedding all sents with BERT: 674it [48:54,  8.63s/it][A[A
cleaning sentances: 674it [48:57,  8.63s/it][ATokenizing tekst text into sentances: 674it [48:57,  8.63s/it]


reducing sentences to a single doc embeding: 675it [48:55,  6.21s/it][A[A[A

embedding all sents with BERT: 675it [48:55,  6.21s/it][A[A
cleaning sentances: 675it [48:57,  6.21s/it][ATokenizing tekst text into sentances: 675it [48:57,  6.21s/it]


reducing sentences to a single doc embeding: 676it [49:20, 11.88s/it][A[A[A

embedding all sents with BERT: 676it [49:20, 11.88s/it][A[A
cleaning sentances: 676it [49:22, 11.88s/it][ATokenizing tekst text into sentances: 676it [49:22, 11.88s/it]


reducing sentences to a single doc embeding: 677it [49:23,  9.05s/it][A[A[A

embedding all sents with BERT: 677it [49:23,  9.05s/it][A[A
cleaning sentances: 677it [49:25,  9.05s/it][ATokenizing tekst text into sentances: 677it [49:25,  9.05s/it]


reducing sentences to a single doc embeding: 678it [49:24,  6.70s/it][A[A[A

embedding all sents with BERT: 678it [49:24,  6.70s/it][A[A
cleaning sentances: 678it [49:26,  6.70s/it][ATokenizing tekst text into sentances: 678it [49:26,  6.70s/it]


reducing sentences to a single doc embeding: 679it [49:24,  4.87s/it][A[A[A

embedding all sents with BERT: 679it [49:24,  4.87s/it][A[A
cleaning sentances: 679it [49:27,  4.87s/it][ATokenizing tekst text into sentances: 679it [49:27,  4.87s/it]


reducing sentences to a single doc embeding: 680it [49:36,  6.96s/it][A[A[A

embedding all sents with BERT: 680it [49:36,  6.96s/it][A[A
cleaning sentances: 680it [49:39,  6.96s/it][ATokenizing tekst text into sentances: 680it [49:39,  6.96s/it]


reducing sentences to a single doc embeding: 681it [49:42,  6.59s/it][A[A[A

embedding all sents with BERT: 681it [49:42,  6.59s/it][A[A
cleaning sentances: 681it [49:44,  6.59s/it][ATokenizing tekst text into sentances: 681it [49:44,  6.59s/it]


reducing sentences to a single doc embeding: 682it [49:48,  6.50s/it][A[A[A

embedding all sents with BERT: 682it [49:48,  6.50s/it][A[A
cleaning sentances: 682it [49:51,  6.50s/it][ATokenizing tekst text into sentances: 682it [49:51,  6.50s/it]


reducing sentences to a single doc embeding: 683it [49:49,  4.69s/it][A[A[A

embedding all sents with BERT: 683it [49:49,  4.69s/it][A[A
cleaning sentances: 683it [49:51,  4.69s/it][ATokenizing tekst text into sentances: 683it [49:51,  4.69s/it]


reducing sentences to a single doc embeding: 684it [49:49,  3.46s/it][A[A[A

embedding all sents with BERT: 684it [49:49,  3.46s/it][A[A
cleaning sentances: 684it [49:52,  3.46s/it][ATokenizing tekst text into sentances: 684it [49:52,  3.46s/it]


reducing sentences to a single doc embeding: 685it [49:50,  2.51s/it][A[A[A

embedding all sents with BERT: 685it [49:50,  2.51s/it][A[A
cleaning sentances: 685it [49:52,  2.51s/it][ATokenizing tekst text into sentances: 685it [49:52,  2.51s/it]


reducing sentences to a single doc embeding: 686it [49:51,  2.06s/it][A[A[A

embedding all sents with BERT: 686it [49:51,  2.06s/it][A[A
cleaning sentances: 686it [49:53,  2.06s/it][ATokenizing tekst text into sentances: 686it [49:53,  2.06s/it]


reducing sentences to a single doc embeding: 687it [49:59,  4.07s/it][A[A[A

embedding all sents with BERT: 687it [49:59,  4.07s/it][A[A
cleaning sentances: 687it [50:02,  4.07s/it][ATokenizing tekst text into sentances: 687it [50:02,  4.07s/it]


reducing sentences to a single doc embeding: 688it [50:00,  3.07s/it][A[A[A

embedding all sents with BERT: 688it [50:00,  3.07s/it][A[A
cleaning sentances: 688it [50:02,  3.07s/it][ATokenizing tekst text into sentances: 688it [50:02,  3.07s/it]


reducing sentences to a single doc embeding: 689it [50:01,  2.38s/it][A[A[A

embedding all sents with BERT: 689it [50:01,  2.38s/it][A[A
cleaning sentances: 689it [50:03,  2.38s/it][ATokenizing tekst text into sentances: 689it [50:03,  2.38s/it]


reducing sentences to a single doc embeding: 690it [50:06,  3.17s/it][A[A[A

embedding all sents with BERT: 690it [50:06,  3.17s/it][A[A
cleaning sentances: 690it [50:08,  3.17s/it][ATokenizing tekst text into sentances: 690it [50:08,  3.17s/it]


reducing sentences to a single doc embeding: 691it [50:06,  2.31s/it][A[A[A

embedding all sents with BERT: 691it [50:06,  2.31s/it][A[A
cleaning sentances: 691it [50:09,  2.31s/it][ATokenizing tekst text into sentances: 691it [50:09,  2.31s/it]


reducing sentences to a single doc embeding: 692it [50:07,  1.80s/it][A[A[A

embedding all sents with BERT: 692it [50:07,  1.80s/it][A[A
cleaning sentances: 692it [50:09,  1.80s/it][ATokenizing tekst text into sentances: 692it [50:09,  1.80s/it]


reducing sentences to a single doc embeding: 693it [50:07,  1.47s/it][A[A[A

embedding all sents with BERT: 693it [50:07,  1.47s/it][A[A
cleaning sentances: 693it [50:10,  1.47s/it][ATokenizing tekst text into sentances: 693it [50:10,  1.47s/it]


reducing sentences to a single doc embeding: 694it [50:09,  1.61s/it][A[A[A

embedding all sents with BERT: 694it [50:09,  1.61s/it][A[A
cleaning sentances: 694it [50:12,  1.61s/it][ATokenizing tekst text into sentances: 694it [50:12,  1.61s/it]


reducing sentences to a single doc embeding: 695it [50:11,  1.49s/it][A[A[A

embedding all sents with BERT: 695it [50:11,  1.49s/it][A[A
cleaning sentances: 695it [50:13,  1.49s/it][ATokenizing tekst text into sentances: 695it [50:13,  1.49s/it]


reducing sentences to a single doc embeding: 696it [50:13,  1.67s/it][A[A[A

embedding all sents with BERT: 696it [50:13,  1.67s/it][A[A
cleaning sentances: 696it [50:15,  1.67s/it][ATokenizing tekst text into sentances: 696it [50:15,  1.67s/it]


reducing sentences to a single doc embeding: 697it [50:13,  1.33s/it][A[A[A

embedding all sents with BERT: 697it [50:13,  1.33s/it][A[A
cleaning sentances: 697it [50:16,  1.33s/it][ATokenizing tekst text into sentances: 697it [50:16,  1.33s/it]


reducing sentences to a single doc embeding: 698it [50:14,  1.23s/it][A[A[A

embedding all sents with BERT: 698it [50:14,  1.23s/it][A[A
cleaning sentances: 698it [50:17,  1.23s/it][ATokenizing tekst text into sentances: 698it [50:17,  1.23s/it]


reducing sentences to a single doc embeding: 699it [50:41,  8.83s/it][A[A[A

embedding all sents with BERT: 699it [50:41,  8.83s/it][A[A
cleaning sentances: 699it [50:43,  8.83s/it][ATokenizing tekst text into sentances: 699it [50:43,  8.83s/it]


reducing sentences to a single doc embeding: 700it [50:41,  6.28s/it][A[A[A

embedding all sents with BERT: 700it [50:41,  6.28s/it][A[A
cleaning sentances: 700it [50:44,  6.28s/it][ATokenizing tekst text into sentances: 700it [50:44,  6.28s/it]


reducing sentences to a single doc embeding: 701it [50:49,  6.76s/it][A[A[A

embedding all sents with BERT: 701it [50:49,  6.76s/it][A[A
cleaning sentances: 701it [50:51,  6.76s/it][ATokenizing tekst text into sentances: 701it [50:51,  6.76s/it]


reducing sentences to a single doc embeding: 702it [50:52,  5.67s/it][A[A[A

embedding all sents with BERT: 702it [50:52,  5.67s/it][A[A
cleaning sentances: 702it [50:55,  5.67s/it][ATokenizing tekst text into sentances: 702it [50:55,  5.67s/it]


reducing sentences to a single doc embeding: 703it [50:56,  5.09s/it][A[A[A

embedding all sents with BERT: 703it [50:56,  5.09s/it][A[A
cleaning sentances: 703it [50:58,  5.09s/it][ATokenizing tekst text into sentances: 703it [50:58,  5.09s/it]


reducing sentences to a single doc embeding: 704it [50:58,  4.10s/it][A[A[A

embedding all sents with BERT: 704it [50:58,  4.10s/it][A[A
cleaning sentances: 704it [51:00,  4.10s/it][ATokenizing tekst text into sentances: 704it [51:00,  4.10s/it]


reducing sentences to a single doc embeding: 705it [50:58,  3.11s/it][A[A[A

embedding all sents with BERT: 705it [50:58,  3.11s/it][A[A
cleaning sentances: 705it [51:01,  3.11s/it][ATokenizing tekst text into sentances: 705it [51:01,  3.11s/it]


reducing sentences to a single doc embeding: 706it [50:59,  2.25s/it][A[A[A

embedding all sents with BERT: 706it [50:59,  2.25s/it][A[A
cleaning sentances: 706it [51:01,  2.25s/it][ATokenizing tekst text into sentances: 706it [51:01,  2.25s/it]


reducing sentences to a single doc embeding: 707it [50:59,  1.68s/it][A[A[A

embedding all sents with BERT: 707it [50:59,  1.68s/it][A[A
cleaning sentances: 707it [51:01,  1.68s/it][ATokenizing tekst text into sentances: 707it [51:01,  1.68s/it]


reducing sentences to a single doc embeding: 708it [51:00,  1.53s/it][A[A[A

embedding all sents with BERT: 708it [51:00,  1.53s/it][A[A
cleaning sentances: 708it [51:03,  1.53s/it][ATokenizing tekst text into sentances: 708it [51:03,  1.53s/it]


reducing sentences to a single doc embeding: 709it [51:01,  1.23s/it][A[A[A

embedding all sents with BERT: 709it [51:01,  1.23s/it][A[A
cleaning sentances: 709it [51:03,  1.23s/it][ATokenizing tekst text into sentances: 709it [51:03,  1.23s/it]


reducing sentences to a single doc embeding: 710it [51:01,  1.09it/s][A[A[A

embedding all sents with BERT: 710it [51:01,  1.09it/s][A[A
cleaning sentances: 710it [51:03,  1.09it/s][ATokenizing tekst text into sentances: 710it [51:03,  1.09it/s]


reducing sentences to a single doc embeding: 711it [51:02,  1.13it/s][A[A[A

embedding all sents with BERT: 711it [51:02,  1.13it/s][A[A
cleaning sentances: 711it [51:04,  1.13it/s][ATokenizing tekst text into sentances: 711it [51:04,  1.13it/s]


reducing sentences to a single doc embeding: 712it [51:08,  2.54s/it][A[A[A

embedding all sents with BERT: 712it [51:08,  2.54s/it][A[A
cleaning sentances: 712it [51:11,  2.54s/it][ATokenizing tekst text into sentances: 712it [51:11,  2.54s/it]


reducing sentences to a single doc embeding: 713it [51:09,  1.89s/it][A[A[A

embedding all sents with BERT: 713it [51:09,  1.89s/it][A[A
cleaning sentances: 713it [51:11,  1.89s/it][ATokenizing tekst text into sentances: 713it [51:11,  1.89s/it]


reducing sentences to a single doc embeding: 714it [51:09,  1.51s/it][A[A[A

embedding all sents with BERT: 714it [51:09,  1.51s/it][A[A
cleaning sentances: 714it [51:12,  1.51s/it][ATokenizing tekst text into sentances: 714it [51:12,  1.51s/it]


reducing sentences to a single doc embeding: 715it [51:09,  1.13s/it][A[A[A

embedding all sents with BERT: 715it [51:09,  1.13s/it][A[A
cleaning sentances: 715it [51:12,  1.13s/it][ATokenizing tekst text into sentances: 715it [51:12,  1.13s/it]


reducing sentences to a single doc embeding: 716it [51:13,  1.77s/it][A[A[A

embedding all sents with BERT: 716it [51:13,  1.77s/it][A[A
cleaning sentances: 716it [51:15,  1.77s/it][ATokenizing tekst text into sentances: 716it [51:15,  1.77s/it]


reducing sentences to a single doc embeding: 717it [51:25,  5.06s/it][A[A[A

embedding all sents with BERT: 717it [51:25,  5.06s/it][A[A
cleaning sentances: 717it [51:28,  5.06s/it][ATokenizing tekst text into sentances: 717it [51:28,  5.06s/it]


reducing sentences to a single doc embeding: 718it [51:27,  4.07s/it][A[A[A

embedding all sents with BERT: 718it [51:27,  4.07s/it][A[A
cleaning sentances: 718it [51:30,  4.07s/it][ATokenizing tekst text into sentances: 718it [51:30,  4.07s/it]


reducing sentences to a single doc embeding: 719it [51:31,  3.98s/it][A[A[A

embedding all sents with BERT: 719it [51:31,  3.98s/it][A[A
cleaning sentances: 719it [51:33,  3.98s/it][ATokenizing tekst text into sentances: 719it [51:33,  3.98s/it]


reducing sentences to a single doc embeding: 720it [51:32,  3.23s/it][A[A[A

embedding all sents with BERT: 720it [51:32,  3.23s/it][A[A
cleaning sentances: 720it [51:35,  3.23s/it][ATokenizing tekst text into sentances: 720it [51:35,  3.23s/it]


reducing sentences to a single doc embeding: 721it [51:55,  8.90s/it][A[A[A

embedding all sents with BERT: 721it [51:55,  8.90s/it][A[A
cleaning sentances: 721it [51:57,  8.90s/it][ATokenizing tekst text into sentances: 721it [51:57,  8.90s/it]


reducing sentences to a single doc embeding: 722it [52:02,  8.49s/it][A[A[A

embedding all sents with BERT: 722it [52:02,  8.49s/it][A[A
cleaning sentances: 722it [52:04,  8.49s/it][ATokenizing tekst text into sentances: 722it [52:04,  8.49s/it]


reducing sentences to a single doc embeding: 723it [52:16, 10.19s/it][A[A[A

embedding all sents with BERT: 723it [52:16, 10.19s/it][A[A
cleaning sentances: 723it [52:19, 10.19s/it][ATokenizing tekst text into sentances: 723it [52:19, 10.19s/it]


reducing sentences to a single doc embeding: 724it [52:17,  7.25s/it][A[A[A

embedding all sents with BERT: 724it [52:17,  7.25s/it][A[A
cleaning sentances: 724it [52:19,  7.25s/it][ATokenizing tekst text into sentances: 724it [52:19,  7.25s/it]


reducing sentences to a single doc embeding: 725it [52:18,  5.62s/it][A[A[A

embedding all sents with BERT: 725it [52:18,  5.62s/it][A[A
cleaning sentances: 725it [52:21,  5.62s/it][ATokenizing tekst text into sentances: 725it [52:21,  5.62s/it]


reducing sentences to a single doc embeding: 726it [52:22,  4.92s/it][A[A[A

embedding all sents with BERT: 726it [52:22,  4.92s/it][A[A
cleaning sentances: 726it [52:24,  4.92s/it][ATokenizing tekst text into sentances: 726it [52:24,  4.92s/it]


reducing sentences to a single doc embeding: 727it [52:25,  4.47s/it][A[A[A

embedding all sents with BERT: 727it [52:25,  4.47s/it][A[A
cleaning sentances: 727it [52:28,  4.47s/it][ATokenizing tekst text into sentances: 727it [52:28,  4.47s/it]


reducing sentences to a single doc embeding: 728it [52:25,  3.22s/it][A[A[A

embedding all sents with BERT: 728it [52:25,  3.22s/it][A[A
cleaning sentances: 728it [52:28,  3.22s/it][ATokenizing tekst text into sentances: 728it [52:28,  3.22s/it]


reducing sentences to a single doc embeding: 729it [52:26,  2.34s/it][A[A[A

embedding all sents with BERT: 729it [52:26,  2.34s/it][A[A
cleaning sentances: 729it [52:28,  2.34s/it][ATokenizing tekst text into sentances: 729it [52:28,  2.34s/it]


reducing sentences to a single doc embeding: 730it [52:29,  2.59s/it][A[A[A

embedding all sents with BERT: 730it [52:29,  2.59s/it][A[A
cleaning sentances: 730it [52:31,  2.59s/it][ATokenizing tekst text into sentances: 730it [52:31,  2.59s/it]


reducing sentences to a single doc embeding: 731it [52:31,  2.49s/it][A[A[A

embedding all sents with BERT: 731it [52:31,  2.49s/it][A[A
cleaning sentances: 731it [52:34,  2.49s/it][ATokenizing tekst text into sentances: 731it [52:34,  2.49s/it]


reducing sentences to a single doc embeding: 732it [52:41,  4.83s/it][A[A[A

embedding all sents with BERT: 732it [52:41,  4.83s/it][A[A
cleaning sentances: 732it [52:44,  4.83s/it][ATokenizing tekst text into sentances: 732it [52:44,  4.83s/it]


reducing sentences to a single doc embeding: 733it [53:09, 11.72s/it][A[A[A

embedding all sents with BERT: 733it [53:09, 11.72s/it][A[A
cleaning sentances: 733it [53:12, 11.72s/it][ATokenizing tekst text into sentances: 733it [53:12, 11.72s/it]


reducing sentences to a single doc embeding: 734it [53:12,  9.11s/it][A[A[A

embedding all sents with BERT: 734it [53:12,  9.11s/it][A[A
cleaning sentances: 734it [53:15,  9.11s/it][ATokenizing tekst text into sentances: 734it [53:15,  9.11s/it]


reducing sentences to a single doc embeding: 735it [53:13,  6.68s/it][A[A[A

embedding all sents with BERT: 735it [53:13,  6.68s/it][A[A
cleaning sentances: 735it [53:16,  6.68s/it][ATokenizing tekst text into sentances: 735it [53:16,  6.68s/it]


reducing sentences to a single doc embeding: 736it [53:14,  4.79s/it][A[A[A

embedding all sents with BERT: 736it [53:14,  4.79s/it][A[A
cleaning sentances: 736it [53:16,  4.79s/it][ATokenizing tekst text into sentances: 736it [53:16,  4.79s/it]


reducing sentences to a single doc embeding: 737it [53:16,  3.92s/it][A[A[A

embedding all sents with BERT: 737it [53:16,  3.92s/it][A[A
cleaning sentances: 737it [53:18,  3.92s/it][ATokenizing tekst text into sentances: 737it [53:18,  3.92s/it]


reducing sentences to a single doc embeding: 738it [53:16,  2.82s/it][A[A[A

embedding all sents with BERT: 738it [53:16,  2.82s/it][A[A
cleaning sentances: 738it [53:18,  2.82s/it][ATokenizing tekst text into sentances: 738it [53:18,  2.82s/it]


reducing sentences to a single doc embeding: 739it [53:18,  2.52s/it][A[A[A

embedding all sents with BERT: 739it [53:18,  2.52s/it][A[A
cleaning sentances: 739it [53:20,  2.52s/it][ATokenizing tekst text into sentances: 739it [53:20,  2.52s/it]


reducing sentences to a single doc embeding: 740it [53:22,  3.05s/it][A[A[A

embedding all sents with BERT: 740it [53:22,  3.05s/it][A[A
cleaning sentances: 740it [53:24,  3.05s/it][ATokenizing tekst text into sentances: 740it [53:24,  3.05s/it]


reducing sentences to a single doc embeding: 741it [53:23,  2.31s/it][A[A[A

embedding all sents with BERT: 741it [53:23,  2.31s/it][A[A
cleaning sentances: 741it [53:25,  2.31s/it][ATokenizing tekst text into sentances: 741it [53:25,  2.31s/it]


reducing sentences to a single doc embeding: 742it [53:23,  1.76s/it][A[A[A

embedding all sents with BERT: 742it [53:23,  1.76s/it][A[A
cleaning sentances: 742it [53:25,  1.76s/it][ATokenizing tekst text into sentances: 742it [53:25,  1.76s/it]


reducing sentences to a single doc embeding: 743it [53:25,  1.71s/it][A[A[A

embedding all sents with BERT: 743it [53:25,  1.71s/it][A[A
cleaning sentances: 743it [53:27,  1.71s/it][ATokenizing tekst text into sentances: 743it [53:27,  1.71s/it]


reducing sentences to a single doc embeding: 744it [53:29,  2.39s/it][A[A[A

embedding all sents with BERT: 744it [53:29,  2.39s/it][A[A
cleaning sentances: 744it [53:31,  2.39s/it][ATokenizing tekst text into sentances: 744it [53:31,  2.39s/it]


reducing sentences to a single doc embeding: 745it [53:32,  2.70s/it][A[A[A

embedding all sents with BERT: 745it [53:32,  2.70s/it][A[A
cleaning sentances: 745it [53:34,  2.70s/it][ATokenizing tekst text into sentances: 745it [53:34,  2.70s/it]


reducing sentences to a single doc embeding: 746it [53:32,  1.97s/it][A[A[A

embedding all sents with BERT: 746it [53:32,  1.97s/it][A[A
cleaning sentances: 746it [53:35,  1.97s/it][ATokenizing tekst text into sentances: 746it [53:35,  1.97s/it]


reducing sentences to a single doc embeding: 747it [53:33,  1.48s/it][A[A[A

embedding all sents with BERT: 747it [53:33,  1.48s/it][A[A
cleaning sentances: 747it [53:35,  1.48s/it][ATokenizing tekst text into sentances: 747it [53:35,  1.48s/it]


reducing sentences to a single doc embeding: 748it [53:36,  2.08s/it][A[A[A

embedding all sents with BERT: 748it [53:36,  2.08s/it][A[A
cleaning sentances: 748it [53:38,  2.08s/it][ATokenizing tekst text into sentances: 748it [53:38,  2.08s/it]


reducing sentences to a single doc embeding: 749it [53:37,  1.69s/it][A[A[A

embedding all sents with BERT: 749it [53:37,  1.69s/it][A[A
cleaning sentances: 749it [53:39,  1.69s/it][ATokenizing tekst text into sentances: 749it [53:39,  1.69s/it]


reducing sentences to a single doc embeding: 750it [53:38,  1.45s/it][A[A[A

embedding all sents with BERT: 750it [53:38,  1.45s/it][A[A
cleaning sentances: 750it [53:40,  1.45s/it][ATokenizing tekst text into sentances: 750it [53:40,  1.45s/it]


reducing sentences to a single doc embeding: 751it [53:39,  1.39s/it][A[A[A

embedding all sents with BERT: 751it [53:39,  1.39s/it][A[A
cleaning sentances: 751it [53:41,  1.39s/it][ATokenizing tekst text into sentances: 751it [53:41,  1.39s/it]


reducing sentences to a single doc embeding: 752it [53:47,  3.29s/it][A[A[A

embedding all sents with BERT: 752it [53:47,  3.29s/it][A[A
cleaning sentances: 752it [53:49,  3.29s/it][ATokenizing tekst text into sentances: 752it [53:49,  3.29s/it]


reducing sentences to a single doc embeding: 753it [54:41, 18.43s/it][A[A[A

embedding all sents with BERT: 753it [54:41, 18.43s/it][A[A
cleaning sentances: 753it [54:43, 18.43s/it][ATokenizing tekst text into sentances: 753it [54:43, 18.43s/it]


reducing sentences to a single doc embeding: 754it [54:45, 14.19s/it][A[A[A

embedding all sents with BERT: 754it [54:45, 14.19s/it][A[A
cleaning sentances: 754it [54:47, 14.19s/it][ATokenizing tekst text into sentances: 754it [54:47, 14.19s/it]


reducing sentences to a single doc embeding: 755it [54:45, 10.13s/it][A[A[A

embedding all sents with BERT: 755it [54:45, 10.13s/it][A[A
cleaning sentances: 755it [54:48, 10.13s/it][ATokenizing tekst text into sentances: 755it [54:48, 10.13s/it]


reducing sentences to a single doc embeding: 756it [55:02, 11.96s/it][A[A[A

embedding all sents with BERT: 756it [55:02, 11.96s/it][A[A
cleaning sentances: 756it [55:04, 11.96s/it][ATokenizing tekst text into sentances: 756it [55:04, 11.96s/it]


reducing sentences to a single doc embeding: 757it [55:05,  9.30s/it][A[A[A

embedding all sents with BERT: 757it [55:05,  9.30s/it][A[A
cleaning sentances: 757it [55:07,  9.30s/it][ATokenizing tekst text into sentances: 757it [55:07,  9.30s/it]


reducing sentences to a single doc embeding: 758it [55:06,  7.03s/it][A[A[A

embedding all sents with BERT: 758it [55:06,  7.03s/it][A[A
cleaning sentances: 758it [55:09,  7.03s/it][ATokenizing tekst text into sentances: 758it [55:09,  7.03s/it]


reducing sentences to a single doc embeding: 759it [55:08,  5.36s/it][A[A[A

embedding all sents with BERT: 759it [55:08,  5.36s/it][A[A
cleaning sentances: 759it [55:10,  5.36s/it][ATokenizing tekst text into sentances: 759it [55:10,  5.36s/it]


reducing sentences to a single doc embeding: 760it [55:10,  4.36s/it][A[A[A

embedding all sents with BERT: 760it [55:10,  4.36s/it][A[A
cleaning sentances: 760it [55:12,  4.36s/it][ATokenizing tekst text into sentances: 760it [55:12,  4.36s/it]


reducing sentences to a single doc embeding: 761it [55:44, 13.11s/it][A[A[A

embedding all sents with BERT: 761it [55:44, 13.11s/it][A[A
cleaning sentances: 761it [55:46, 13.11s/it][ATokenizing tekst text into sentances: 761it [55:46, 13.11s/it]


reducing sentences to a single doc embeding: 762it [55:44,  9.36s/it][A[A[A

embedding all sents with BERT: 762it [55:44,  9.36s/it][A[A
cleaning sentances: 762it [55:46,  9.36s/it][ATokenizing tekst text into sentances: 762it [55:46,  9.36s/it]


reducing sentences to a single doc embeding: 763it [55:52,  8.97s/it][A[A[A

embedding all sents with BERT: 763it [55:52,  8.97s/it][A[A
cleaning sentances: 763it [55:55,  8.97s/it][ATokenizing tekst text into sentances: 763it [55:55,  8.97s/it]


reducing sentences to a single doc embeding: 764it [55:53,  6.48s/it][A[A[A

embedding all sents with BERT: 764it [55:53,  6.48s/it][A[A
cleaning sentances: 764it [55:55,  6.48s/it][ATokenizing tekst text into sentances: 764it [55:55,  6.48s/it]


reducing sentences to a single doc embeding: 765it [55:53,  4.60s/it][A[A[A

embedding all sents with BERT: 765it [55:53,  4.60s/it][A[A
cleaning sentances: 765it [55:55,  4.60s/it][ATokenizing tekst text into sentances: 765it [55:55,  4.60s/it]


reducing sentences to a single doc embeding: 766it [56:47, 19.36s/it][A[A[A

embedding all sents with BERT: 766it [56:47, 19.36s/it][A[A
cleaning sentances: 766it [56:49, 19.36s/it][ATokenizing tekst text into sentances: 766it [56:49, 19.36s/it]


reducing sentences to a single doc embeding: 767it [56:50, 14.47s/it][A[A[A

embedding all sents with BERT: 767it [56:50, 14.47s/it][A[A
cleaning sentances: 767it [56:52, 14.47s/it][ATokenizing tekst text into sentances: 767it [56:52, 14.47s/it]


reducing sentences to a single doc embeding: 768it [56:55, 11.77s/it][A[A[A

embedding all sents with BERT: 768it [56:55, 11.77s/it][A[A
cleaning sentances: 768it [56:58, 11.77s/it][ATokenizing tekst text into sentances: 768it [56:58, 11.77s/it]


reducing sentences to a single doc embeding: 769it [56:56,  8.45s/it][A[A[A

embedding all sents with BERT: 769it [56:56,  8.45s/it][A[A
cleaning sentances: 769it [56:58,  8.45s/it][ATokenizing tekst text into sentances: 769it [56:58,  8.45s/it]


reducing sentences to a single doc embeding: 770it [56:58,  6.39s/it][A[A[A

embedding all sents with BERT: 770it [56:58,  6.39s/it][A[A
cleaning sentances: 770it [57:00,  6.39s/it][ATokenizing tekst text into sentances: 770it [57:00,  6.39s/it]


reducing sentences to a single doc embeding: 771it [56:58,  4.65s/it][A[A[A

embedding all sents with BERT: 771it [56:58,  4.65s/it][A[A
cleaning sentances: 771it [57:01,  4.65s/it][ATokenizing tekst text into sentances: 771it [57:01,  4.65s/it]


reducing sentences to a single doc embeding: 772it [57:03,  4.73s/it][A[A[A

embedding all sents with BERT: 772it [57:03,  4.73s/it][A[A
cleaning sentances: 772it [57:06,  4.73s/it][ATokenizing tekst text into sentances: 772it [57:06,  4.73s/it]


reducing sentences to a single doc embeding: 773it [57:03,  3.35s/it][A[A[A

embedding all sents with BERT: 773it [57:03,  3.35s/it][A[A
cleaning sentances: 773it [57:06,  3.35s/it][ATokenizing tekst text into sentances: 773it [57:06,  3.35s/it]


reducing sentences to a single doc embeding: 774it [57:04,  2.53s/it][A[A[A

embedding all sents with BERT: 774it [57:04,  2.53s/it][A[A
cleaning sentances: 774it [57:06,  2.53s/it][ATokenizing tekst text into sentances: 774it [57:06,  2.53s/it]


reducing sentences to a single doc embeding: 775it [57:14,  4.68s/it][A[A[A

embedding all sents with BERT: 775it [57:14,  4.68s/it][A[A
cleaning sentances: 775it [57:16,  4.68s/it][ATokenizing tekst text into sentances: 775it [57:16,  4.68s/it]


reducing sentences to a single doc embeding: 776it [57:19,  4.99s/it][A[A[A

embedding all sents with BERT: 776it [57:19,  4.99s/it][A[A
cleaning sentances: 776it [57:22,  4.99s/it][ATokenizing tekst text into sentances: 776it [57:22,  4.99s/it]


reducing sentences to a single doc embeding: 777it [57:22,  4.24s/it][A[A[A

embedding all sents with BERT: 777it [57:22,  4.24s/it][A[A
cleaning sentances: 777it [57:24,  4.24s/it][ATokenizing tekst text into sentances: 777it [57:24,  4.24s/it]


reducing sentences to a single doc embeding: 778it [57:41,  8.69s/it][A[A[A

embedding all sents with BERT: 778it [57:41,  8.69s/it][A[A
cleaning sentances: 778it [57:43,  8.69s/it][ATokenizing tekst text into sentances: 778it [57:43,  8.69s/it]


reducing sentences to a single doc embeding: 779it [57:41,  6.23s/it][A[A[A

embedding all sents with BERT: 779it [57:41,  6.23s/it][A[A
cleaning sentances: 779it [57:44,  6.23s/it][ATokenizing tekst text into sentances: 779it [57:44,  6.23s/it]


reducing sentences to a single doc embeding: 780it [57:45,  5.31s/it][A[A[A

embedding all sents with BERT: 780it [57:45,  5.31s/it][A[A
cleaning sentances: 780it [57:47,  5.31s/it][ATokenizing tekst text into sentances: 780it [57:47,  5.31s/it]


reducing sentences to a single doc embeding: 781it [57:49,  5.02s/it][A[A[A

embedding all sents with BERT: 781it [57:49,  5.02s/it][A[A
cleaning sentances: 781it [57:51,  5.02s/it][ATokenizing tekst text into sentances: 781it [57:51,  5.02s/it]


reducing sentences to a single doc embeding: 782it [57:49,  3.68s/it][A[A[A

embedding all sents with BERT: 782it [57:49,  3.68s/it][A[A
cleaning sentances: 782it [57:52,  3.68s/it][ATokenizing tekst text into sentances: 782it [57:52,  3.68s/it]


reducing sentences to a single doc embeding: 783it [57:50,  2.71s/it][A[A[A

embedding all sents with BERT: 783it [57:50,  2.71s/it][A[A
cleaning sentances: 783it [57:52,  2.71s/it][ATokenizing tekst text into sentances: 783it [57:52,  2.71s/it]


reducing sentences to a single doc embeding: 784it [57:50,  2.02s/it][A[A[A

embedding all sents with BERT: 784it [57:50,  2.02s/it][A[A
cleaning sentances: 784it [57:53,  2.02s/it][ATokenizing tekst text into sentances: 784it [57:53,  2.02s/it]


reducing sentences to a single doc embeding: 785it [57:52,  1.97s/it][A[A[A

embedding all sents with BERT: 785it [57:52,  1.97s/it][A[A
cleaning sentances: 785it [57:55,  1.97s/it][ATokenizing tekst text into sentances: 785it [57:55,  1.97s/it]


reducing sentences to a single doc embeding: 786it [57:54,  1.97s/it][A[A[A

embedding all sents with BERT: 786it [57:54,  1.97s/it][A[A
cleaning sentances: 786it [57:56,  1.97s/it][ATokenizing tekst text into sentances: 786it [57:56,  1.97s/it]


reducing sentences to a single doc embeding: 787it [57:54,  1.44s/it][A[A[A

embedding all sents with BERT: 787it [57:54,  1.44s/it][A[A
cleaning sentances: 787it [57:57,  1.44s/it][ATokenizing tekst text into sentances: 787it [57:57,  1.44s/it]


reducing sentences to a single doc embeding: 788it [57:56,  1.60s/it][A[A[A

embedding all sents with BERT: 788it [57:56,  1.60s/it][A[A
cleaning sentances: 788it [57:59,  1.60s/it][ATokenizing tekst text into sentances: 788it [57:59,  1.60s/it]


reducing sentences to a single doc embeding: 789it [57:58,  1.50s/it][A[A[A

embedding all sents with BERT: 789it [57:58,  1.50s/it][A[A
cleaning sentances: 789it [58:00,  1.50s/it][ATokenizing tekst text into sentances: 789it [58:00,  1.50s/it]


reducing sentences to a single doc embeding: 790it [57:58,  1.32s/it][A[A[A

embedding all sents with BERT: 790it [57:58,  1.32s/it][A[A
cleaning sentances: 790it [58:01,  1.32s/it][ATokenizing tekst text into sentances: 790it [58:01,  1.32s/it]


reducing sentences to a single doc embeding: 791it [58:05,  2.96s/it][A[A[A

embedding all sents with BERT: 791it [58:05,  2.96s/it][A[A
cleaning sentances: 791it [58:08,  2.96s/it][ATokenizing tekst text into sentances: 791it [58:08,  2.96s/it]


reducing sentences to a single doc embeding: 792it [58:07,  2.60s/it][A[A[A

embedding all sents with BERT: 792it [58:07,  2.60s/it][A[A
cleaning sentances: 792it [58:09,  2.60s/it][ATokenizing tekst text into sentances: 792it [58:09,  2.60s/it]


reducing sentences to a single doc embeding: 793it [58:12,  3.45s/it][A[A[A

embedding all sents with BERT: 793it [58:12,  3.45s/it][A[A
cleaning sentances: 793it [58:15,  3.45s/it][ATokenizing tekst text into sentances: 793it [58:15,  3.45s/it]


reducing sentences to a single doc embeding: 794it [58:17,  3.85s/it][A[A[A

embedding all sents with BERT: 794it [58:17,  3.85s/it][A[A
cleaning sentances: 794it [58:20,  3.85s/it][ATokenizing tekst text into sentances: 794it [58:20,  3.85s/it]


reducing sentences to a single doc embeding: 795it [58:18,  2.86s/it][A[A[A

embedding all sents with BERT: 795it [58:18,  2.86s/it][A[A
cleaning sentances: 795it [58:20,  2.86s/it][ATokenizing tekst text into sentances: 795it [58:20,  2.86s/it]


reducing sentences to a single doc embeding: 796it [58:25,  4.18s/it][A[A[A

embedding all sents with BERT: 796it [58:25,  4.18s/it][A[A
cleaning sentances: 796it [58:27,  4.18s/it][ATokenizing tekst text into sentances: 796it [58:27,  4.18s/it]


reducing sentences to a single doc embeding: 797it [58:27,  3.58s/it][A[A[A

embedding all sents with BERT: 797it [58:27,  3.58s/it][A[A
cleaning sentances: 797it [58:30,  3.58s/it][ATokenizing tekst text into sentances: 797it [58:30,  3.58s/it]


reducing sentences to a single doc embeding: 798it [58:28,  2.85s/it][A[A[A

embedding all sents with BERT: 798it [58:28,  2.85s/it][A[A
cleaning sentances: 798it [58:31,  2.85s/it][ATokenizing tekst text into sentances: 798it [58:31,  2.85s/it]


reducing sentences to a single doc embeding: 799it [58:29,  2.19s/it][A[A[A

embedding all sents with BERT: 799it [58:29,  2.19s/it][A[A
cleaning sentances: 799it [58:31,  2.19s/it][ATokenizing tekst text into sentances: 799it [58:31,  2.19s/it]


reducing sentences to a single doc embeding: 800it [58:30,  1.82s/it][A[A[A

embedding all sents with BERT: 800it [58:30,  1.82s/it][A[A
cleaning sentances: 800it [58:32,  1.82s/it][ATokenizing tekst text into sentances: 800it [58:32,  1.82s/it]


reducing sentences to a single doc embeding: 801it [58:31,  1.54s/it][A[A[A

embedding all sents with BERT: 801it [58:31,  1.54s/it][A[A
cleaning sentances: 801it [58:33,  1.54s/it][ATokenizing tekst text into sentances: 801it [58:33,  1.54s/it]


reducing sentences to a single doc embeding: 802it [58:33,  1.83s/it][A[A[A

embedding all sents with BERT: 802it [58:33,  1.83s/it][A[A
cleaning sentances: 802it [58:36,  1.83s/it][ATokenizing tekst text into sentances: 802it [58:36,  1.83s/it]


reducing sentences to a single doc embeding: 803it [59:12, 12.78s/it][A[A[A

embedding all sents with BERT: 803it [59:12, 12.78s/it][A[A
cleaning sentances: 803it [59:14, 12.78s/it][ATokenizing tekst text into sentances: 803it [59:14, 12.78s/it]


reducing sentences to a single doc embeding: 804it [59:14,  9.49s/it][A[A[A

embedding all sents with BERT: 804it [59:14,  9.49s/it][A[A
cleaning sentances: 804it [59:16,  9.49s/it][ATokenizing tekst text into sentances: 804it [59:16,  9.49s/it]


reducing sentences to a single doc embeding: 805it [59:15,  7.08s/it][A[A[A

embedding all sents with BERT: 805it [59:15,  7.08s/it][A[A
cleaning sentances: 805it [59:17,  7.08s/it][ATokenizing tekst text into sentances: 805it [59:17,  7.08s/it]


reducing sentences to a single doc embeding: 806it [59:15,  5.07s/it][A[A[A

embedding all sents with BERT: 806it [59:15,  5.07s/it][A[A
cleaning sentances: 806it [59:18,  5.07s/it][ATokenizing tekst text into sentances: 806it [59:18,  5.07s/it]


reducing sentences to a single doc embeding: 807it [59:18,  4.48s/it][A[A[A

embedding all sents with BERT: 807it [59:18,  4.48s/it][A[A
cleaning sentances: 807it [59:21,  4.48s/it][ATokenizing tekst text into sentances: 807it [59:21,  4.48s/it]


reducing sentences to a single doc embeding: 808it [59:22,  4.26s/it][A[A[A

embedding all sents with BERT: 808it [59:22,  4.26s/it][A[A
cleaning sentances: 808it [59:25,  4.26s/it][ATokenizing tekst text into sentances: 808it [59:25,  4.26s/it]


reducing sentences to a single doc embeding: 809it [59:25,  3.95s/it][A[A[A

embedding all sents with BERT: 809it [59:25,  3.95s/it][A[A
cleaning sentances: 809it [59:28,  3.95s/it][ATokenizing tekst text into sentances: 809it [59:28,  3.95s/it]


reducing sentences to a single doc embeding: 810it [59:37,  6.28s/it][A[A[A

embedding all sents with BERT: 810it [59:37,  6.28s/it][A[A
cleaning sentances: 810it [59:40,  6.28s/it][ATokenizing tekst text into sentances: 810it [59:40,  6.28s/it]


reducing sentences to a single doc embeding: 811it [1:00:03, 12.28s/it][A[A[A

embedding all sents with BERT: 811it [1:00:03, 12.28s/it][A[A
cleaning sentances: 811it [1:00:06, 12.28s/it][ATokenizing tekst text into sentances: 811it [1:00:06, 12.28s/it]


reducing sentences to a single doc embeding: 812it [1:00:11, 10.95s/it][A[A[A

embedding all sents with BERT: 812it [1:00:11, 10.95s/it][A[A
cleaning sentances: 812it [1:00:14, 10.95s/it][ATokenizing tekst text into sentances: 812it [1:00:14, 10.95s/it]


reducing sentences to a single doc embeding: 813it [1:00:16,  9.15s/it][A[A[A

embedding all sents with BERT: 813it [1:00:16,  9.15s/it][A[A
cleaning sentances: 813it [1:00:19,  9.15s/it][ATokenizing tekst text into sentances: 813it [1:00:19,  9.15s/it]


reducing sentences to a single doc embeding: 814it [1:00:17,  6.52s/it][A[A[A

embedding all sents with BERT: 814it [1:00:17,  6.52s/it][A[A
cleaning sentances: 814it [1:00:19,  6.52s/it][ATokenizing tekst text into sentances: 814it [1:00:19,  6.52s/it]


reducing sentences to a single doc embeding: 815it [1:00:18,  5.06s/it][A[A[A

embedding all sents with BERT: 815it [1:00:18,  5.06s/it][A[A
cleaning sentances: 815it [1:00:21,  5.06s/it][ATokenizing tekst text into sentances: 815it [1:00:21,  5.06s/it]


reducing sentences to a single doc embeding: 816it [1:00:19,  3.80s/it][A[A[A

embedding all sents with BERT: 816it [1:00:19,  3.80s/it][A[A
cleaning sentances: 816it [1:00:21,  3.80s/it][ATokenizing tekst text into sentances: 816it [1:00:21,  3.80s/it]


reducing sentences to a single doc embeding: 817it [1:00:20,  2.82s/it][A[A[A

embedding all sents with BERT: 817it [1:00:20,  2.82s/it][A[A
cleaning sentances: 817it [1:00:22,  2.82s/it][ATokenizing tekst text into sentances: 817it [1:00:22,  2.82s/it]


reducing sentences to a single doc embeding: 818it [1:00:21,  2.23s/it][A[A[A

embedding all sents with BERT: 818it [1:00:21,  2.23s/it][A[A
cleaning sentances: 818it [1:00:23,  2.23s/it][ATokenizing tekst text into sentances: 818it [1:00:23,  2.23s/it]


reducing sentences to a single doc embeding: 819it [1:00:21,  1.81s/it][A[A[A

embedding all sents with BERT: 819it [1:00:21,  1.81s/it][A[A
cleaning sentances: 819it [1:00:24,  1.81s/it][ATokenizing tekst text into sentances: 819it [1:00:24,  1.81s/it]


reducing sentences to a single doc embeding: 820it [1:00:23,  1.79s/it][A[A[A

embedding all sents with BERT: 820it [1:00:23,  1.79s/it][A[A
cleaning sentances: 820it [1:00:25,  1.79s/it][ATokenizing tekst text into sentances: 820it [1:00:25,  1.79s/it]


reducing sentences to a single doc embeding: 821it [1:00:24,  1.37s/it][A[A[A

embedding all sents with BERT: 821it [1:00:24,  1.37s/it][A[A
cleaning sentances: 821it [1:00:26,  1.37s/it][ATokenizing tekst text into sentances: 821it [1:00:26,  1.37s/it]


reducing sentences to a single doc embeding: 822it [1:00:29,  2.70s/it][A[A[A

embedding all sents with BERT: 822it [1:00:29,  2.70s/it][A[A
cleaning sentances: 822it [1:00:32,  2.70s/it][ATokenizing tekst text into sentances: 822it [1:00:32,  2.70s/it]


reducing sentences to a single doc embeding: 823it [1:00:31,  2.43s/it][A[A[A

embedding all sents with BERT: 823it [1:00:31,  2.43s/it][A[A
cleaning sentances: 823it [1:00:33,  2.43s/it][ATokenizing tekst text into sentances: 823it [1:00:33,  2.43s/it]


reducing sentences to a single doc embeding: 824it [1:00:37,  3.46s/it][A[A[A

embedding all sents with BERT: 824it [1:00:37,  3.46s/it][A[A
cleaning sentances: 824it [1:00:39,  3.46s/it][ATokenizing tekst text into sentances: 824it [1:00:39,  3.46s/it]


reducing sentences to a single doc embeding: 825it [1:00:46,  5.27s/it][A[A[A

embedding all sents with BERT: 825it [1:00:46,  5.27s/it][A[A
cleaning sentances: 825it [1:00:49,  5.27s/it][ATokenizing tekst text into sentances: 825it [1:00:49,  5.27s/it]


reducing sentences to a single doc embeding: 826it [1:00:49,  4.56s/it][A[A[A

embedding all sents with BERT: 826it [1:00:49,  4.56s/it][A[A
cleaning sentances: 826it [1:00:52,  4.56s/it][ATokenizing tekst text into sentances: 826it [1:00:52,  4.56s/it]


reducing sentences to a single doc embeding: 827it [1:00:50,  3.38s/it][A[A[A

embedding all sents with BERT: 827it [1:00:50,  3.38s/it][A[A
cleaning sentances: 827it [1:00:52,  3.38s/it][ATokenizing tekst text into sentances: 827it [1:00:52,  3.38s/it]


reducing sentences to a single doc embeding: 828it [1:00:51,  2.56s/it][A[A[A

embedding all sents with BERT: 828it [1:00:51,  2.56s/it][A[A
cleaning sentances: 828it [1:00:53,  2.56s/it][ATokenizing tekst text into sentances: 828it [1:00:53,  2.56s/it]


reducing sentences to a single doc embeding: 829it [1:00:53,  2.64s/it][A[A[A

embedding all sents with BERT: 829it [1:00:53,  2.64s/it][A[A
cleaning sentances: 829it [1:00:56,  2.64s/it][ATokenizing tekst text into sentances: 829it [1:00:56,  2.64s/it]


reducing sentences to a single doc embeding: 830it [1:00:56,  2.55s/it][A[A[A

embedding all sents with BERT: 830it [1:00:56,  2.55s/it][A[A
cleaning sentances: 830it [1:00:58,  2.55s/it][ATokenizing tekst text into sentances: 830it [1:00:58,  2.55s/it]


reducing sentences to a single doc embeding: 831it [1:00:57,  2.12s/it][A[A[A

embedding all sents with BERT: 831it [1:00:57,  2.12s/it][A[A
cleaning sentances: 831it [1:00:59,  2.12s/it][ATokenizing tekst text into sentances: 831it [1:00:59,  2.12s/it]


reducing sentences to a single doc embeding: 832it [1:01:06,  4.16s/it][A[A[A

embedding all sents with BERT: 832it [1:01:06,  4.16s/it][A[A
cleaning sentances: 832it [1:01:08,  4.16s/it][ATokenizing tekst text into sentances: 832it [1:01:08,  4.16s/it]


reducing sentences to a single doc embeding: 833it [1:01:08,  3.43s/it][A[A[A

embedding all sents with BERT: 833it [1:01:08,  3.43s/it][A[A
cleaning sentances: 833it [1:01:10,  3.43s/it][ATokenizing tekst text into sentances: 833it [1:01:10,  3.43s/it]


reducing sentences to a single doc embeding: 834it [1:01:13,  4.07s/it][A[A[A

embedding all sents with BERT: 834it [1:01:13,  4.07s/it][A[A
cleaning sentances: 834it [1:01:15,  4.07s/it][ATokenizing tekst text into sentances: 834it [1:01:15,  4.07s/it]


reducing sentences to a single doc embeding: 835it [1:01:16,  3.59s/it][A[A[A

embedding all sents with BERT: 835it [1:01:16,  3.59s/it][A[A
cleaning sentances: 835it [1:01:18,  3.59s/it][ATokenizing tekst text into sentances: 835it [1:01:18,  3.59s/it]


reducing sentences to a single doc embeding: 836it [1:01:17,  3.04s/it][A[A[A

embedding all sents with BERT: 836it [1:01:17,  3.04s/it][A[A
cleaning sentances: 836it [1:01:20,  3.04s/it][ATokenizing tekst text into sentances: 836it [1:01:20,  3.04s/it]


reducing sentences to a single doc embeding: 837it [1:01:21,  3.28s/it][A[A[A

embedding all sents with BERT: 837it [1:01:21,  3.28s/it][A[A
cleaning sentances: 837it [1:01:24,  3.28s/it][ATokenizing tekst text into sentances: 837it [1:01:24,  3.28s/it]


reducing sentences to a single doc embeding: 838it [1:01:25,  3.52s/it][A[A[A

embedding all sents with BERT: 838it [1:01:25,  3.52s/it][A[A
cleaning sentances: 838it [1:01:28,  3.52s/it][ATokenizing tekst text into sentances: 838it [1:01:28,  3.52s/it]


reducing sentences to a single doc embeding: 839it [1:01:29,  3.64s/it][A[A[A

embedding all sents with BERT: 839it [1:01:29,  3.64s/it][A[A
cleaning sentances: 839it [1:01:32,  3.64s/it][ATokenizing tekst text into sentances: 839it [1:01:32,  3.64s/it]


reducing sentences to a single doc embeding: 840it [1:01:33,  3.57s/it][A[A[A

embedding all sents with BERT: 840it [1:01:33,  3.57s/it][A[A
cleaning sentances: 840it [1:01:35,  3.57s/it][ATokenizing tekst text into sentances: 840it [1:01:35,  3.57s/it]


reducing sentences to a single doc embeding: 841it [1:01:33,  2.69s/it][A[A[A

embedding all sents with BERT: 841it [1:01:33,  2.69s/it][A[A
cleaning sentances: 841it [1:01:36,  2.69s/it][ATokenizing tekst text into sentances: 841it [1:01:36,  2.69s/it]


reducing sentences to a single doc embeding: 842it [1:01:40,  3.92s/it][A[A[A

embedding all sents with BERT: 842it [1:01:40,  3.92s/it][A[A
cleaning sentances: 842it [1:01:42,  3.92s/it][ATokenizing tekst text into sentances: 842it [1:01:42,  3.92s/it]


reducing sentences to a single doc embeding: 843it [1:01:40,  2.81s/it][A[A[A

embedding all sents with BERT: 843it [1:01:40,  2.81s/it][A[A
cleaning sentances: 843it [1:01:43,  2.81s/it][ATokenizing tekst text into sentances: 843it [1:01:43,  2.81s/it]


reducing sentences to a single doc embeding: 844it [1:01:41,  2.12s/it][A[A[A

embedding all sents with BERT: 844it [1:01:41,  2.12s/it][A[A
cleaning sentances: 844it [1:01:43,  2.12s/it][ATokenizing tekst text into sentances: 844it [1:01:43,  2.12s/it]


reducing sentences to a single doc embeding: 845it [1:01:41,  1.62s/it][A[A[A

embedding all sents with BERT: 845it [1:01:41,  1.62s/it][A[A
cleaning sentances: 845it [1:01:44,  1.62s/it][ATokenizing tekst text into sentances: 845it [1:01:44,  1.62s/it]


reducing sentences to a single doc embeding: 846it [1:01:44,  2.10s/it][A[A[A

embedding all sents with BERT: 846it [1:01:44,  2.10s/it][A[A
cleaning sentances: 846it [1:01:47,  2.10s/it][ATokenizing tekst text into sentances: 846it [1:01:47,  2.10s/it]


reducing sentences to a single doc embeding: 847it [1:01:47,  2.17s/it][A[A[A

embedding all sents with BERT: 847it [1:01:47,  2.17s/it][A[A
cleaning sentances: 847it [1:01:49,  2.17s/it][ATokenizing tekst text into sentances: 847it [1:01:49,  2.17s/it]


reducing sentences to a single doc embeding: 848it [1:01:58,  4.85s/it][A[A[A

embedding all sents with BERT: 848it [1:01:58,  4.85s/it][A[A
cleaning sentances: 848it [1:02:00,  4.85s/it][ATokenizing tekst text into sentances: 848it [1:02:00,  4.85s/it]


reducing sentences to a single doc embeding: 849it [1:02:06,  5.91s/it][A[A[A

embedding all sents with BERT: 849it [1:02:06,  5.91s/it][A[A
cleaning sentances: 849it [1:02:09,  5.91s/it][ATokenizing tekst text into sentances: 849it [1:02:09,  5.91s/it]


reducing sentences to a single doc embeding: 850it [1:02:12,  5.92s/it][A[A[A

embedding all sents with BERT: 850it [1:02:12,  5.92s/it][A[A
cleaning sentances: 850it [1:02:15,  5.92s/it][ATokenizing tekst text into sentances: 850it [1:02:15,  5.92s/it]


reducing sentences to a single doc embeding: 851it [1:02:18,  6.04s/it][A[A[A

embedding all sents with BERT: 851it [1:02:18,  6.04s/it][A[A
cleaning sentances: 851it [1:02:21,  6.04s/it][ATokenizing tekst text into sentances: 851it [1:02:21,  6.04s/it]


reducing sentences to a single doc embeding: 852it [1:02:19,  4.46s/it][A[A[A

embedding all sents with BERT: 852it [1:02:19,  4.46s/it][A[A
cleaning sentances: 852it [1:02:22,  4.46s/it][ATokenizing tekst text into sentances: 852it [1:02:22,  4.46s/it]


reducing sentences to a single doc embeding: 853it [1:02:23,  4.12s/it][A[A[A

embedding all sents with BERT: 853it [1:02:23,  4.12s/it][A[A
cleaning sentances: 853it [1:02:25,  4.12s/it][ATokenizing tekst text into sentances: 853it [1:02:25,  4.12s/it]


reducing sentences to a single doc embeding: 854it [1:02:24,  3.33s/it][A[A[A

embedding all sents with BERT: 854it [1:02:24,  3.33s/it][A[A
cleaning sentances: 854it [1:02:26,  3.33s/it][ATokenizing tekst text into sentances: 854it [1:02:26,  3.33s/it]


reducing sentences to a single doc embeding: 855it [1:02:39,  6.87s/it][A[A[A

embedding all sents with BERT: 855it [1:02:39,  6.87s/it][A[A
cleaning sentances: 855it [1:02:42,  6.87s/it][ATokenizing tekst text into sentances: 855it [1:02:42,  6.87s/it]


reducing sentences to a single doc embeding: 856it [1:02:40,  4.98s/it][A[A[A

embedding all sents with BERT: 856it [1:02:40,  4.98s/it][A[A
cleaning sentances: 856it [1:02:42,  4.98s/it][ATokenizing tekst text into sentances: 856it [1:02:42,  4.98s/it]


reducing sentences to a single doc embeding: 857it [1:02:41,  3.91s/it][A[A[A

embedding all sents with BERT: 857it [1:02:41,  3.91s/it][A[A
cleaning sentances: 857it [1:02:44,  3.91s/it][ATokenizing tekst text into sentances: 857it [1:02:44,  3.91s/it]


reducing sentences to a single doc embeding: 858it [1:02:44,  3.52s/it][A[A[A

embedding all sents with BERT: 858it [1:02:44,  3.52s/it][A[A
cleaning sentances: 858it [1:02:46,  3.52s/it][ATokenizing tekst text into sentances: 858it [1:02:46,  3.52s/it]


reducing sentences to a single doc embeding: 859it [1:02:49,  3.96s/it][A[A[A

embedding all sents with BERT: 859it [1:02:49,  3.96s/it][A[A
cleaning sentances: 859it [1:02:51,  3.96s/it][ATokenizing tekst text into sentances: 859it [1:02:51,  3.96s/it]


reducing sentences to a single doc embeding: 860it [1:02:59,  5.97s/it][A[A[A

embedding all sents with BERT: 860it [1:02:59,  5.97s/it][A[A
cleaning sentances: 860it [1:03:02,  5.97s/it][ATokenizing tekst text into sentances: 860it [1:03:02,  5.97s/it]


reducing sentences to a single doc embeding: 861it [1:03:05,  5.87s/it][A[A[A

embedding all sents with BERT: 861it [1:03:05,  5.87s/it][A[A
cleaning sentances: 861it [1:03:07,  5.87s/it][ATokenizing tekst text into sentances: 861it [1:03:07,  5.87s/it]


reducing sentences to a single doc embeding: 862it [1:03:06,  4.25s/it][A[A[A

embedding all sents with BERT: 862it [1:03:06,  4.25s/it][A[A
cleaning sentances: 862it [1:03:08,  4.25s/it][ATokenizing tekst text into sentances: 862it [1:03:08,  4.25s/it]


reducing sentences to a single doc embeding: 863it [1:03:06,  3.16s/it][A[A[A

embedding all sents with BERT: 863it [1:03:06,  3.16s/it][A[A
cleaning sentances: 863it [1:03:09,  3.16s/it][ATokenizing tekst text into sentances: 863it [1:03:09,  3.16s/it]


reducing sentences to a single doc embeding: 864it [1:03:06,  2.25s/it][A[A[A

embedding all sents with BERT: 864it [1:03:06,  2.25s/it][A[A
cleaning sentances: 864it [1:03:09,  2.25s/it][ATokenizing tekst text into sentances: 864it [1:03:09,  2.25s/it]


reducing sentences to a single doc embeding: 865it [1:03:07,  1.74s/it][A[A[A

embedding all sents with BERT: 865it [1:03:07,  1.74s/it][A[A
cleaning sentances: 865it [1:03:09,  1.74s/it][ATokenizing tekst text into sentances: 865it [1:03:09,  1.74s/it]


reducing sentences to a single doc embeding: 866it [1:03:07,  1.36s/it][A[A[A

embedding all sents with BERT: 866it [1:03:07,  1.36s/it][A[A
cleaning sentances: 866it [1:03:10,  1.36s/it][ATokenizing tekst text into sentances: 866it [1:03:10,  1.36s/it]


reducing sentences to a single doc embeding: 867it [1:03:09,  1.37s/it][A[A[A

embedding all sents with BERT: 867it [1:03:09,  1.37s/it][A[A
cleaning sentances: 867it [1:03:11,  1.37s/it][ATokenizing tekst text into sentances: 867it [1:03:11,  1.37s/it]


reducing sentences to a single doc embeding: 868it [1:03:09,  1.14s/it][A[A[A

embedding all sents with BERT: 868it [1:03:09,  1.14s/it][A[A
cleaning sentances: 868it [1:03:12,  1.14s/it][ATokenizing tekst text into sentances: 868it [1:03:12,  1.14s/it]


reducing sentences to a single doc embeding: 869it [1:03:24,  5.12s/it][A[A[A

embedding all sents with BERT: 869it [1:03:24,  5.12s/it][A[A
cleaning sentances: 869it [1:03:26,  5.12s/it][ATokenizing tekst text into sentances: 869it [1:03:26,  5.12s/it]


reducing sentences to a single doc embeding: 870it [1:03:33,  6.38s/it][A[A[A

embedding all sents with BERT: 870it [1:03:33,  6.38s/it][A[A
cleaning sentances: 870it [1:03:35,  6.38s/it][ATokenizing tekst text into sentances: 870it [1:03:35,  6.38s/it]


reducing sentences to a single doc embeding: 871it [1:03:42,  7.16s/it][A[A[A

embedding all sents with BERT: 871it [1:03:42,  7.16s/it][A[A
cleaning sentances: 871it [1:03:44,  7.16s/it][ATokenizing tekst text into sentances: 871it [1:03:44,  7.16s/it]


reducing sentences to a single doc embeding: 872it [1:03:43,  5.16s/it][A[A[A

embedding all sents with BERT: 872it [1:03:43,  5.16s/it][A[A
cleaning sentances: 872it [1:03:45,  5.16s/it][ATokenizing tekst text into sentances: 872it [1:03:45,  5.16s/it]


reducing sentences to a single doc embeding: 873it [1:03:43,  3.76s/it][A[A[A

embedding all sents with BERT: 873it [1:03:43,  3.76s/it][A[A
cleaning sentances: 873it [1:03:45,  3.76s/it][ATokenizing tekst text into sentances: 873it [1:03:45,  3.76s/it]


reducing sentences to a single doc embeding: 874it [1:03:44,  2.81s/it][A[A[A

embedding all sents with BERT: 874it [1:03:44,  2.81s/it][A[A
cleaning sentances: 874it [1:03:46,  2.81s/it][ATokenizing tekst text into sentances: 874it [1:03:46,  2.81s/it]


reducing sentences to a single doc embeding: 875it [1:03:45,  2.24s/it][A[A[A

embedding all sents with BERT: 875it [1:03:45,  2.24s/it][A[A
cleaning sentances: 875it [1:03:47,  2.24s/it][ATokenizing tekst text into sentances: 875it [1:03:47,  2.24s/it]


reducing sentences to a single doc embeding: 876it [1:03:46,  2.14s/it][A[A[A

embedding all sents with BERT: 876it [1:03:46,  2.14s/it][A[A
cleaning sentances: 876it [1:03:49,  2.14s/it][ATokenizing tekst text into sentances: 876it [1:03:49,  2.14s/it]


reducing sentences to a single doc embeding: 877it [1:04:27, 13.67s/it][A[A[A

embedding all sents with BERT: 877it [1:04:27, 13.67s/it][A[A
cleaning sentances: 877it [1:04:29, 13.67s/it][ATokenizing tekst text into sentances: 877it [1:04:29, 13.67s/it]


reducing sentences to a single doc embeding: 878it [1:04:28,  9.75s/it][A[A[A

embedding all sents with BERT: 878it [1:04:28,  9.75s/it][A[A
cleaning sentances: 878it [1:04:30,  9.75s/it][ATokenizing tekst text into sentances: 878it [1:04:30,  9.75s/it]


reducing sentences to a single doc embeding: 879it [1:04:30,  7.65s/it][A[A[A

embedding all sents with BERT: 879it [1:04:30,  7.65s/it][A[A
cleaning sentances: 879it [1:04:33,  7.65s/it][ATokenizing tekst text into sentances: 879it [1:04:33,  7.65s/it]


reducing sentences to a single doc embeding: 880it [1:04:32,  5.93s/it][A[A[A

embedding all sents with BERT: 880it [1:04:32,  5.93s/it][A[A
cleaning sentances: 880it [1:04:35,  5.93s/it][ATokenizing tekst text into sentances: 880it [1:04:35,  5.93s/it]


reducing sentences to a single doc embeding: 881it [1:04:41,  6.71s/it][A[A[A

embedding all sents with BERT: 881it [1:04:41,  6.71s/it][A[A
cleaning sentances: 881it [1:04:43,  6.71s/it][ATokenizing tekst text into sentances: 881it [1:04:43,  6.71s/it]


reducing sentences to a single doc embeding: 882it [1:04:45,  6.00s/it][A[A[A

embedding all sents with BERT: 882it [1:04:45,  6.00s/it][A[A
cleaning sentances: 882it [1:04:48,  6.00s/it][ATokenizing tekst text into sentances: 882it [1:04:48,  6.00s/it]


reducing sentences to a single doc embeding: 883it [1:04:46,  4.44s/it][A[A[A

embedding all sents with BERT: 883it [1:04:46,  4.44s/it][A[A
cleaning sentances: 883it [1:04:48,  4.44s/it][ATokenizing tekst text into sentances: 883it [1:04:48,  4.44s/it]


reducing sentences to a single doc embeding: 884it [1:04:48,  3.64s/it][A[A[A

embedding all sents with BERT: 884it [1:04:48,  3.64s/it][A[A
cleaning sentances: 884it [1:04:50,  3.64s/it][ATokenizing tekst text into sentances: 884it [1:04:50,  3.64s/it]


reducing sentences to a single doc embeding: 885it [1:04:53,  4.06s/it][A[A[A

embedding all sents with BERT: 885it [1:04:53,  4.06s/it][A[A
cleaning sentances: 885it [1:04:55,  4.06s/it][ATokenizing tekst text into sentances: 885it [1:04:55,  4.06s/it]


reducing sentences to a single doc embeding: 886it [1:04:54,  3.29s/it][A[A[A

embedding all sents with BERT: 886it [1:04:54,  3.29s/it][A[A
cleaning sentances: 886it [1:04:57,  3.29s/it][ATokenizing tekst text into sentances: 886it [1:04:57,  3.29s/it]


reducing sentences to a single doc embeding: 887it [1:04:55,  2.61s/it][A[A[A

embedding all sents with BERT: 887it [1:04:55,  2.61s/it][A[A
cleaning sentances: 887it [1:04:58,  2.61s/it][ATokenizing tekst text into sentances: 887it [1:04:58,  2.61s/it]


reducing sentences to a single doc embeding: 888it [1:04:56,  2.10s/it][A[A[A

embedding all sents with BERT: 888it [1:04:56,  2.10s/it][A[A
cleaning sentances: 888it [1:04:59,  2.10s/it][ATokenizing tekst text into sentances: 888it [1:04:59,  2.10s/it]


reducing sentences to a single doc embeding: 889it [1:05:02,  3.17s/it][A[A[A

embedding all sents with BERT: 889it [1:05:02,  3.17s/it][A[A
cleaning sentances: 889it [1:05:04,  3.17s/it][ATokenizing tekst text into sentances: 889it [1:05:04,  3.17s/it]


reducing sentences to a single doc embeding: 890it [1:05:07,  3.61s/it][A[A[A

embedding all sents with BERT: 890it [1:05:07,  3.61s/it][A[A
cleaning sentances: 890it [1:05:09,  3.61s/it][ATokenizing tekst text into sentances: 890it [1:05:09,  3.61s/it]


reducing sentences to a single doc embeding: 891it [1:05:07,  2.80s/it][A[A[A

embedding all sents with BERT: 891it [1:05:07,  2.80s/it][A[A
cleaning sentances: 891it [1:05:10,  2.80s/it][ATokenizing tekst text into sentances: 891it [1:05:10,  2.80s/it]


reducing sentences to a single doc embeding: 892it [1:05:08,  2.11s/it][A[A[A

embedding all sents with BERT: 892it [1:05:08,  2.11s/it][A[A
cleaning sentances: 892it [1:05:10,  2.11s/it][ATokenizing tekst text into sentances: 892it [1:05:10,  2.11s/it]


reducing sentences to a single doc embeding: 893it [1:05:12,  2.72s/it][A[A[A

embedding all sents with BERT: 893it [1:05:12,  2.72s/it][A[A
cleaning sentances: 893it [1:05:14,  2.72s/it][ATokenizing tekst text into sentances: 893it [1:05:14,  2.72s/it]


reducing sentences to a single doc embeding: 894it [1:05:13,  2.08s/it][A[A[A

embedding all sents with BERT: 894it [1:05:13,  2.08s/it][A[A
cleaning sentances: 894it [1:05:15,  2.08s/it][ATokenizing tekst text into sentances: 894it [1:05:15,  2.08s/it]


reducing sentences to a single doc embeding: 895it [1:05:31,  6.87s/it][A[A[A

embedding all sents with BERT: 895it [1:05:31,  6.87s/it][A[A
cleaning sentances: 895it [1:05:33,  6.87s/it][ATokenizing tekst text into sentances: 895it [1:05:33,  6.87s/it]


reducing sentences to a single doc embeding: 896it [1:05:31,  5.03s/it][A[A[A

embedding all sents with BERT: 896it [1:05:31,  5.03s/it][A[A
cleaning sentances: 896it [1:05:34,  5.03s/it][ATokenizing tekst text into sentances: 896it [1:05:34,  5.03s/it]


reducing sentences to a single doc embeding: 897it [1:05:34,  4.19s/it][A[A[A

embedding all sents with BERT: 897it [1:05:34,  4.19s/it][A[A
cleaning sentances: 897it [1:05:36,  4.19s/it][ATokenizing tekst text into sentances: 897it [1:05:36,  4.19s/it]


reducing sentences to a single doc embeding: 898it [1:05:37,  3.81s/it][A[A[A

embedding all sents with BERT: 898it [1:05:37,  3.81s/it][A[A
cleaning sentances: 898it [1:05:39,  3.81s/it][ATokenizing tekst text into sentances: 898it [1:05:39,  3.81s/it]


reducing sentences to a single doc embeding: 899it [1:05:37,  2.74s/it][A[A[A

embedding all sents with BERT: 899it [1:05:37,  2.74s/it][A[A
cleaning sentances: 899it [1:05:39,  2.74s/it][ATokenizing tekst text into sentances: 899it [1:05:39,  2.74s/it]


reducing sentences to a single doc embeding: 900it [1:05:38,  2.14s/it][A[A[A

embedding all sents with BERT: 900it [1:05:38,  2.14s/it][A[A
cleaning sentances: 900it [1:05:40,  2.14s/it][ATokenizing tekst text into sentances: 900it [1:05:40,  2.14s/it]


reducing sentences to a single doc embeding: 901it [1:05:38,  1.56s/it][A[A[A

embedding all sents with BERT: 901it [1:05:38,  1.56s/it][A[A
cleaning sentances: 901it [1:05:40,  1.56s/it][ATokenizing tekst text into sentances: 901it [1:05:40,  1.56s/it]


reducing sentences to a single doc embeding: 902it [1:05:42,  2.33s/it][A[A[A

embedding all sents with BERT: 902it [1:05:42,  2.33s/it][A[A
cleaning sentances: 902it [1:05:44,  2.33s/it][ATokenizing tekst text into sentances: 902it [1:05:44,  2.33s/it]


reducing sentences to a single doc embeding: 903it [1:05:43,  1.95s/it][A[A[A

embedding all sents with BERT: 903it [1:05:43,  1.95s/it][A[A
cleaning sentances: 903it [1:05:45,  1.95s/it][ATokenizing tekst text into sentances: 903it [1:05:45,  1.95s/it]


reducing sentences to a single doc embeding: 904it [1:05:46,  2.31s/it][A[A[A

embedding all sents with BERT: 904it [1:05:46,  2.31s/it][A[A
cleaning sentances: 904it [1:05:48,  2.31s/it][ATokenizing tekst text into sentances: 904it [1:05:48,  2.31s/it]


reducing sentences to a single doc embeding: 905it [1:05:51,  2.98s/it][A[A[A

embedding all sents with BERT: 905it [1:05:51,  2.98s/it][A[A
cleaning sentances: 905it [1:05:53,  2.98s/it][ATokenizing tekst text into sentances: 905it [1:05:53,  2.98s/it]


reducing sentences to a single doc embeding: 906it [1:05:54,  3.08s/it][A[A[A

embedding all sents with BERT: 906it [1:05:54,  3.08s/it][A[A
cleaning sentances: 906it [1:05:56,  3.08s/it][ATokenizing tekst text into sentances: 906it [1:05:56,  3.08s/it]


reducing sentences to a single doc embeding: 907it [1:06:01,  4.27s/it][A[A[A

embedding all sents with BERT: 907it [1:06:01,  4.27s/it][A[A
cleaning sentances: 907it [1:06:03,  4.27s/it][ATokenizing tekst text into sentances: 907it [1:06:03,  4.27s/it]


reducing sentences to a single doc embeding: 908it [1:06:01,  3.10s/it][A[A[A

embedding all sents with BERT: 908it [1:06:01,  3.10s/it][A[A
cleaning sentances: 908it [1:06:04,  3.10s/it][ATokenizing tekst text into sentances: 908it [1:06:04,  3.10s/it]


reducing sentences to a single doc embeding: 909it [1:06:04,  2.83s/it][A[A[A

embedding all sents with BERT: 909it [1:06:04,  2.83s/it][A[A
cleaning sentances: 909it [1:06:06,  2.83s/it][ATokenizing tekst text into sentances: 909it [1:06:06,  2.83s/it]


reducing sentences to a single doc embeding: 910it [1:06:05,  2.40s/it][A[A[A

embedding all sents with BERT: 910it [1:06:05,  2.40s/it][A[A
cleaning sentances: 910it [1:06:07,  2.40s/it][ATokenizing tekst text into sentances: 910it [1:06:07,  2.40s/it]


reducing sentences to a single doc embeding: 911it [1:06:06,  2.06s/it][A[A[A

embedding all sents with BERT: 911it [1:06:06,  2.06s/it][A[A
cleaning sentances: 911it [1:06:09,  2.06s/it][ATokenizing tekst text into sentances: 911it [1:06:09,  2.06s/it]


reducing sentences to a single doc embeding: 912it [1:06:08,  1.94s/it][A[A[A

embedding all sents with BERT: 912it [1:06:08,  1.94s/it][A[A
cleaning sentances: 912it [1:06:10,  1.94s/it][ATokenizing tekst text into sentances: 912it [1:06:10,  1.94s/it]


reducing sentences to a single doc embeding: 913it [1:06:09,  1.57s/it][A[A[A

embedding all sents with BERT: 913it [1:06:09,  1.57s/it][A[A
cleaning sentances: 913it [1:06:11,  1.57s/it][ATokenizing tekst text into sentances: 913it [1:06:11,  1.57s/it]


reducing sentences to a single doc embeding: 914it [1:06:51, 13.82s/it][A[A[A

embedding all sents with BERT: 914it [1:06:51, 13.82s/it][A[A
cleaning sentances: 914it [1:06:53, 13.82s/it][ATokenizing tekst text into sentances: 914it [1:06:53, 13.82s/it]


reducing sentences to a single doc embeding: 915it [1:06:51,  9.73s/it][A[A[A

embedding all sents with BERT: 915it [1:06:51,  9.73s/it][A[A
cleaning sentances: 915it [1:06:54,  9.73s/it][ATokenizing tekst text into sentances: 915it [1:06:54,  9.73s/it]


reducing sentences to a single doc embeding: 916it [1:06:54,  7.56s/it][A[A[A

embedding all sents with BERT: 916it [1:06:54,  7.56s/it][A[A
cleaning sentances: 916it [1:06:56,  7.56s/it][ATokenizing tekst text into sentances: 916it [1:06:56,  7.56s/it]


reducing sentences to a single doc embeding: 917it [1:07:07,  9.32s/it][A[A[A

embedding all sents with BERT: 917it [1:07:07,  9.32s/it][A[A
cleaning sentances: 917it [1:07:10,  9.32s/it][ATokenizing tekst text into sentances: 917it [1:07:10,  9.32s/it]


reducing sentences to a single doc embeding: 918it [1:07:08,  6.87s/it][A[A[A

embedding all sents with BERT: 918it [1:07:08,  6.87s/it][A[A
cleaning sentances: 918it [1:07:11,  6.87s/it][ATokenizing tekst text into sentances: 918it [1:07:11,  6.87s/it]


reducing sentences to a single doc embeding: 919it [1:07:11,  5.68s/it][A[A[A

embedding all sents with BERT: 919it [1:07:11,  5.68s/it][A[A
cleaning sentances: 919it [1:07:14,  5.68s/it][ATokenizing tekst text into sentances: 919it [1:07:14,  5.68s/it]


reducing sentences to a single doc embeding: 920it [1:07:15,  5.08s/it][A[A[A

embedding all sents with BERT: 920it [1:07:15,  5.08s/it][A[A
cleaning sentances: 920it [1:07:17,  5.08s/it][ATokenizing tekst text into sentances: 920it [1:07:17,  5.08s/it]


reducing sentences to a single doc embeding: 921it [1:07:18,  4.40s/it][A[A[A

embedding all sents with BERT: 921it [1:07:18,  4.40s/it][A[A
cleaning sentances: 921it [1:07:20,  4.40s/it][ATokenizing tekst text into sentances: 921it [1:07:20,  4.40s/it]


reducing sentences to a single doc embeding: 922it [1:07:18,  3.20s/it][A[A[A

embedding all sents with BERT: 922it [1:07:18,  3.20s/it][A[A
cleaning sentances: 922it [1:07:20,  3.20s/it][ATokenizing tekst text into sentances: 922it [1:07:20,  3.20s/it]


reducing sentences to a single doc embeding: 923it [1:07:19,  2.38s/it][A[A[A

embedding all sents with BERT: 923it [1:07:19,  2.38s/it][A[A
cleaning sentances: 923it [1:07:21,  2.38s/it][ATokenizing tekst text into sentances: 923it [1:07:21,  2.38s/it]


reducing sentences to a single doc embeding: 924it [1:07:29,  4.67s/it][A[A[A

embedding all sents with BERT: 924it [1:07:29,  4.67s/it][A[A
cleaning sentances: 924it [1:07:31,  4.67s/it][ATokenizing tekst text into sentances: 924it [1:07:31,  4.67s/it]


reducing sentences to a single doc embeding: 925it [1:07:32,  4.16s/it][A[A[A

embedding all sents with BERT: 925it [1:07:32,  4.16s/it][A[A
cleaning sentances: 925it [1:07:34,  4.16s/it][ATokenizing tekst text into sentances: 925it [1:07:34,  4.16s/it]


reducing sentences to a single doc embeding: 926it [1:07:35,  3.82s/it][A[A[A

embedding all sents with BERT: 926it [1:07:35,  3.82s/it][A[A
cleaning sentances: 926it [1:07:37,  3.82s/it][ATokenizing tekst text into sentances: 926it [1:07:37,  3.82s/it]


reducing sentences to a single doc embeding: 927it [1:07:37,  3.33s/it][A[A[A

embedding all sents with BERT: 927it [1:07:37,  3.33s/it][A[A
cleaning sentances: 927it [1:07:39,  3.33s/it][ATokenizing tekst text into sentances: 927it [1:07:39,  3.33s/it]


reducing sentences to a single doc embeding: 928it [1:07:38,  2.57s/it][A[A[A

embedding all sents with BERT: 928it [1:07:38,  2.57s/it][A[A
cleaning sentances: 928it [1:07:40,  2.57s/it][ATokenizing tekst text into sentances: 928it [1:07:40,  2.57s/it]


reducing sentences to a single doc embeding: 929it [1:07:42,  3.13s/it][A[A[A

embedding all sents with BERT: 929it [1:07:42,  3.13s/it][A[A
cleaning sentances: 929it [1:07:44,  3.13s/it][ATokenizing tekst text into sentances: 929it [1:07:44,  3.13s/it]


reducing sentences to a single doc embeding: 930it [1:07:42,  2.27s/it][A[A[A

embedding all sents with BERT: 930it [1:07:42,  2.27s/it][A[A
cleaning sentances: 930it [1:07:45,  2.27s/it][ATokenizing tekst text into sentances: 930it [1:07:45,  2.27s/it]


reducing sentences to a single doc embeding: 931it [1:08:10, 10.00s/it][A[A[A

embedding all sents with BERT: 931it [1:08:10, 10.00s/it][A[A
cleaning sentances: 931it [1:08:13, 10.00s/it][ATokenizing tekst text into sentances: 931it [1:08:13, 10.00s/it]


reducing sentences to a single doc embeding: 932it [1:08:13,  7.87s/it][A[A[A

embedding all sents with BERT: 932it [1:08:13,  7.87s/it][A[A
cleaning sentances: 932it [1:08:16,  7.87s/it][ATokenizing tekst text into sentances: 932it [1:08:16,  7.87s/it]


reducing sentences to a single doc embeding: 933it [1:08:18,  6.81s/it][A[A[A

embedding all sents with BERT: 933it [1:08:18,  6.81s/it][A[A
cleaning sentances: 933it [1:08:20,  6.81s/it][ATokenizing tekst text into sentances: 933it [1:08:20,  6.81s/it]


reducing sentences to a single doc embeding: 934it [1:08:18,  4.82s/it][A[A[A

embedding all sents with BERT: 934it [1:08:18,  4.82s/it][A[A
cleaning sentances: 934it [1:08:20,  4.82s/it][ATokenizing tekst text into sentances: 934it [1:08:20,  4.82s/it]


reducing sentences to a single doc embeding: 935it [1:08:20,  4.08s/it][A[A[A

embedding all sents with BERT: 935it [1:08:20,  4.08s/it][A[A
cleaning sentances: 935it [1:08:22,  4.08s/it][ATokenizing tekst text into sentances: 935it [1:08:22,  4.08s/it]


reducing sentences to a single doc embeding: 936it [1:08:21,  3.06s/it][A[A[A

embedding all sents with BERT: 936it [1:08:21,  3.06s/it][A[A
cleaning sentances: 936it [1:08:23,  3.06s/it][ATokenizing tekst text into sentances: 936it [1:08:23,  3.06s/it]


reducing sentences to a single doc embeding: 937it [1:08:22,  2.66s/it][A[A[A

embedding all sents with BERT: 937it [1:08:22,  2.66s/it][A[A
cleaning sentances: 937it [1:08:25,  2.66s/it][ATokenizing tekst text into sentances: 937it [1:08:25,  2.66s/it]


reducing sentences to a single doc embeding: 938it [1:08:25,  2.58s/it][A[A[A

embedding all sents with BERT: 938it [1:08:25,  2.58s/it][A[A
cleaning sentances: 938it [1:08:27,  2.58s/it][ATokenizing tekst text into sentances: 938it [1:08:27,  2.58s/it]


reducing sentences to a single doc embeding: 939it [1:08:28,  2.75s/it][A[A[A

embedding all sents with BERT: 939it [1:08:28,  2.75s/it][A[A
cleaning sentances: 939it [1:08:30,  2.75s/it][ATokenizing tekst text into sentances: 939it [1:08:30,  2.75s/it]


reducing sentences to a single doc embeding: 940it [1:08:41,  5.74s/it][A[A[A

embedding all sents with BERT: 940it [1:08:41,  5.74s/it][A[A
cleaning sentances: 940it [1:08:43,  5.74s/it][ATokenizing tekst text into sentances: 940it [1:08:43,  5.74s/it]


reducing sentences to a single doc embeding: 941it [1:08:44,  4.88s/it][A[A[A

embedding all sents with BERT: 941it [1:08:44,  4.88s/it][A[A
cleaning sentances: 941it [1:08:46,  4.88s/it][ATokenizing tekst text into sentances: 941it [1:08:46,  4.88s/it]


reducing sentences to a single doc embeding: 942it [1:08:51,  5.62s/it][A[A[A

embedding all sents with BERT: 942it [1:08:51,  5.62s/it][A[A
cleaning sentances: 942it [1:08:53,  5.62s/it][ATokenizing tekst text into sentances: 942it [1:08:53,  5.62s/it]


reducing sentences to a single doc embeding: 943it [1:08:51,  4.07s/it][A[A[A

embedding all sents with BERT: 943it [1:08:51,  4.07s/it][A[A
cleaning sentances: 943it [1:08:54,  4.07s/it][ATokenizing tekst text into sentances: 943it [1:08:54,  4.07s/it]


reducing sentences to a single doc embeding: 944it [1:08:53,  3.44s/it][A[A[A

embedding all sents with BERT: 944it [1:08:53,  3.44s/it][A[A
cleaning sentances: 944it [1:08:56,  3.44s/it][ATokenizing tekst text into sentances: 944it [1:08:56,  3.44s/it]


reducing sentences to a single doc embeding: 945it [1:08:54,  2.72s/it][A[A[A

embedding all sents with BERT: 945it [1:08:54,  2.72s/it][A[A
cleaning sentances: 945it [1:08:57,  2.72s/it][ATokenizing tekst text into sentances: 945it [1:08:57,  2.72s/it]


reducing sentences to a single doc embeding: 946it [1:08:55,  2.14s/it][A[A[A

embedding all sents with BERT: 946it [1:08:55,  2.14s/it][A[A
cleaning sentances: 946it [1:08:58,  2.14s/it][ATokenizing tekst text into sentances: 946it [1:08:58,  2.14s/it]


reducing sentences to a single doc embeding: 947it [1:08:56,  1.73s/it][A[A[A

embedding all sents with BERT: 947it [1:08:56,  1.73s/it][A[A
cleaning sentances: 947it [1:08:58,  1.73s/it][ATokenizing tekst text into sentances: 947it [1:08:58,  1.73s/it]


reducing sentences to a single doc embeding: 948it [1:08:57,  1.39s/it][A[A[A

embedding all sents with BERT: 948it [1:08:57,  1.39s/it][A[A
cleaning sentances: 948it [1:08:59,  1.39s/it][ATokenizing tekst text into sentances: 948it [1:08:59,  1.39s/it]


reducing sentences to a single doc embeding: 949it [1:09:51, 17.32s/it][A[A[A

embedding all sents with BERT: 949it [1:09:51, 17.32s/it][A[A
cleaning sentances: 949it [1:09:53, 17.32s/it][ATokenizing tekst text into sentances: 949it [1:09:53, 17.32s/it]


reducing sentences to a single doc embeding: 950it [1:10:22, 21.33s/it][A[A[A

embedding all sents with BERT: 950it [1:10:22, 21.33s/it][A[A
cleaning sentances: 950it [1:10:24, 21.33s/it][ATokenizing tekst text into sentances: 950it [1:10:24, 21.33s/it]


reducing sentences to a single doc embeding: 951it [1:10:27, 16.39s/it][A[A[A

embedding all sents with BERT: 951it [1:10:27, 16.39s/it][A[A
cleaning sentances: 951it [1:10:29, 16.39s/it][ATokenizing tekst text into sentances: 951it [1:10:29, 16.39s/it]


reducing sentences to a single doc embeding: 952it [1:10:29, 12.05s/it][A[A[A

embedding all sents with BERT: 952it [1:10:29, 12.05s/it][A[A
cleaning sentances: 952it [1:10:31, 12.05s/it][ATokenizing tekst text into sentances: 952it [1:10:31, 12.05s/it]


reducing sentences to a single doc embeding: 953it [1:10:32,  9.50s/it][A[A[A

embedding all sents with BERT: 953it [1:10:32,  9.50s/it][A[A
cleaning sentances: 953it [1:10:34,  9.50s/it][ATokenizing tekst text into sentances: 953it [1:10:34,  9.50s/it]


reducing sentences to a single doc embeding: 954it [1:10:33,  6.80s/it][A[A[A

embedding all sents with BERT: 954it [1:10:33,  6.80s/it][A[A
cleaning sentances: 954it [1:10:35,  6.80s/it][ATokenizing tekst text into sentances: 954it [1:10:35,  6.80s/it]


reducing sentences to a single doc embeding: 955it [1:10:35,  5.50s/it][A[A[A

embedding all sents with BERT: 955it [1:10:35,  5.50s/it][A[A
cleaning sentances: 955it [1:10:37,  5.50s/it][ATokenizing tekst text into sentances: 955it [1:10:37,  5.50s/it]


reducing sentences to a single doc embeding: 956it [1:10:49,  8.01s/it][A[A[A

embedding all sents with BERT: 956it [1:10:49,  8.01s/it][A[A
cleaning sentances: 956it [1:10:51,  8.01s/it][ATokenizing tekst text into sentances: 956it [1:10:51,  8.01s/it]


reducing sentences to a single doc embeding: 957it [1:10:51,  6.09s/it][A[A[A

embedding all sents with BERT: 957it [1:10:51,  6.09s/it][A[A
cleaning sentances: 957it [1:10:53,  6.09s/it][ATokenizing tekst text into sentances: 957it [1:10:53,  6.09s/it]


reducing sentences to a single doc embeding: 958it [1:10:58,  6.60s/it][A[A[A

embedding all sents with BERT: 958it [1:10:58,  6.60s/it][A[A
cleaning sentances: 958it [1:11:01,  6.60s/it][ATokenizing tekst text into sentances: 958it [1:11:01,  6.60s/it]


reducing sentences to a single doc embeding: 959it [1:11:00,  5.22s/it][A[A[A

embedding all sents with BERT: 959it [1:11:00,  5.22s/it][A[A
cleaning sentances: 959it [1:11:03,  5.22s/it][ATokenizing tekst text into sentances: 959it [1:11:03,  5.22s/it]


reducing sentences to a single doc embeding: 960it [1:11:03,  4.45s/it][A[A[A

embedding all sents with BERT: 960it [1:11:03,  4.45s/it][A[A
cleaning sentances: 960it [1:11:05,  4.45s/it][ATokenizing tekst text into sentances: 960it [1:11:05,  4.45s/it]


reducing sentences to a single doc embeding: 961it [1:11:04,  3.34s/it][A[A[A

embedding all sents with BERT: 961it [1:11:04,  3.34s/it][A[A
cleaning sentances: 961it [1:11:06,  3.34s/it][ATokenizing tekst text into sentances: 961it [1:11:06,  3.34s/it]


reducing sentences to a single doc embeding: 962it [1:11:04,  2.57s/it][A[A[A

embedding all sents with BERT: 962it [1:11:04,  2.57s/it][A[A
cleaning sentances: 962it [1:11:07,  2.57s/it][ATokenizing tekst text into sentances: 962it [1:11:07,  2.57s/it]


reducing sentences to a single doc embeding: 963it [1:11:05,  2.03s/it][A[A[A

embedding all sents with BERT: 963it [1:11:05,  2.03s/it][A[A
cleaning sentances: 963it [1:11:08,  2.03s/it][ATokenizing tekst text into sentances: 963it [1:11:08,  2.03s/it]


reducing sentences to a single doc embeding: 964it [1:11:08,  2.37s/it][A[A[A

embedding all sents with BERT: 964it [1:11:08,  2.37s/it][A[A
cleaning sentances: 964it [1:11:11,  2.37s/it][ATokenizing tekst text into sentances: 964it [1:11:11,  2.37s/it]


reducing sentences to a single doc embeding: 965it [1:11:09,  1.90s/it][A[A[A

embedding all sents with BERT: 965it [1:11:09,  1.90s/it][A[A
cleaning sentances: 965it [1:11:12,  1.90s/it][ATokenizing tekst text into sentances: 965it [1:11:12,  1.90s/it]


reducing sentences to a single doc embeding: 966it [1:11:11,  1.85s/it][A[A[A

embedding all sents with BERT: 966it [1:11:11,  1.85s/it][A[A
cleaning sentances: 966it [1:11:13,  1.85s/it][ATokenizing tekst text into sentances: 966it [1:11:13,  1.85s/it]


reducing sentences to a single doc embeding: 967it [1:11:19,  3.67s/it][A[A[A

embedding all sents with BERT: 967it [1:11:19,  3.67s/it][A[A
cleaning sentances: 967it [1:11:21,  3.67s/it][ATokenizing tekst text into sentances: 967it [1:11:21,  3.67s/it]


reducing sentences to a single doc embeding: 968it [1:11:31,  6.26s/it][A[A[A

embedding all sents with BERT: 968it [1:11:31,  6.26s/it][A[A
cleaning sentances: 968it [1:11:34,  6.26s/it][ATokenizing tekst text into sentances: 968it [1:11:34,  6.26s/it]


reducing sentences to a single doc embeding: 969it [1:11:37,  6.00s/it][A[A[A

embedding all sents with BERT: 969it [1:11:37,  6.00s/it][A[A
cleaning sentances: 969it [1:11:39,  6.00s/it][ATokenizing tekst text into sentances: 969it [1:11:39,  6.00s/it]


reducing sentences to a single doc embeding: 970it [1:11:37,  4.46s/it][A[A[A

embedding all sents with BERT: 970it [1:11:37,  4.46s/it][A[A
cleaning sentances: 970it [1:11:40,  4.46s/it][ATokenizing tekst text into sentances: 970it [1:11:40,  4.46s/it]


reducing sentences to a single doc embeding: 971it [1:11:39,  3.52s/it][A[A[A

embedding all sents with BERT: 971it [1:11:39,  3.52s/it][A[A
cleaning sentances: 971it [1:11:41,  3.52s/it][ATokenizing tekst text into sentances: 971it [1:11:41,  3.52s/it]


reducing sentences to a single doc embeding: 972it [1:11:40,  2.90s/it][A[A[A

embedding all sents with BERT: 972it [1:11:40,  2.90s/it][A[A
cleaning sentances: 972it [1:11:43,  2.90s/it][ATokenizing tekst text into sentances: 972it [1:11:43,  2.90s/it]


reducing sentences to a single doc embeding: 973it [1:11:41,  2.26s/it][A[A[A

embedding all sents with BERT: 973it [1:11:41,  2.26s/it][A[A
cleaning sentances: 973it [1:11:43,  2.26s/it][ATokenizing tekst text into sentances: 973it [1:11:43,  2.26s/it]


reducing sentences to a single doc embeding: 974it [1:11:42,  2.00s/it][A[A[A

embedding all sents with BERT: 974it [1:11:42,  2.00s/it][A[A
cleaning sentances: 974it [1:11:45,  2.00s/it][ATokenizing tekst text into sentances: 974it [1:11:45,  2.00s/it]


reducing sentences to a single doc embeding: 975it [1:11:46,  2.49s/it][A[A[A

embedding all sents with BERT: 975it [1:11:46,  2.49s/it][A[A
cleaning sentances: 975it [1:11:48,  2.49s/it][ATokenizing tekst text into sentances: 975it [1:11:48,  2.49s/it]


reducing sentences to a single doc embeding: 976it [1:11:47,  2.14s/it][A[A[A

embedding all sents with BERT: 976it [1:11:47,  2.14s/it][A[A
cleaning sentances: 976it [1:11:50,  2.14s/it][ATokenizing tekst text into sentances: 976it [1:11:50,  2.14s/it]


reducing sentences to a single doc embeding: 977it [1:11:52,  2.88s/it][A[A[A

embedding all sents with BERT: 977it [1:11:52,  2.88s/it][A[A
cleaning sentances: 977it [1:11:54,  2.88s/it][ATokenizing tekst text into sentances: 977it [1:11:54,  2.88s/it]


reducing sentences to a single doc embeding: 978it [1:11:55,  2.90s/it][A[A[A

embedding all sents with BERT: 978it [1:11:55,  2.90s/it][A[A
cleaning sentances: 978it [1:11:57,  2.90s/it][ATokenizing tekst text into sentances: 978it [1:11:57,  2.90s/it]


reducing sentences to a single doc embeding: 979it [1:11:57,  2.70s/it][A[A[A

embedding all sents with BERT: 979it [1:11:57,  2.70s/it][A[A
cleaning sentances: 979it [1:11:59,  2.70s/it][ATokenizing tekst text into sentances: 979it [1:11:59,  2.70s/it]


reducing sentences to a single doc embeding: 980it [1:11:58,  2.28s/it][A[A[A

embedding all sents with BERT: 980it [1:11:58,  2.28s/it][A[A
cleaning sentances: 980it [1:12:01,  2.28s/it][ATokenizing tekst text into sentances: 980it [1:12:01,  2.28s/it]


reducing sentences to a single doc embeding: 981it [1:12:02,  2.72s/it][A[A[A

embedding all sents with BERT: 981it [1:12:02,  2.72s/it][A[A
cleaning sentances: 981it [1:12:05,  2.72s/it][ATokenizing tekst text into sentances: 981it [1:12:05,  2.72s/it]


reducing sentences to a single doc embeding: 982it [1:12:05,  2.80s/it][A[A[A

embedding all sents with BERT: 982it [1:12:05,  2.80s/it][A[A
cleaning sentances: 982it [1:12:08,  2.80s/it][ATokenizing tekst text into sentances: 982it [1:12:08,  2.80s/it]


reducing sentences to a single doc embeding: 983it [1:12:07,  2.66s/it][A[A[A

embedding all sents with BERT: 983it [1:12:07,  2.66s/it][A[A
cleaning sentances: 983it [1:12:10,  2.66s/it][ATokenizing tekst text into sentances: 983it [1:12:10,  2.66s/it]


reducing sentences to a single doc embeding: 984it [1:12:11,  2.93s/it][A[A[A

embedding all sents with BERT: 984it [1:12:11,  2.93s/it][A[A
cleaning sentances: 984it [1:12:13,  2.93s/it][ATokenizing tekst text into sentances: 984it [1:12:13,  2.93s/it]


reducing sentences to a single doc embeding: 985it [1:12:37,  9.88s/it][A[A[A

embedding all sents with BERT: 985it [1:12:37,  9.88s/it][A[A
cleaning sentances: 985it [1:12:39,  9.88s/it][ATokenizing tekst text into sentances: 985it [1:12:39,  9.88s/it]


reducing sentences to a single doc embeding: 986it [1:12:39,  7.52s/it][A[A[A

embedding all sents with BERT: 986it [1:12:39,  7.52s/it][A[A
cleaning sentances: 986it [1:12:42,  7.52s/it][ATokenizing tekst text into sentances: 986it [1:12:42,  7.52s/it]


reducing sentences to a single doc embeding: 987it [1:12:48,  7.79s/it][A[A[A

embedding all sents with BERT: 987it [1:12:48,  7.79s/it][A[A
cleaning sentances: 987it [1:12:50,  7.79s/it][ATokenizing tekst text into sentances: 987it [1:12:50,  7.79s/it]


reducing sentences to a single doc embeding: 988it [1:12:48,  5.66s/it][A[A[A

embedding all sents with BERT: 988it [1:12:48,  5.66s/it][A[A
cleaning sentances: 988it [1:12:51,  5.66s/it][ATokenizing tekst text into sentances: 988it [1:12:51,  5.66s/it]


reducing sentences to a single doc embeding: 989it [1:12:48,  4.01s/it][A[A[A

embedding all sents with BERT: 989it [1:12:48,  4.01s/it][A[A
cleaning sentances: 989it [1:12:51,  4.01s/it][ATokenizing tekst text into sentances: 989it [1:12:51,  4.01s/it]


reducing sentences to a single doc embeding: 990it [1:12:50,  3.31s/it][A[A[A

embedding all sents with BERT: 990it [1:12:50,  3.31s/it][A[A
cleaning sentances: 990it [1:12:52,  3.31s/it][ATokenizing tekst text into sentances: 990it [1:12:52,  3.31s/it]


reducing sentences to a single doc embeding: 991it [1:12:55,  3.79s/it][A[A[A

embedding all sents with BERT: 991it [1:12:55,  3.79s/it][A[A
cleaning sentances: 991it [1:12:57,  3.79s/it][ATokenizing tekst text into sentances: 991it [1:12:57,  3.79s/it]


reducing sentences to a single doc embeding: 992it [1:13:00,  4.11s/it][A[A[A

embedding all sents with BERT: 992it [1:13:00,  4.11s/it][A[A
cleaning sentances: 992it [1:13:02,  4.11s/it][ATokenizing tekst text into sentances: 992it [1:13:02,  4.11s/it]


reducing sentences to a single doc embeding: 993it [1:13:09,  5.59s/it][A[A[A

embedding all sents with BERT: 993it [1:13:09,  5.59s/it][A[A
cleaning sentances: 993it [1:13:11,  5.59s/it][ATokenizing tekst text into sentances: 993it [1:13:11,  5.59s/it]


reducing sentences to a single doc embeding: 994it [1:13:15,  5.77s/it][A[A[A

embedding all sents with BERT: 994it [1:13:15,  5.77s/it][A[A
cleaning sentances: 994it [1:13:17,  5.77s/it][ATokenizing tekst text into sentances: 994it [1:13:17,  5.77s/it]


reducing sentences to a single doc embeding: 995it [1:13:26,  7.21s/it][A[A[A

embedding all sents with BERT: 995it [1:13:26,  7.21s/it][A[A
cleaning sentances: 995it [1:13:28,  7.21s/it][ATokenizing tekst text into sentances: 995it [1:13:28,  7.21s/it]


reducing sentences to a single doc embeding: 996it [1:13:42, 10.08s/it][A[A[A

embedding all sents with BERT: 996it [1:13:42, 10.08s/it][A[A
cleaning sentances: 996it [1:13:45, 10.08s/it][ATokenizing tekst text into sentances: 996it [1:13:45, 10.08s/it]


reducing sentences to a single doc embeding: 997it [1:13:49,  9.10s/it][A[A[A

embedding all sents with BERT: 997it [1:13:49,  9.10s/it][A[A
cleaning sentances: 997it [1:13:52,  9.10s/it][ATokenizing tekst text into sentances: 997it [1:13:52,  9.10s/it]


reducing sentences to a single doc embeding: 998it [1:13:55,  8.17s/it][A[A[A

embedding all sents with BERT: 998it [1:13:55,  8.17s/it][A[A
cleaning sentances: 998it [1:13:58,  8.17s/it][ATokenizing tekst text into sentances: 998it [1:13:58,  8.17s/it]


reducing sentences to a single doc embeding: 999it [1:13:59,  6.94s/it][A[A[A

embedding all sents with BERT: 999it [1:13:59,  6.94s/it][A[A
cleaning sentances: 999it [1:14:02,  6.94s/it][ATokenizing tekst text into sentances: 999it [1:14:02,  6.94s/it]


reducing sentences to a single doc embeding: 1000it [1:14:01,  5.25s/it][A[A[A

embedding all sents with BERT: 1000it [1:14:01,  5.25s/it][A[A
cleaning sentances: 1000it [1:14:03,  5.25s/it][ATokenizing tekst text into sentances: 1000it [1:14:03,  5.25s/it]


reducing sentences to a single doc embeding: 1001it [1:14:01,  3.76s/it][A[A[A

embedding all sents with BERT: 1001it [1:14:01,  3.76s/it][A[A
cleaning sentances: 1001it [1:14:03,  3.76s/it][ATokenizing tekst text into sentances: 1001it [1:14:03,  3.76s/it]


reducing sentences to a single doc embeding: 1002it [1:14:02,  2.82s/it][A[A[A

embedding all sents with BERT: 1002it [1:14:02,  2.82s/it][A[A
cleaning sentances: 1002it [1:14:04,  2.82s/it][ATokenizing tekst text into sentances: 1002it [1:14:04,  2.82s/it]


reducing sentences to a single doc embeding: 1003it [1:14:02,  2.18s/it][A[A[A

embedding all sents with BERT: 1003it [1:14:02,  2.18s/it][A[A
cleaning sentances: 1003it [1:14:05,  2.18s/it][ATokenizing tekst text into sentances: 1003it [1:14:05,  2.18s/it]


reducing sentences to a single doc embeding: 1004it [1:14:57, 18.04s/it][A[A[A

embedding all sents with BERT: 1004it [1:14:57, 18.04s/it][A[A
cleaning sentances: 1004it [1:15:00, 18.04s/it][ATokenizing tekst text into sentances: 1004it [1:15:00, 18.04s/it]


reducing sentences to a single doc embeding: 1005it [1:15:02, 14.06s/it][A[A[A

embedding all sents with BERT: 1005it [1:15:02, 14.06s/it][A[A
cleaning sentances: 1005it [1:15:04, 14.06s/it][ATokenizing tekst text into sentances: 1005it [1:15:04, 14.06s/it]


reducing sentences to a single doc embeding: 1006it [1:15:03, 10.08s/it][A[A[A

embedding all sents with BERT: 1006it [1:15:03, 10.08s/it][A[A
cleaning sentances: 1006it [1:15:05, 10.08s/it][ATokenizing tekst text into sentances: 1006it [1:15:05, 10.08s/it]


reducing sentences to a single doc embeding: 1007it [1:15:04,  7.27s/it][A[A[A

embedding all sents with BERT: 1007it [1:15:04,  7.27s/it][A[A
cleaning sentances: 1007it [1:15:06,  7.27s/it][ATokenizing tekst text into sentances: 1007it [1:15:06,  7.27s/it]


reducing sentences to a single doc embeding: 1008it [1:15:06,  5.76s/it][A[A[A

embedding all sents with BERT: 1008it [1:15:06,  5.76s/it][A[A
cleaning sentances: 1008it [1:15:08,  5.76s/it][ATokenizing tekst text into sentances: 1008it [1:15:08,  5.76s/it]


reducing sentences to a single doc embeding: 1009it [1:15:07,  4.28s/it][A[A[A

embedding all sents with BERT: 1009it [1:15:07,  4.28s/it][A[A
cleaning sentances: 1009it [1:15:09,  4.28s/it][ATokenizing tekst text into sentances: 1009it [1:15:09,  4.28s/it]


reducing sentences to a single doc embeding: 1010it [1:15:07,  3.20s/it][A[A[A

embedding all sents with BERT: 1010it [1:15:07,  3.20s/it][A[A
cleaning sentances: 1010it [1:15:10,  3.20s/it][ATokenizing tekst text into sentances: 1010it [1:15:10,  3.20s/it]


reducing sentences to a single doc embeding: 1011it [1:15:08,  2.34s/it][A[A[A

embedding all sents with BERT: 1011it [1:15:08,  2.34s/it][A[A
cleaning sentances: 1011it [1:15:10,  2.34s/it][ATokenizing tekst text into sentances: 1011it [1:15:10,  2.34s/it]


reducing sentences to a single doc embeding: 1012it [1:15:31,  8.54s/it][A[A[A

embedding all sents with BERT: 1012it [1:15:31,  8.54s/it][A[A
cleaning sentances: 1012it [1:15:33,  8.54s/it][ATokenizing tekst text into sentances: 1012it [1:15:33,  8.54s/it]


reducing sentences to a single doc embeding: 1013it [1:15:32,  6.34s/it][A[A[A

embedding all sents with BERT: 1013it [1:15:32,  6.34s/it][A[A
cleaning sentances: 1013it [1:15:34,  6.34s/it][ATokenizing tekst text into sentances: 1013it [1:15:34,  6.34s/it]


reducing sentences to a single doc embeding: 1014it [1:15:32,  4.59s/it][A[A[A

embedding all sents with BERT: 1014it [1:15:32,  4.59s/it][A[A
cleaning sentances: 1014it [1:15:35,  4.59s/it][ATokenizing tekst text into sentances: 1014it [1:15:35,  4.59s/it]


reducing sentences to a single doc embeding: 1015it [1:15:33,  3.51s/it][A[A[A

embedding all sents with BERT: 1015it [1:15:33,  3.51s/it][A[A
cleaning sentances: 1015it [1:15:36,  3.51s/it][ATokenizing tekst text into sentances: 1015it [1:15:36,  3.51s/it]


reducing sentences to a single doc embeding: 1016it [1:15:34,  2.55s/it][A[A[A

embedding all sents with BERT: 1016it [1:15:34,  2.55s/it][A[A
cleaning sentances: 1016it [1:15:36,  2.55s/it][ATokenizing tekst text into sentances: 1016it [1:15:36,  2.55s/it]


reducing sentences to a single doc embeding: 1017it [1:16:35, 20.29s/it][A[A[A

embedding all sents with BERT: 1017it [1:16:35, 20.29s/it][A[A
cleaning sentances: 1017it [1:16:38, 20.29s/it][ATokenizing tekst text into sentances: 1017it [1:16:38, 20.29s/it]


reducing sentences to a single doc embeding: 1018it [1:16:51, 18.89s/it][A[A[A

embedding all sents with BERT: 1018it [1:16:51, 18.89s/it][A[A
cleaning sentances: 1018it [1:16:53, 18.89s/it][ATokenizing tekst text into sentances: 1018it [1:16:53, 18.89s/it]


reducing sentences to a single doc embeding: 1019it [1:16:51, 13.33s/it][A[A[A

embedding all sents with BERT: 1019it [1:16:51, 13.33s/it][A[A
cleaning sentances: 1019it [1:16:54, 13.33s/it][ATokenizing tekst text into sentances: 1019it [1:16:54, 13.33s/it]


reducing sentences to a single doc embeding: 1020it [1:16:52,  9.40s/it][A[A[A

embedding all sents with BERT: 1020it [1:16:52,  9.40s/it][A[A
cleaning sentances: 1020it [1:16:54,  9.40s/it][ATokenizing tekst text into sentances: 1020it [1:16:54,  9.40s/it]


reducing sentences to a single doc embeding: 1021it [1:16:52,  6.65s/it][A[A[A

embedding all sents with BERT: 1021it [1:16:52,  6.65s/it][A[A
cleaning sentances: 1021it [1:16:54,  6.65s/it][ATokenizing tekst text into sentances: 1021it [1:16:54,  6.65s/it]


reducing sentences to a single doc embeding: 1022it [1:16:52,  4.87s/it][A[A[A

embedding all sents with BERT: 1022it [1:16:53,  4.87s/it][A[A
cleaning sentances: 1022it [1:16:55,  4.87s/it][ATokenizing tekst text into sentances: 1022it [1:16:55,  4.87s/it]


reducing sentences to a single doc embeding: 1023it [1:17:24, 12.79s/it][A[A[A

embedding all sents with BERT: 1023it [1:17:24, 12.79s/it][A[A
cleaning sentances: 1023it [1:17:26, 12.79s/it][ATokenizing tekst text into sentances: 1023it [1:17:26, 12.79s/it]


reducing sentences to a single doc embeding: 1024it [1:17:30, 10.79s/it][A[A[A

embedding all sents with BERT: 1024it [1:17:30, 10.79s/it][A[A
cleaning sentances: 1024it [1:17:32, 10.79s/it][ATokenizing tekst text into sentances: 1024it [1:17:32, 10.79s/it]


reducing sentences to a single doc embeding: 1025it [1:17:31,  7.90s/it][A[A[A

embedding all sents with BERT: 1025it [1:17:31,  7.90s/it][A[A
cleaning sentances: 1025it [1:17:33,  7.90s/it][ATokenizing tekst text into sentances: 1025it [1:17:33,  7.90s/it]


reducing sentences to a single doc embeding: 1026it [1:17:39,  7.79s/it][A[A[A

embedding all sents with BERT: 1026it [1:17:39,  7.79s/it][A[A
cleaning sentances: 1026it [1:17:41,  7.79s/it][ATokenizing tekst text into sentances: 1026it [1:17:41,  7.79s/it]


reducing sentences to a single doc embeding: 1027it [1:17:41,  6.15s/it][A[A[A

embedding all sents with BERT: 1027it [1:17:41,  6.15s/it][A[A
cleaning sentances: 1027it [1:17:43,  6.15s/it][ATokenizing tekst text into sentances: 1027it [1:17:43,  6.15s/it]


reducing sentences to a single doc embeding: 1028it [1:17:43,  5.07s/it][A[A[A

embedding all sents with BERT: 1028it [1:17:43,  5.07s/it][A[A
cleaning sentances: 1028it [1:17:46,  5.07s/it][ATokenizing tekst text into sentances: 1028it [1:17:46,  5.07s/it]


reducing sentences to a single doc embeding: 1029it [1:17:44,  3.74s/it][A[A[A

embedding all sents with BERT: 1029it [1:17:44,  3.74s/it][A[A
cleaning sentances: 1029it [1:17:46,  3.74s/it][ATokenizing tekst text into sentances: 1029it [1:17:46,  3.74s/it]


reducing sentences to a single doc embeding: 1030it [1:17:52,  4.97s/it][A[A[A

embedding all sents with BERT: 1030it [1:17:52,  4.97s/it][A[A
cleaning sentances: 1030it [1:17:54,  4.97s/it][ATokenizing tekst text into sentances: 1030it [1:17:54,  4.97s/it]


reducing sentences to a single doc embeding: 1031it [1:17:53,  3.66s/it][A[A[A

embedding all sents with BERT: 1031it [1:17:53,  3.66s/it][A[A
cleaning sentances: 1031it [1:17:55,  3.66s/it][ATokenizing tekst text into sentances: 1031it [1:17:55,  3.66s/it]


reducing sentences to a single doc embeding: 1032it [1:17:54,  2.96s/it][A[A[A

embedding all sents with BERT: 1032it [1:17:54,  2.96s/it][A[A
cleaning sentances: 1032it [1:17:56,  2.96s/it][ATokenizing tekst text into sentances: 1032it [1:17:56,  2.96s/it]


reducing sentences to a single doc embeding: 1033it [1:17:58,  3.20s/it][A[A[A

embedding all sents with BERT: 1033it [1:17:58,  3.20s/it][A[A
cleaning sentances: 1033it [1:18:00,  3.20s/it][ATokenizing tekst text into sentances: 1033it [1:18:00,  3.20s/it]


reducing sentences to a single doc embeding: 1034it [1:18:00,  2.99s/it][A[A[A

embedding all sents with BERT: 1034it [1:18:00,  2.99s/it][A[A
cleaning sentances: 1034it [1:18:02,  2.99s/it][ATokenizing tekst text into sentances: 1034it [1:18:02,  2.99s/it]


reducing sentences to a single doc embeding: 1035it [1:18:05,  3.45s/it][A[A[A

embedding all sents with BERT: 1035it [1:18:05,  3.45s/it][A[A
cleaning sentances: 1035it [1:18:07,  3.45s/it][ATokenizing tekst text into sentances: 1035it [1:18:07,  3.45s/it]


reducing sentences to a single doc embeding: 1036it [1:18:05,  2.60s/it][A[A[A

embedding all sents with BERT: 1036it [1:18:05,  2.60s/it][A[A
cleaning sentances: 1036it [1:18:08,  2.60s/it][ATokenizing tekst text into sentances: 1036it [1:18:08,  2.60s/it]


reducing sentences to a single doc embeding: 1037it [1:18:06,  2.02s/it][A[A[A

embedding all sents with BERT: 1037it [1:18:06,  2.02s/it][A[A
cleaning sentances: 1037it [1:18:08,  2.02s/it][ATokenizing tekst text into sentances: 1037it [1:18:08,  2.02s/it]


reducing sentences to a single doc embeding: 1038it [1:18:07,  1.61s/it][A[A[A

embedding all sents with BERT: 1038it [1:18:07,  1.61s/it][A[A
cleaning sentances: 1038it [1:18:09,  1.61s/it][ATokenizing tekst text into sentances: 1038it [1:18:09,  1.61s/it]


reducing sentences to a single doc embeding: 1039it [1:18:08,  1.64s/it][A[A[A

embedding all sents with BERT: 1039it [1:18:08,  1.64s/it][A[A
cleaning sentances: 1039it [1:18:11,  1.64s/it][ATokenizing tekst text into sentances: 1039it [1:18:11,  1.64s/it]


reducing sentences to a single doc embeding: 1040it [1:18:09,  1.27s/it][A[A[A

embedding all sents with BERT: 1040it [1:18:09,  1.27s/it][A[A
cleaning sentances: 1040it [1:18:11,  1.27s/it][ATokenizing tekst text into sentances: 1040it [1:18:11,  1.27s/it]


reducing sentences to a single doc embeding: 1041it [1:18:23,  5.10s/it][A[A[A

embedding all sents with BERT: 1041it [1:18:23,  5.10s/it][A[A
cleaning sentances: 1041it [1:18:25,  5.10s/it][ATokenizing tekst text into sentances: 1041it [1:18:25,  5.10s/it]


reducing sentences to a single doc embeding: 1042it [1:18:23,  3.69s/it][A[A[A

embedding all sents with BERT: 1042it [1:18:23,  3.69s/it][A[A
cleaning sentances: 1042it [1:18:25,  3.69s/it][ATokenizing tekst text into sentances: 1042it [1:18:25,  3.69s/it]


reducing sentences to a single doc embeding: 1043it [1:18:34,  5.70s/it][A[A[A

embedding all sents with BERT: 1043it [1:18:34,  5.70s/it][A[A
cleaning sentances: 1043it [1:18:36,  5.70s/it][ATokenizing tekst text into sentances: 1043it [1:18:36,  5.70s/it]


reducing sentences to a single doc embeding: 1044it [1:18:35,  4.29s/it][A[A[A

embedding all sents with BERT: 1044it [1:18:35,  4.29s/it][A[A
cleaning sentances: 1044it [1:18:37,  4.29s/it][ATokenizing tekst text into sentances: 1044it [1:18:37,  4.29s/it]


reducing sentences to a single doc embeding: 1045it [1:18:45,  6.13s/it][A[A[A

embedding all sents with BERT: 1045it [1:18:45,  6.13s/it][A[A
cleaning sentances: 1045it [1:18:47,  6.13s/it][ATokenizing tekst text into sentances: 1045it [1:18:47,  6.13s/it]


reducing sentences to a single doc embeding: 1046it [1:18:51,  5.97s/it][A[A[A

embedding all sents with BERT: 1046it [1:18:51,  5.97s/it][A[A
cleaning sentances: 1046it [1:18:53,  5.97s/it][ATokenizing tekst text into sentances: 1046it [1:18:53,  5.97s/it]


reducing sentences to a single doc embeding: 1047it [1:18:51,  4.26s/it][A[A[A

embedding all sents with BERT: 1047it [1:18:51,  4.26s/it][A[A
cleaning sentances: 1047it [1:18:53,  4.26s/it][ATokenizing tekst text into sentances: 1047it [1:18:53,  4.26s/it]


reducing sentences to a single doc embeding: 1048it [1:18:53,  3.73s/it][A[A[A

embedding all sents with BERT: 1048it [1:18:53,  3.73s/it][A[A
cleaning sentances: 1048it [1:18:56,  3.73s/it][ATokenizing tekst text into sentances: 1048it [1:18:56,  3.73s/it]


reducing sentences to a single doc embeding: 1049it [1:18:53,  2.66s/it][A[A[A

embedding all sents with BERT: 1049it [1:18:53,  2.66s/it][A[A
cleaning sentances: 1049it [1:18:56,  2.66s/it][ATokenizing tekst text into sentances: 1049it [1:18:56,  2.66s/it]


reducing sentences to a single doc embeding: 1050it [1:18:56,  2.65s/it][A[A[A

embedding all sents with BERT: 1050it [1:18:56,  2.65s/it][A[A
cleaning sentances: 1050it [1:18:58,  2.65s/it][ATokenizing tekst text into sentances: 1050it [1:18:58,  2.65s/it]


reducing sentences to a single doc embeding: 1051it [1:18:57,  2.05s/it][A[A[A

embedding all sents with BERT: 1051it [1:18:57,  2.05s/it][A[A
cleaning sentances: 1051it [1:18:59,  2.05s/it][ATokenizing tekst text into sentances: 1051it [1:18:59,  2.05s/it]


reducing sentences to a single doc embeding: 1052it [1:19:09,  5.26s/it][A[A[A

embedding all sents with BERT: 1052it [1:19:09,  5.26s/it][A[A
cleaning sentances: 1052it [1:19:12,  5.26s/it][ATokenizing tekst text into sentances: 1052it [1:19:12,  5.26s/it]


reducing sentences to a single doc embeding: 1053it [1:19:12,  4.34s/it][A[A[A

embedding all sents with BERT: 1053it [1:19:12,  4.34s/it][A[A
cleaning sentances: 1053it [1:19:14,  4.34s/it][ATokenizing tekst text into sentances: 1053it [1:19:14,  4.34s/it]


reducing sentences to a single doc embeding: 1054it [1:19:16,  4.19s/it][A[A[A

embedding all sents with BERT: 1054it [1:19:16,  4.19s/it][A[A
cleaning sentances: 1054it [1:19:18,  4.19s/it][ATokenizing tekst text into sentances: 1054it [1:19:18,  4.19s/it]


reducing sentences to a single doc embeding: 1055it [1:19:18,  3.60s/it][A[A[A

embedding all sents with BERT: 1055it [1:19:18,  3.60s/it][A[A
cleaning sentances: 1055it [1:19:20,  3.60s/it][ATokenizing tekst text into sentances: 1055it [1:19:20,  3.60s/it]


reducing sentences to a single doc embeding: 1056it [1:19:19,  2.82s/it][A[A[A

embedding all sents with BERT: 1056it [1:19:19,  2.82s/it][A[A
cleaning sentances: 1056it [1:19:21,  2.82s/it][ATokenizing tekst text into sentances: 1056it [1:19:21,  2.82s/it]


reducing sentences to a single doc embeding: 1057it [1:19:21,  2.74s/it][A[A[A

embedding all sents with BERT: 1057it [1:19:21,  2.74s/it][A[A
cleaning sentances: 1057it [1:19:24,  2.74s/it][ATokenizing tekst text into sentances: 1057it [1:19:24,  2.74s/it]


reducing sentences to a single doc embeding: 1058it [1:19:28,  4.06s/it][A[A[A

embedding all sents with BERT: 1058it [1:19:28,  4.06s/it][A[A
cleaning sentances: 1058it [1:19:31,  4.06s/it][ATokenizing tekst text into sentances: 1058it [1:19:31,  4.06s/it]


reducing sentences to a single doc embeding: 1059it [1:19:29,  2.91s/it][A[A[A

embedding all sents with BERT: 1059it [1:19:29,  2.91s/it][A[A
cleaning sentances: 1059it [1:19:31,  2.91s/it][ATokenizing tekst text into sentances: 1059it [1:19:31,  2.91s/it]


reducing sentences to a single doc embeding: 1060it [1:19:34,  3.79s/it][A[A[A

embedding all sents with BERT: 1060it [1:19:34,  3.79s/it][A[A
cleaning sentances: 1060it [1:19:37,  3.79s/it][ATokenizing tekst text into sentances: 1060it [1:19:37,  3.79s/it]


reducing sentences to a single doc embeding: 1061it [1:19:35,  2.89s/it][A[A[A

embedding all sents with BERT: 1061it [1:19:35,  2.89s/it][A[A
cleaning sentances: 1061it [1:19:38,  2.89s/it][ATokenizing tekst text into sentances: 1061it [1:19:38,  2.89s/it]


reducing sentences to a single doc embeding: 1062it [1:19:36,  2.22s/it][A[A[A

embedding all sents with BERT: 1062it [1:19:36,  2.22s/it][A[A
cleaning sentances: 1062it [1:19:38,  2.22s/it][ATokenizing tekst text into sentances: 1062it [1:19:38,  2.22s/it]


reducing sentences to a single doc embeding: 1063it [1:19:36,  1.63s/it][A[A[A

embedding all sents with BERT: 1063it [1:19:36,  1.63s/it][A[A
cleaning sentances: 1063it [1:19:39,  1.63s/it][ATokenizing tekst text into sentances: 1063it [1:19:39,  1.63s/it]


reducing sentences to a single doc embeding: 1064it [1:19:37,  1.27s/it][A[A[A

embedding all sents with BERT: 1064it [1:19:37,  1.27s/it][A[A
cleaning sentances: 1064it [1:19:39,  1.27s/it][ATokenizing tekst text into sentances: 1064it [1:19:39,  1.27s/it]


reducing sentences to a single doc embeding: 1065it [1:19:41,  2.34s/it][A[A[A

embedding all sents with BERT: 1065it [1:19:41,  2.34s/it][A[A
cleaning sentances: 1065it [1:19:44,  2.34s/it][ATokenizing tekst text into sentances: 1065it [1:19:44,  2.34s/it]


reducing sentences to a single doc embeding: 1066it [1:19:44,  2.36s/it][A[A[A

embedding all sents with BERT: 1066it [1:19:44,  2.36s/it][A[A
cleaning sentances: 1066it [1:19:46,  2.36s/it][ATokenizing tekst text into sentances: 1066it [1:19:46,  2.36s/it]


reducing sentences to a single doc embeding: 1067it [1:19:45,  1.98s/it][A[A[A

embedding all sents with BERT: 1067it [1:19:45,  1.98s/it][A[A
cleaning sentances: 1067it [1:19:47,  1.98s/it][ATokenizing tekst text into sentances: 1067it [1:19:47,  1.98s/it]


reducing sentences to a single doc embeding: 1068it [1:19:59,  5.47s/it][A[A[A

embedding all sents with BERT: 1068it [1:19:59,  5.47s/it][A[A
cleaning sentances: 1068it [1:20:01,  5.47s/it][ATokenizing tekst text into sentances: 1068it [1:20:01,  5.47s/it]


reducing sentences to a single doc embeding: 1069it [1:19:59,  4.06s/it][A[A[A

embedding all sents with BERT: 1069it [1:19:59,  4.06s/it][A[A
cleaning sentances: 1069it [1:20:02,  4.06s/it][ATokenizing tekst text into sentances: 1069it [1:20:02,  4.06s/it]


reducing sentences to a single doc embeding: 1070it [1:20:00,  3.16s/it][A[A[A

embedding all sents with BERT: 1070it [1:20:00,  3.16s/it][A[A
cleaning sentances: 1070it [1:20:03,  3.16s/it][ATokenizing tekst text into sentances: 1070it [1:20:03,  3.16s/it]


reducing sentences to a single doc embeding: 1071it [1:20:01,  2.41s/it][A[A[A

embedding all sents with BERT: 1071it [1:20:01,  2.41s/it][A[A
cleaning sentances: 1071it [1:20:03,  2.41s/it][ATokenizing tekst text into sentances: 1071it [1:20:03,  2.41s/it]


reducing sentences to a single doc embeding: 1072it [1:20:02,  1.98s/it][A[A[A

embedding all sents with BERT: 1072it [1:20:02,  1.98s/it][A[A
cleaning sentances: 1072it [1:20:04,  1.98s/it][ATokenizing tekst text into sentances: 1072it [1:20:04,  1.98s/it]


reducing sentences to a single doc embeding: 1073it [1:20:19,  6.48s/it][A[A[A

embedding all sents with BERT: 1073it [1:20:19,  6.48s/it][A[A
cleaning sentances: 1073it [1:20:21,  6.48s/it][ATokenizing tekst text into sentances: 1073it [1:20:21,  6.48s/it]


reducing sentences to a single doc embeding: 1074it [1:20:30,  7.86s/it][A[A[A

embedding all sents with BERT: 1074it [1:20:30,  7.86s/it][A[A
cleaning sentances: 1074it [1:20:32,  7.86s/it][ATokenizing tekst text into sentances: 1074it [1:20:32,  7.86s/it]


reducing sentences to a single doc embeding: 1075it [1:20:30,  5.56s/it][A[A[A

embedding all sents with BERT: 1075it [1:20:30,  5.56s/it][A[A
cleaning sentances: 1075it [1:20:33,  5.56s/it][ATokenizing tekst text into sentances: 1075it [1:20:33,  5.56s/it]


reducing sentences to a single doc embeding: 1076it [1:20:31,  4.13s/it][A[A[A

embedding all sents with BERT: 1076it [1:20:31,  4.13s/it][A[A
cleaning sentances: 1076it [1:20:33,  4.13s/it][ATokenizing tekst text into sentances: 1076it [1:20:33,  4.13s/it]


reducing sentences to a single doc embeding: 1077it [1:20:32,  3.04s/it][A[A[A

embedding all sents with BERT: 1077it [1:20:32,  3.04s/it][A[A
cleaning sentances: 1077it [1:20:34,  3.04s/it][ATokenizing tekst text into sentances: 1077it [1:20:34,  3.04s/it]


reducing sentences to a single doc embeding: 1078it [1:20:40,  4.55s/it][A[A[A

embedding all sents with BERT: 1078it [1:20:40,  4.55s/it][A[A
cleaning sentances: 1078it [1:20:42,  4.55s/it][ATokenizing tekst text into sentances: 1078it [1:20:42,  4.55s/it]


reducing sentences to a single doc embeding: 1079it [1:20:41,  3.61s/it][A[A[A

embedding all sents with BERT: 1079it [1:20:41,  3.61s/it][A[A
cleaning sentances: 1079it [1:20:43,  3.61s/it][ATokenizing tekst text into sentances: 1079it [1:20:43,  3.61s/it]


reducing sentences to a single doc embeding: 1080it [1:20:44,  3.38s/it][A[A[A

embedding all sents with BERT: 1080it [1:20:44,  3.38s/it][A[A
cleaning sentances: 1080it [1:20:46,  3.38s/it][ATokenizing tekst text into sentances: 1080it [1:20:46,  3.38s/it]


reducing sentences to a single doc embeding: 1081it [1:20:45,  2.64s/it][A[A[A

embedding all sents with BERT: 1081it [1:20:45,  2.64s/it][A[A
cleaning sentances: 1081it [1:20:47,  2.64s/it][ATokenizing tekst text into sentances: 1081it [1:20:47,  2.64s/it]


reducing sentences to a single doc embeding: 1082it [1:20:45,  1.97s/it][A[A[A

embedding all sents with BERT: 1082it [1:20:45,  1.97s/it][A[A
cleaning sentances: 1082it [1:20:48,  1.97s/it][ATokenizing tekst text into sentances: 1082it [1:20:48,  1.97s/it]


reducing sentences to a single doc embeding: 1083it [1:20:45,  1.44s/it][A[A[A

embedding all sents with BERT: 1083it [1:20:45,  1.44s/it][A[A
cleaning sentances: 1083it [1:20:48,  1.44s/it][ATokenizing tekst text into sentances: 1083it [1:20:48,  1.44s/it]


reducing sentences to a single doc embeding: 1084it [1:20:51,  2.81s/it][A[A[A

embedding all sents with BERT: 1084it [1:20:51,  2.81s/it][A[A
cleaning sentances: 1084it [1:20:54,  2.81s/it][ATokenizing tekst text into sentances: 1084it [1:20:54,  2.81s/it]


reducing sentences to a single doc embeding: 1085it [1:20:52,  2.13s/it][A[A[A

embedding all sents with BERT: 1085it [1:20:52,  2.13s/it][A[A
cleaning sentances: 1085it [1:20:54,  2.13s/it][ATokenizing tekst text into sentances: 1085it [1:20:54,  2.13s/it]


reducing sentences to a single doc embeding: 1086it [1:20:53,  1.71s/it][A[A[A

embedding all sents with BERT: 1086it [1:20:53,  1.71s/it][A[A
cleaning sentances: 1086it [1:20:55,  1.71s/it][ATokenizing tekst text into sentances: 1086it [1:20:55,  1.71s/it]


reducing sentences to a single doc embeding: 1087it [1:21:05,  4.78s/it][A[A[A

embedding all sents with BERT: 1087it [1:21:05,  4.78s/it][A[A
cleaning sentances: 1087it [1:21:07,  4.78s/it][ATokenizing tekst text into sentances: 1087it [1:21:07,  4.78s/it]


reducing sentences to a single doc embeding: 1088it [1:21:05,  3.56s/it][A[A[A

embedding all sents with BERT: 1088it [1:21:05,  3.56s/it][A[A
cleaning sentances: 1088it [1:21:08,  3.56s/it][ATokenizing tekst text into sentances: 1088it [1:21:08,  3.56s/it]


reducing sentences to a single doc embeding: 1089it [1:21:11,  4.29s/it][A[A[A

embedding all sents with BERT: 1089it [1:21:11,  4.29s/it][A[A
cleaning sentances: 1089it [1:21:14,  4.29s/it][ATokenizing tekst text into sentances: 1089it [1:21:14,  4.29s/it]


reducing sentences to a single doc embeding: 1090it [1:21:12,  3.15s/it][A[A[A

embedding all sents with BERT: 1090it [1:21:12,  3.15s/it][A[A
cleaning sentances: 1090it [1:21:14,  3.15s/it][ATokenizing tekst text into sentances: 1090it [1:21:14,  3.15s/it]


reducing sentences to a single doc embeding: 1091it [1:21:12,  2.32s/it][A[A[A

embedding all sents with BERT: 1091it [1:21:12,  2.32s/it][A[A
cleaning sentances: 1091it [1:21:15,  2.32s/it][ATokenizing tekst text into sentances: 1091it [1:21:15,  2.32s/it]


reducing sentences to a single doc embeding: 1092it [1:21:13,  1.77s/it][A[A[A

embedding all sents with BERT: 1092it [1:21:13,  1.77s/it][A[A
cleaning sentances: 1092it [1:21:15,  1.77s/it][ATokenizing tekst text into sentances: 1092it [1:21:15,  1.77s/it]


reducing sentences to a single doc embeding: 1093it [1:21:13,  1.27s/it][A[A[A

embedding all sents with BERT: 1093it [1:21:13,  1.27s/it][A[A
cleaning sentances: 1093it [1:21:15,  1.27s/it][ATokenizing tekst text into sentances: 1093it [1:21:15,  1.27s/it]


reducing sentences to a single doc embeding: 1094it [1:21:21,  3.47s/it][A[A[A

embedding all sents with BERT: 1094it [1:21:21,  3.47s/it][A[A
cleaning sentances: 1094it [1:21:24,  3.47s/it][ATokenizing tekst text into sentances: 1094it [1:21:24,  3.47s/it]


reducing sentences to a single doc embeding: 1095it [1:21:22,  2.59s/it][A[A[A

embedding all sents with BERT: 1095it [1:21:22,  2.59s/it][A[A
cleaning sentances: 1095it [1:21:24,  2.59s/it][ATokenizing tekst text into sentances: 1095it [1:21:24,  2.59s/it]


reducing sentences to a single doc embeding: 1096it [1:21:23,  2.00s/it][A[A[A

embedding all sents with BERT: 1096it [1:21:23,  2.00s/it][A[A
cleaning sentances: 1096it [1:21:25,  2.00s/it][ATokenizing tekst text into sentances: 1096it [1:21:25,  2.00s/it]


reducing sentences to a single doc embeding: 1097it [1:21:23,  1.54s/it][A[A[A

embedding all sents with BERT: 1097it [1:21:23,  1.54s/it][A[A
cleaning sentances: 1097it [1:21:25,  1.54s/it][ATokenizing tekst text into sentances: 1097it [1:21:25,  1.54s/it]


reducing sentences to a single doc embeding: 1098it [1:21:43,  7.14s/it][A[A[A

embedding all sents with BERT: 1098it [1:21:43,  7.14s/it][A[A
cleaning sentances: 1098it [1:21:46,  7.14s/it][ATokenizing tekst text into sentances: 1098it [1:21:46,  7.14s/it]


reducing sentences to a single doc embeding: 1099it [1:21:44,  5.12s/it][A[A[A

embedding all sents with BERT: 1099it [1:21:44,  5.12s/it][A[A
cleaning sentances: 1099it [1:21:46,  5.12s/it][ATokenizing tekst text into sentances: 1099it [1:21:46,  5.12s/it]


reducing sentences to a single doc embeding: 1100it [1:22:29, 17.04s/it][A[A[A

embedding all sents with BERT: 1100it [1:22:29, 17.04s/it][A[A
cleaning sentances: 1100it [1:22:31, 17.04s/it][ATokenizing tekst text into sentances: 1100it [1:22:31, 17.04s/it]


reducing sentences to a single doc embeding: 1101it [1:22:29, 12.15s/it][A[A[A

embedding all sents with BERT: 1101it [1:22:29, 12.15s/it][A[A
cleaning sentances: 1101it [1:22:32, 12.15s/it][ATokenizing tekst text into sentances: 1101it [1:22:32, 12.15s/it]


reducing sentences to a single doc embeding: 1102it [1:22:30,  8.61s/it][A[A[A

embedding all sents with BERT: 1102it [1:22:30,  8.61s/it][A[A
cleaning sentances: 1102it [1:22:32,  8.61s/it][ATokenizing tekst text into sentances: 1102it [1:22:32,  8.61s/it]


reducing sentences to a single doc embeding: 1103it [1:22:31,  6.44s/it][A[A[A

embedding all sents with BERT: 1103it [1:22:31,  6.44s/it][A[A
cleaning sentances: 1103it [1:22:33,  6.44s/it][ATokenizing tekst text into sentances: 1103it [1:22:33,  6.44s/it]


reducing sentences to a single doc embeding: 1104it [1:22:31,  4.65s/it][A[A[A

embedding all sents with BERT: 1104it [1:22:31,  4.65s/it][A[A
cleaning sentances: 1104it [1:22:34,  4.65s/it][ATokenizing tekst text into sentances: 1104it [1:22:34,  4.65s/it]


reducing sentences to a single doc embeding: 1105it [1:22:37,  4.87s/it][A[A[A

embedding all sents with BERT: 1105it [1:22:37,  4.87s/it][A[A
cleaning sentances: 1105it [1:22:39,  4.87s/it][ATokenizing tekst text into sentances: 1105it [1:22:39,  4.87s/it]


reducing sentences to a single doc embeding: 1106it [1:22:39,  4.11s/it][A[A[A

embedding all sents with BERT: 1106it [1:22:39,  4.11s/it][A[A
cleaning sentances: 1106it [1:22:42,  4.11s/it][ATokenizing tekst text into sentances: 1106it [1:22:42,  4.11s/it]


reducing sentences to a single doc embeding: 1107it [1:22:58,  8.61s/it][A[A[A

embedding all sents with BERT: 1107it [1:22:58,  8.61s/it][A[A
cleaning sentances: 1107it [1:23:01,  8.61s/it][ATokenizing tekst text into sentances: 1107it [1:23:01,  8.61s/it]


reducing sentences to a single doc embeding: 1108it [1:23:00,  6.41s/it][A[A[A

embedding all sents with BERT: 1108it [1:23:00,  6.41s/it][A[A
cleaning sentances: 1108it [1:23:02,  6.41s/it][ATokenizing tekst text into sentances: 1108it [1:23:02,  6.41s/it]


reducing sentences to a single doc embeding: 1109it [1:23:00,  4.55s/it][A[A[A

embedding all sents with BERT: 1109it [1:23:00,  4.55s/it][A[A
cleaning sentances: 1109it [1:23:02,  4.55s/it][ATokenizing tekst text into sentances: 1109it [1:23:02,  4.55s/it]


reducing sentences to a single doc embeding: 1110it [1:23:03,  4.29s/it][A[A[A

embedding all sents with BERT: 1110it [1:23:03,  4.29s/it][A[A
cleaning sentances: 1110it [1:23:06,  4.29s/it][ATokenizing tekst text into sentances: 1110it [1:23:06,  4.29s/it]


reducing sentences to a single doc embeding: 1111it [1:23:04,  3.06s/it][A[A[A

embedding all sents with BERT: 1111it [1:23:04,  3.06s/it][A[A
cleaning sentances: 1111it [1:23:06,  3.06s/it][ATokenizing tekst text into sentances: 1111it [1:23:06,  3.06s/it]


reducing sentences to a single doc embeding: 1112it [1:23:06,  2.88s/it][A[A[A

embedding all sents with BERT: 1112it [1:23:06,  2.88s/it][A[A
cleaning sentances: 1112it [1:23:09,  2.88s/it][ATokenizing tekst text into sentances: 1112it [1:23:09,  2.88s/it]


reducing sentences to a single doc embeding: 1113it [1:23:12,  3.65s/it][A[A[A

embedding all sents with BERT: 1113it [1:23:12,  3.65s/it][A[A
cleaning sentances: 1113it [1:23:14,  3.65s/it][ATokenizing tekst text into sentances: 1113it [1:23:14,  3.65s/it]


reducing sentences to a single doc embeding: 1114it [1:23:12,  2.78s/it][A[A[A

embedding all sents with BERT: 1114it [1:23:12,  2.78s/it][A[A
cleaning sentances: 1114it [1:23:15,  2.78s/it][ATokenizing tekst text into sentances: 1114it [1:23:15,  2.78s/it]


reducing sentences to a single doc embeding: 1115it [1:23:13,  2.20s/it][A[A[A

embedding all sents with BERT: 1115it [1:23:13,  2.20s/it][A[A
cleaning sentances: 1115it [1:23:16,  2.20s/it][ATokenizing tekst text into sentances: 1115it [1:23:16,  2.20s/it]


reducing sentences to a single doc embeding: 1116it [1:23:15,  2.06s/it][A[A[A

embedding all sents with BERT: 1116it [1:23:15,  2.06s/it][A[A
cleaning sentances: 1116it [1:23:17,  2.06s/it][ATokenizing tekst text into sentances: 1116it [1:23:17,  2.06s/it]


reducing sentences to a single doc embeding: 1117it [1:23:19,  2.58s/it][A[A[A

embedding all sents with BERT: 1117it [1:23:19,  2.58s/it][A[A
cleaning sentances: 1117it [1:23:21,  2.58s/it][ATokenizing tekst text into sentances: 1117it [1:23:21,  2.58s/it]


reducing sentences to a single doc embeding: 1118it [1:23:21,  2.60s/it][A[A[A

embedding all sents with BERT: 1118it [1:23:21,  2.60s/it][A[A
cleaning sentances: 1118it [1:23:24,  2.60s/it][ATokenizing tekst text into sentances: 1118it [1:23:24,  2.60s/it]


reducing sentences to a single doc embeding: 1119it [1:23:22,  2.02s/it][A[A[A

embedding all sents with BERT: 1119it [1:23:22,  2.02s/it][A[A
cleaning sentances: 1119it [1:23:24,  2.02s/it][ATokenizing tekst text into sentances: 1119it [1:23:24,  2.02s/it]


reducing sentences to a single doc embeding: 1120it [1:23:22,  1.55s/it][A[A[A

embedding all sents with BERT: 1120it [1:23:22,  1.55s/it][A[A
cleaning sentances: 1120it [1:23:25,  1.55s/it][ATokenizing tekst text into sentances: 1120it [1:23:25,  1.55s/it]


reducing sentences to a single doc embeding: 1121it [1:23:24,  1.39s/it][A[A[A

embedding all sents with BERT: 1121it [1:23:24,  1.39s/it][A[A
cleaning sentances: 1121it [1:23:26,  1.39s/it][ATokenizing tekst text into sentances: 1121it [1:23:26,  1.39s/it]


reducing sentences to a single doc embeding: 1122it [1:23:28,  2.45s/it][A[A[A

embedding all sents with BERT: 1122it [1:23:28,  2.45s/it][A[A
cleaning sentances: 1122it [1:23:31,  2.45s/it][ATokenizing tekst text into sentances: 1122it [1:23:31,  2.45s/it]


reducing sentences to a single doc embeding: 1123it [1:23:29,  2.03s/it][A[A[A

embedding all sents with BERT: 1123it [1:23:29,  2.03s/it][A[A
cleaning sentances: 1123it [1:23:32,  2.03s/it][ATokenizing tekst text into sentances: 1123it [1:23:32,  2.03s/it]


reducing sentences to a single doc embeding: 1124it [1:23:30,  1.61s/it][A[A[A

embedding all sents with BERT: 1124it [1:23:30,  1.61s/it][A[A
cleaning sentances: 1124it [1:23:32,  1.61s/it][ATokenizing tekst text into sentances: 1124it [1:23:32,  1.61s/it]


reducing sentences to a single doc embeding: 1125it [1:23:52,  7.61s/it][A[A[A

embedding all sents with BERT: 1125it [1:23:52,  7.61s/it][A[A
cleaning sentances: 1125it [1:23:54,  7.61s/it][ATokenizing tekst text into sentances: 1125it [1:23:54,  7.61s/it]


reducing sentences to a single doc embeding: 1126it [1:23:57,  6.80s/it][A[A[A

embedding all sents with BERT: 1126it [1:23:57,  6.81s/it][A[A
cleaning sentances: 1126it [1:23:59,  6.81s/it][ATokenizing tekst text into sentances: 1126it [1:23:59,  6.80s/it]


reducing sentences to a single doc embeding: 1127it [1:24:24, 12.86s/it][A[A[A

embedding all sents with BERT: 1127it [1:24:24, 12.86s/it][A[A
cleaning sentances: 1127it [1:24:26, 12.86s/it][ATokenizing tekst text into sentances: 1127it [1:24:26, 12.86s/it]


reducing sentences to a single doc embeding: 1128it [1:24:28, 10.31s/it][A[A[A

embedding all sents with BERT: 1128it [1:24:28, 10.31s/it][A[A
cleaning sentances: 1128it [1:24:30, 10.31s/it][ATokenizing tekst text into sentances: 1128it [1:24:30, 10.31s/it]


reducing sentences to a single doc embeding: 1129it [1:24:29,  7.42s/it][A[A[A

embedding all sents with BERT: 1129it [1:24:29,  7.42s/it][A[A
cleaning sentances: 1129it [1:24:31,  7.42s/it][ATokenizing tekst text into sentances: 1129it [1:24:31,  7.42s/it]


reducing sentences to a single doc embeding: 1130it [1:24:38,  8.11s/it][A[A[A

embedding all sents with BERT: 1130it [1:24:38,  8.11s/it][A[A
cleaning sentances: 1130it [1:24:41,  8.11s/it][ATokenizing tekst text into sentances: 1130it [1:24:41,  8.11s/it]


reducing sentences to a single doc embeding: 1131it [1:24:40,  6.28s/it][A[A[A

embedding all sents with BERT: 1131it [1:24:40,  6.28s/it][A[A
cleaning sentances: 1131it [1:24:43,  6.28s/it][ATokenizing tekst text into sentances: 1131it [1:24:43,  6.28s/it]


reducing sentences to a single doc embeding: 1132it [1:24:44,  5.45s/it][A[A[A

embedding all sents with BERT: 1132it [1:24:44,  5.45s/it][A[A
cleaning sentances: 1132it [1:24:46,  5.45s/it][ATokenizing tekst text into sentances: 1132it [1:24:46,  5.45s/it]


reducing sentences to a single doc embeding: 1133it [1:24:45,  4.17s/it][A[A[A

embedding all sents with BERT: 1133it [1:24:45,  4.17s/it][A[A
cleaning sentances: 1133it [1:24:47,  4.17s/it][ATokenizing tekst text into sentances: 1133it [1:24:47,  4.17s/it]


reducing sentences to a single doc embeding: 1134it [1:24:47,  3.56s/it][A[A[A

embedding all sents with BERT: 1134it [1:24:47,  3.56s/it][A[A
cleaning sentances: 1134it [1:24:50,  3.56s/it][ATokenizing tekst text into sentances: 1134it [1:24:50,  3.56s/it]


reducing sentences to a single doc embeding: 1135it [1:24:49,  3.07s/it][A[A[A

embedding all sents with BERT: 1135it [1:24:49,  3.07s/it][A[A
cleaning sentances: 1135it [1:24:52,  3.07s/it][ATokenizing tekst text into sentances: 1135it [1:24:52,  3.07s/it]


reducing sentences to a single doc embeding: 1136it [1:24:50,  2.31s/it][A[A[A

embedding all sents with BERT: 1136it [1:24:50,  2.31s/it][A[A
cleaning sentances: 1136it [1:24:52,  2.31s/it][ATokenizing tekst text into sentances: 1136it [1:24:52,  2.31s/it]


reducing sentences to a single doc embeding: 1137it [1:24:59,  4.47s/it][A[A[A

embedding all sents with BERT: 1137it [1:24:59,  4.47s/it][A[A
cleaning sentances: 1137it [1:25:02,  4.47s/it][ATokenizing tekst text into sentances: 1137it [1:25:02,  4.47s/it]


reducing sentences to a single doc embeding: 1138it [1:25:00,  3.29s/it][A[A[A

embedding all sents with BERT: 1138it [1:25:00,  3.29s/it][A[A
cleaning sentances: 1138it [1:25:02,  3.29s/it][ATokenizing tekst text into sentances: 1138it [1:25:02,  3.29s/it]


reducing sentences to a single doc embeding: 1139it [1:25:06,  4.13s/it][A[A[A

embedding all sents with BERT: 1139it [1:25:06,  4.13s/it][A[A
cleaning sentances: 1139it [1:25:08,  4.13s/it][ATokenizing tekst text into sentances: 1139it [1:25:08,  4.13s/it]


reducing sentences to a single doc embeding: 1140it [1:25:08,  3.59s/it][A[A[A

embedding all sents with BERT: 1140it [1:25:08,  3.59s/it][A[A
cleaning sentances: 1140it [1:25:11,  3.59s/it][ATokenizing tekst text into sentances: 1140it [1:25:11,  3.59s/it]


reducing sentences to a single doc embeding: 1141it [1:25:10,  2.94s/it][A[A[A

embedding all sents with BERT: 1141it [1:25:10,  2.94s/it][A[A
cleaning sentances: 1141it [1:25:12,  2.94s/it][ATokenizing tekst text into sentances: 1141it [1:25:12,  2.94s/it]


reducing sentences to a single doc embeding: 1142it [1:25:13,  3.17s/it][A[A[A

embedding all sents with BERT: 1142it [1:25:13,  3.17s/it][A[A
cleaning sentances: 1142it [1:25:16,  3.17s/it][ATokenizing tekst text into sentances: 1142it [1:25:16,  3.17s/it]


reducing sentences to a single doc embeding: 1143it [1:25:18,  3.51s/it][A[A[A

embedding all sents with BERT: 1143it [1:25:18,  3.51s/it][A[A
cleaning sentances: 1143it [1:25:20,  3.51s/it][ATokenizing tekst text into sentances: 1143it [1:25:20,  3.51s/it]


reducing sentences to a single doc embeding: 1144it [1:25:20,  3.17s/it][A[A[A

embedding all sents with BERT: 1144it [1:25:20,  3.17s/it][A[A
cleaning sentances: 1144it [1:25:22,  3.17s/it][ATokenizing tekst text into sentances: 1144it [1:25:22,  3.17s/it]


reducing sentences to a single doc embeding: 1145it [1:25:27,  4.43s/it][A[A[A

embedding all sents with BERT: 1145it [1:25:27,  4.43s/it][A[A
cleaning sentances: 1145it [1:25:30,  4.43s/it][ATokenizing tekst text into sentances: 1145it [1:25:30,  4.43s/it]


reducing sentences to a single doc embeding: 1146it [1:25:44,  8.25s/it][A[A[A

embedding all sents with BERT: 1146it [1:25:44,  8.25s/it][A[A
cleaning sentances: 1146it [1:25:47,  8.25s/it][ATokenizing tekst text into sentances: 1146it [1:25:47,  8.25s/it]


reducing sentences to a single doc embeding: 1147it [1:25:46,  6.12s/it][A[A[A

embedding all sents with BERT: 1147it [1:25:46,  6.12s/it][A[A
cleaning sentances: 1147it [1:25:48,  6.12s/it][ATokenizing tekst text into sentances: 1147it [1:25:48,  6.12s/it]


reducing sentences to a single doc embeding: 1148it [1:25:52,  6.28s/it][A[A[A

embedding all sents with BERT: 1148it [1:25:52,  6.28s/it][A[A
cleaning sentances: 1148it [1:25:55,  6.28s/it][ATokenizing tekst text into sentances: 1148it [1:25:55,  6.28s/it]


reducing sentences to a single doc embeding: 1149it [1:25:57,  5.84s/it][A[A[A

embedding all sents with BERT: 1149it [1:25:57,  5.84s/it][A[A
cleaning sentances: 1149it [1:25:59,  5.84s/it][ATokenizing tekst text into sentances: 1149it [1:25:59,  5.84s/it]


reducing sentences to a single doc embeding: 1150it [1:25:58,  4.48s/it][A[A[A

embedding all sents with BERT: 1150it [1:25:58,  4.48s/it][A[A
cleaning sentances: 1150it [1:26:01,  4.48s/it][ATokenizing tekst text into sentances: 1150it [1:26:01,  4.48s/it]


reducing sentences to a single doc embeding: 1151it [1:26:03,  4.66s/it][A[A[A

embedding all sents with BERT: 1151it [1:26:03,  4.66s/it][A[A
cleaning sentances: 1151it [1:26:06,  4.66s/it][ATokenizing tekst text into sentances: 1151it [1:26:06,  4.66s/it]


reducing sentences to a single doc embeding: 1152it [1:26:09,  4.97s/it][A[A[A

embedding all sents with BERT: 1152it [1:26:09,  4.97s/it][A[A
cleaning sentances: 1152it [1:26:12,  4.97s/it][ATokenizing tekst text into sentances: 1152it [1:26:12,  4.97s/it]


reducing sentences to a single doc embeding: 1153it [1:26:10,  3.65s/it][A[A[A

embedding all sents with BERT: 1153it [1:26:10,  3.65s/it][A[A
cleaning sentances: 1153it [1:26:12,  3.65s/it][ATokenizing tekst text into sentances: 1153it [1:26:12,  3.65s/it]


reducing sentences to a single doc embeding: 1154it [1:26:10,  2.76s/it][A[A[A

embedding all sents with BERT: 1154it [1:26:10,  2.76s/it][A[A
cleaning sentances: 1154it [1:26:13,  2.76s/it][ATokenizing tekst text into sentances: 1154it [1:26:13,  2.76s/it]


reducing sentences to a single doc embeding: 1155it [1:26:12,  2.53s/it][A[A[A

embedding all sents with BERT: 1155it [1:26:12,  2.53s/it][A[A
cleaning sentances: 1155it [1:26:15,  2.53s/it][ATokenizing tekst text into sentances: 1155it [1:26:15,  2.53s/it]


reducing sentences to a single doc embeding: 1156it [1:26:13,  1.84s/it][A[A[A

embedding all sents with BERT: 1156it [1:26:13,  1.84s/it][A[A
cleaning sentances: 1156it [1:26:15,  1.84s/it][ATokenizing tekst text into sentances: 1156it [1:26:15,  1.84s/it]


reducing sentences to a single doc embeding: 1157it [1:26:19,  3.12s/it][A[A[A

embedding all sents with BERT: 1157it [1:26:19,  3.12s/it][A[A
cleaning sentances: 1157it [1:26:21,  3.12s/it][ATokenizing tekst text into sentances: 1157it [1:26:21,  3.12s/it]


reducing sentences to a single doc embeding: 1158it [1:26:20,  2.42s/it][A[A[A

embedding all sents with BERT: 1158it [1:26:20,  2.42s/it][A[A
cleaning sentances: 1158it [1:26:22,  2.42s/it][ATokenizing tekst text into sentances: 1158it [1:26:22,  2.42s/it]


reducing sentences to a single doc embeding: 1159it [1:26:20,  1.82s/it][A[A[A

embedding all sents with BERT: 1159it [1:26:20,  1.82s/it][A[A
cleaning sentances: 1159it [1:26:22,  1.82s/it][ATokenizing tekst text into sentances: 1159it [1:26:22,  1.82s/it]


reducing sentences to a single doc embeding: 1160it [1:26:21,  1.49s/it][A[A[A

embedding all sents with BERT: 1160it [1:26:21,  1.49s/it][A[A
cleaning sentances: 1160it [1:26:23,  1.49s/it][ATokenizing tekst text into sentances: 1160it [1:26:23,  1.49s/it]


reducing sentences to a single doc embeding: 1161it [1:26:33,  4.85s/it][A[A[A

embedding all sents with BERT: 1161it [1:26:33,  4.85s/it][A[A
cleaning sentances: 1161it [1:26:36,  4.85s/it][ATokenizing tekst text into sentances: 1161it [1:26:36,  4.85s/it]


reducing sentences to a single doc embeding: 1162it [1:26:34,  3.44s/it][A[A[A

embedding all sents with BERT: 1162it [1:26:34,  3.44s/it][A[A
cleaning sentances: 1162it [1:26:36,  3.44s/it][ATokenizing tekst text into sentances: 1162it [1:26:36,  3.44s/it]


reducing sentences to a single doc embeding: 1163it [1:26:34,  2.62s/it][A[A[A

embedding all sents with BERT: 1163it [1:26:34,  2.62s/it][A[A
cleaning sentances: 1163it [1:26:37,  2.62s/it][ATokenizing tekst text into sentances: 1163it [1:26:37,  2.62s/it]


reducing sentences to a single doc embeding: 1164it [1:26:49,  6.25s/it][A[A[A

embedding all sents with BERT: 1164it [1:26:49,  6.25s/it][A[A
cleaning sentances: 1164it [1:26:51,  6.25s/it][ATokenizing tekst text into sentances: 1164it [1:26:51,  6.25s/it]


reducing sentences to a single doc embeding: 1165it [1:26:51,  5.12s/it][A[A[A

embedding all sents with BERT: 1165it [1:26:51,  5.12s/it][A[A
cleaning sentances: 1165it [1:26:54,  5.12s/it][ATokenizing tekst text into sentances: 1165it [1:26:54,  5.12s/it]


reducing sentences to a single doc embeding: 1166it [1:26:52,  3.85s/it][A[A[A

embedding all sents with BERT: 1166it [1:26:52,  3.85s/it][A[A
cleaning sentances: 1166it [1:26:55,  3.85s/it][ATokenizing tekst text into sentances: 1166it [1:26:55,  3.85s/it]


reducing sentences to a single doc embeding: 1167it [1:26:53,  2.99s/it][A[A[A

embedding all sents with BERT: 1167it [1:26:53,  2.99s/it][A[A
cleaning sentances: 1167it [1:26:56,  2.99s/it][ATokenizing tekst text into sentances: 1167it [1:26:56,  2.99s/it]


reducing sentences to a single doc embeding: 1168it [1:27:06,  5.82s/it][A[A[A

embedding all sents with BERT: 1168it [1:27:06,  5.82s/it][A[A
cleaning sentances: 1168it [1:27:08,  5.82s/it][ATokenizing tekst text into sentances: 1168it [1:27:08,  5.82s/it]


reducing sentences to a single doc embeding: 1169it [1:27:32, 11.84s/it][A[A[A

embedding all sents with BERT: 1169it [1:27:32, 11.84s/it][A[A
cleaning sentances: 1169it [1:27:34, 11.84s/it][ATokenizing tekst text into sentances: 1169it [1:27:34, 11.84s/it]


reducing sentences to a single doc embeding: 1170it [1:27:32,  8.40s/it][A[A[A

embedding all sents with BERT: 1170it [1:27:32,  8.40s/it][A[A
cleaning sentances: 1170it [1:27:34,  8.40s/it][ATokenizing tekst text into sentances: 1170it [1:27:34,  8.40s/it]


reducing sentences to a single doc embeding: 1171it [1:27:34,  6.35s/it][A[A[A

embedding all sents with BERT: 1171it [1:27:34,  6.35s/it][A[A
cleaning sentances: 1171it [1:27:36,  6.35s/it][ATokenizing tekst text into sentances: 1171it [1:27:36,  6.35s/it]


reducing sentences to a single doc embeding: 1172it [1:27:34,  4.68s/it][A[A[A

embedding all sents with BERT: 1172it [1:27:34,  4.68s/it][A[A
cleaning sentances: 1172it [1:27:37,  4.68s/it][ATokenizing tekst text into sentances: 1172it [1:27:37,  4.68s/it]


reducing sentences to a single doc embeding: 1173it [1:27:35,  3.44s/it][A[A[A

embedding all sents with BERT: 1173it [1:27:35,  3.44s/it][A[A
cleaning sentances: 1173it [1:27:37,  3.44s/it][ATokenizing tekst text into sentances: 1173it [1:27:37,  3.44s/it]


reducing sentences to a single doc embeding: 1174it [1:27:36,  2.66s/it][A[A[A

embedding all sents with BERT: 1174it [1:27:36,  2.66s/it][A[A
cleaning sentances: 1174it [1:27:38,  2.66s/it][ATokenizing tekst text into sentances: 1174it [1:27:38,  2.66s/it]


reducing sentences to a single doc embeding: 1175it [1:27:42,  3.82s/it][A[A[A

embedding all sents with BERT: 1175it [1:27:42,  3.82s/it][A[A
cleaning sentances: 1175it [1:27:45,  3.82s/it][ATokenizing tekst text into sentances: 1175it [1:27:45,  3.82s/it]


reducing sentences to a single doc embeding: 1176it [1:27:48,  4.43s/it][A[A[A

embedding all sents with BERT: 1176it [1:27:48,  4.43s/it][A[A
cleaning sentances: 1176it [1:27:50,  4.43s/it][ATokenizing tekst text into sentances: 1176it [1:27:50,  4.43s/it]


reducing sentences to a single doc embeding: 1177it [1:27:49,  3.36s/it][A[A[A

embedding all sents with BERT: 1177it [1:27:49,  3.36s/it][A[A
cleaning sentances: 1177it [1:27:51,  3.36s/it][ATokenizing tekst text into sentances: 1177it [1:27:51,  3.36s/it]


reducing sentences to a single doc embeding: 1178it [1:28:07,  7.61s/it][A[A[A

embedding all sents with BERT: 1178it [1:28:07,  7.61s/it][A[A
cleaning sentances: 1178it [1:28:09,  7.61s/it][ATokenizing tekst text into sentances: 1178it [1:28:09,  7.61s/it]


reducing sentences to a single doc embeding: 1179it [1:28:08,  5.82s/it][A[A[A

embedding all sents with BERT: 1179it [1:28:08,  5.82s/it][A[A
cleaning sentances: 1179it [1:28:10,  5.82s/it][ATokenizing tekst text into sentances: 1179it [1:28:10,  5.82s/it]


reducing sentences to a single doc embeding: 1180it [1:28:14,  5.68s/it][A[A[A

embedding all sents with BERT: 1180it [1:28:14,  5.68s/it][A[A
cleaning sentances: 1180it [1:28:16,  5.68s/it][ATokenizing tekst text into sentances: 1180it [1:28:16,  5.68s/it]


reducing sentences to a single doc embeding: 1181it [1:28:17,  4.99s/it][A[A[A

embedding all sents with BERT: 1181it [1:28:17,  4.99s/it][A[A
cleaning sentances: 1181it [1:28:19,  4.99s/it][ATokenizing tekst text into sentances: 1181it [1:28:19,  4.99s/it]


reducing sentences to a single doc embeding: 1182it [1:28:20,  4.30s/it][A[A[A

embedding all sents with BERT: 1182it [1:28:20,  4.30s/it][A[A
cleaning sentances: 1182it [1:28:22,  4.30s/it][ATokenizing tekst text into sentances: 1182it [1:28:22,  4.30s/it]


reducing sentences to a single doc embeding: 1183it [1:28:20,  3.17s/it][A[A[A

embedding all sents with BERT: 1183it [1:28:20,  3.17s/it][A[A
cleaning sentances: 1183it [1:28:22,  3.17s/it][ATokenizing tekst text into sentances: 1183it [1:28:22,  3.17s/it]


reducing sentences to a single doc embeding: 1184it [1:28:26,  3.98s/it][A[A[A

embedding all sents with BERT: 1184it [1:28:26,  3.98s/it][A[A
cleaning sentances: 1184it [1:28:28,  3.98s/it][ATokenizing tekst text into sentances: 1184it [1:28:28,  3.98s/it]


reducing sentences to a single doc embeding: 1185it [1:28:28,  3.33s/it][A[A[A

embedding all sents with BERT: 1185it [1:28:28,  3.33s/it][A[A
cleaning sentances: 1185it [1:28:30,  3.33s/it][ATokenizing tekst text into sentances: 1185it [1:28:30,  3.33s/it]


reducing sentences to a single doc embeding: 1186it [1:28:33,  3.89s/it][A[A[A

embedding all sents with BERT: 1186it [1:28:33,  3.89s/it][A[A
cleaning sentances: 1186it [1:28:35,  3.89s/it][ATokenizing tekst text into sentances: 1186it [1:28:35,  3.89s/it]


reducing sentences to a single doc embeding: 1187it [1:28:33,  2.82s/it][A[A[A

embedding all sents with BERT: 1187it [1:28:33,  2.82s/it][A[A
cleaning sentances: 1187it [1:28:36,  2.82s/it][ATokenizing tekst text into sentances: 1187it [1:28:36,  2.82s/it]


reducing sentences to a single doc embeding: 1188it [1:28:35,  2.42s/it][A[A[A

embedding all sents with BERT: 1188it [1:28:35,  2.42s/it][A[A
cleaning sentances: 1188it [1:28:37,  2.42s/it][ATokenizing tekst text into sentances: 1188it [1:28:37,  2.42s/it]


reducing sentences to a single doc embeding: 1189it [1:28:35,  1.89s/it][A[A[A

embedding all sents with BERT: 1189it [1:28:35,  1.89s/it][A[A
cleaning sentances: 1189it [1:28:38,  1.89s/it][ATokenizing tekst text into sentances: 1189it [1:28:38,  1.89s/it]


reducing sentences to a single doc embeding: 1190it [1:28:38,  2.22s/it][A[A[A

embedding all sents with BERT: 1190it [1:28:38,  2.22s/it][A[A
cleaning sentances: 1190it [1:28:41,  2.22s/it][ATokenizing tekst text into sentances: 1190it [1:28:41,  2.22s/it]


reducing sentences to a single doc embeding: 1191it [1:28:40,  1.97s/it][A[A[A

embedding all sents with BERT: 1191it [1:28:40,  1.97s/it][A[A
cleaning sentances: 1191it [1:28:42,  1.97s/it][ATokenizing tekst text into sentances: 1191it [1:28:42,  1.97s/it]


reducing sentences to a single doc embeding: 1192it [1:28:41,  1.70s/it][A[A[A

embedding all sents with BERT: 1192it [1:28:41,  1.70s/it][A[A
cleaning sentances: 1192it [1:28:43,  1.70s/it][ATokenizing tekst text into sentances: 1192it [1:28:43,  1.70s/it]


reducing sentences to a single doc embeding: 1193it [1:28:41,  1.30s/it][A[A[A

embedding all sents with BERT: 1193it [1:28:41,  1.30s/it][A[A
cleaning sentances: 1193it [1:28:44,  1.30s/it][ATokenizing tekst text into sentances: 1193it [1:28:44,  1.30s/it]


reducing sentences to a single doc embeding: 1194it [1:28:42,  1.04s/it][A[A[A

embedding all sents with BERT: 1194it [1:28:42,  1.04s/it][A[A
cleaning sentances: 1194it [1:28:44,  1.04s/it][ATokenizing tekst text into sentances: 1194it [1:28:44,  1.04s/it]


reducing sentences to a single doc embeding: 1195it [1:28:50,  3.26s/it][A[A[A

embedding all sents with BERT: 1195it [1:28:50,  3.26s/it][A[A
cleaning sentances: 1195it [1:28:52,  3.26s/it][ATokenizing tekst text into sentances: 1195it [1:28:52,  3.26s/it]


reducing sentences to a single doc embeding: 1196it [1:28:51,  2.47s/it][A[A[A

embedding all sents with BERT: 1196it [1:28:51,  2.47s/it][A[A
cleaning sentances: 1196it [1:28:53,  2.47s/it][ATokenizing tekst text into sentances: 1196it [1:28:53,  2.47s/it]


reducing sentences to a single doc embeding: 1197it [1:28:54,  2.76s/it][A[A[A

embedding all sents with BERT: 1197it [1:28:54,  2.76s/it][A[A
cleaning sentances: 1197it [1:28:57,  2.76s/it][ATokenizing tekst text into sentances: 1197it [1:28:57,  2.76s/it]


reducing sentences to a single doc embeding: 1198it [1:28:59,  3.39s/it][A[A[A

embedding all sents with BERT: 1198it [1:28:59,  3.39s/it][A[A
cleaning sentances: 1198it [1:29:01,  3.39s/it][ATokenizing tekst text into sentances: 1198it [1:29:01,  3.39s/it]


reducing sentences to a single doc embeding: 1199it [1:29:00,  2.55s/it][A[A[A

embedding all sents with BERT: 1199it [1:29:00,  2.55s/it][A[A
cleaning sentances: 1199it [1:29:02,  2.55s/it][ATokenizing tekst text into sentances: 1199it [1:29:02,  2.55s/it]


reducing sentences to a single doc embeding: 1200it [1:29:01,  2.32s/it][A[A[A

embedding all sents with BERT: 1200it [1:29:01,  2.32s/it][A[A
cleaning sentances: 1200it [1:29:04,  2.32s/it][ATokenizing tekst text into sentances: 1200it [1:29:04,  2.32s/it]


reducing sentences to a single doc embeding: 1201it [1:29:08,  3.49s/it][A[A[A

embedding all sents with BERT: 1201it [1:29:08,  3.49s/it][A[A
cleaning sentances: 1201it [1:29:10,  3.49s/it][ATokenizing tekst text into sentances: 1201it [1:29:10,  3.49s/it]


reducing sentences to a single doc embeding: 1202it [1:29:08,  2.64s/it][A[A[A

embedding all sents with BERT: 1202it [1:29:08,  2.64s/it][A[A
cleaning sentances: 1202it [1:29:11,  2.64s/it][ATokenizing tekst text into sentances: 1202it [1:29:11,  2.64s/it]


reducing sentences to a single doc embeding: 1203it [1:29:33,  9.12s/it][A[A[A

embedding all sents with BERT: 1203it [1:29:33,  9.12s/it][A[A
cleaning sentances: 1203it [1:29:35,  9.12s/it][ATokenizing tekst text into sentances: 1203it [1:29:35,  9.12s/it]


reducing sentences to a single doc embeding: 1204it [1:29:35,  7.10s/it][A[A[A

embedding all sents with BERT: 1204it [1:29:35,  7.10s/it][A[A
cleaning sentances: 1204it [1:29:37,  7.10s/it][ATokenizing tekst text into sentances: 1204it [1:29:37,  7.10s/it]


reducing sentences to a single doc embeding: 1205it [1:29:37,  5.74s/it][A[A[A

embedding all sents with BERT: 1205it [1:29:37,  5.74s/it][A[A
cleaning sentances: 1205it [1:29:40,  5.74s/it][ATokenizing tekst text into sentances: 1205it [1:29:40,  5.74s/it]


reducing sentences to a single doc embeding: 1206it [1:29:51,  8.10s/it][A[A[A

embedding all sents with BERT: 1206it [1:29:51,  8.10s/it][A[A
cleaning sentances: 1206it [1:29:53,  8.10s/it][ATokenizing tekst text into sentances: 1206it [1:29:53,  8.10s/it]


reducing sentences to a single doc embeding: 1207it [1:29:52,  5.91s/it][A[A[A

embedding all sents with BERT: 1207it [1:29:52,  5.91s/it][A[A
cleaning sentances: 1207it [1:29:54,  5.91s/it][ATokenizing tekst text into sentances: 1207it [1:29:54,  5.91s/it]


reducing sentences to a single doc embeding: 1208it [1:30:08,  8.85s/it][A[A[A

embedding all sents with BERT: 1208it [1:30:08,  8.85s/it][A[A
cleaning sentances: 1208it [1:30:10,  8.85s/it][ATokenizing tekst text into sentances: 1208it [1:30:10,  8.85s/it]


reducing sentences to a single doc embeding: 1209it [1:30:08,  6.30s/it][A[A[A

embedding all sents with BERT: 1209it [1:30:08,  6.30s/it][A[A
cleaning sentances: 1209it [1:30:10,  6.30s/it][ATokenizing tekst text into sentances: 1209it [1:30:10,  6.30s/it]


reducing sentences to a single doc embeding: 1210it [1:30:11,  5.46s/it][A[A[A

embedding all sents with BERT: 1210it [1:30:11,  5.46s/it][A[A
cleaning sentances: 1210it [1:30:14,  5.46s/it][ATokenizing tekst text into sentances: 1210it [1:30:14,  5.46s/it]


reducing sentences to a single doc embeding: 1211it [1:30:12,  4.02s/it][A[A[A

embedding all sents with BERT: 1211it [1:30:12,  4.02s/it][A[A
cleaning sentances: 1211it [1:30:14,  4.02s/it][ATokenizing tekst text into sentances: 1211it [1:30:14,  4.02s/it]


reducing sentences to a single doc embeding: 1212it [1:30:13,  2.93s/it][A[A[A

embedding all sents with BERT: 1212it [1:30:13,  2.93s/it][A[A
cleaning sentances: 1212it [1:30:15,  2.93s/it][ATokenizing tekst text into sentances: 1212it [1:30:15,  2.93s/it]


reducing sentences to a single doc embeding: 1213it [1:30:16,  3.02s/it][A[A[A

embedding all sents with BERT: 1213it [1:30:16,  3.02s/it][A[A
cleaning sentances: 1213it [1:30:18,  3.02s/it][ATokenizing tekst text into sentances: 1213it [1:30:18,  3.02s/it]


reducing sentences to a single doc embeding: 1214it [1:30:18,  2.73s/it][A[A[A

embedding all sents with BERT: 1214it [1:30:18,  2.73s/it][A[A
cleaning sentances: 1214it [1:30:20,  2.73s/it][ATokenizing tekst text into sentances: 1214it [1:30:20,  2.73s/it]


reducing sentences to a single doc embeding: 1215it [1:30:20,  2.49s/it][A[A[A

embedding all sents with BERT: 1215it [1:30:20,  2.49s/it][A[A
cleaning sentances: 1215it [1:30:22,  2.49s/it][ATokenizing tekst text into sentances: 1215it [1:30:22,  2.49s/it]


reducing sentences to a single doc embeding: 1216it [1:30:21,  2.05s/it][A[A[A

embedding all sents with BERT: 1216it [1:30:21,  2.05s/it][A[A
cleaning sentances: 1216it [1:30:23,  2.05s/it][ATokenizing tekst text into sentances: 1216it [1:30:23,  2.05s/it]


reducing sentences to a single doc embeding: 1217it [1:30:23,  2.08s/it][A[A[A

embedding all sents with BERT: 1217it [1:30:23,  2.08s/it][A[A
cleaning sentances: 1217it [1:30:25,  2.08s/it][ATokenizing tekst text into sentances: 1217it [1:30:25,  2.08s/it]


reducing sentences to a single doc embeding: 1218it [1:30:28,  3.03s/it][A[A[A

embedding all sents with BERT: 1218it [1:30:28,  3.03s/it][A[A
cleaning sentances: 1218it [1:30:30,  3.03s/it][ATokenizing tekst text into sentances: 1218it [1:30:30,  3.03s/it]


reducing sentences to a single doc embeding: 1219it [1:30:34,  3.78s/it][A[A[A

embedding all sents with BERT: 1219it [1:30:34,  3.78s/it][A[A
cleaning sentances: 1219it [1:30:36,  3.78s/it][ATokenizing tekst text into sentances: 1219it [1:30:36,  3.78s/it]


reducing sentences to a single doc embeding: 1220it [1:30:36,  3.24s/it][A[A[A

embedding all sents with BERT: 1220it [1:30:36,  3.24s/it][A[A
cleaning sentances: 1220it [1:30:38,  3.24s/it][ATokenizing tekst text into sentances: 1220it [1:30:38,  3.24s/it]


reducing sentences to a single doc embeding: 1221it [1:30:54,  7.92s/it][A[A[A

embedding all sents with BERT: 1221it [1:30:54,  7.92s/it][A[A
cleaning sentances: 1221it [1:30:57,  7.92s/it][ATokenizing tekst text into sentances: 1221it [1:30:57,  7.92s/it]


reducing sentences to a single doc embeding: 1222it [1:30:56,  6.02s/it][A[A[A

embedding all sents with BERT: 1222it [1:30:56,  6.02s/it][A[A
cleaning sentances: 1222it [1:30:58,  6.02s/it][ATokenizing tekst text into sentances: 1222it [1:30:58,  6.02s/it]


reducing sentences to a single doc embeding: 1223it [1:30:58,  4.73s/it][A[A[A

embedding all sents with BERT: 1223it [1:30:58,  4.73s/it][A[A
cleaning sentances: 1223it [1:31:00,  4.73s/it][ATokenizing tekst text into sentances: 1223it [1:31:00,  4.73s/it]


reducing sentences to a single doc embeding: 1224it [1:30:58,  3.35s/it][A[A[A

embedding all sents with BERT: 1224it [1:30:58,  3.35s/it][A[A
cleaning sentances: 1224it [1:31:00,  3.35s/it][ATokenizing tekst text into sentances: 1224it [1:31:00,  3.35s/it]


reducing sentences to a single doc embeding: 1225it [1:30:59,  2.57s/it][A[A[A

embedding all sents with BERT: 1225it [1:30:59,  2.57s/it][A[A
cleaning sentances: 1225it [1:31:01,  2.57s/it][ATokenizing tekst text into sentances: 1225it [1:31:01,  2.57s/it]


reducing sentences to a single doc embeding: 1226it [1:30:59,  1.85s/it][A[A[A

embedding all sents with BERT: 1226it [1:30:59,  1.85s/it][A[A
cleaning sentances: 1226it [1:31:01,  1.85s/it][ATokenizing tekst text into sentances: 1226it [1:31:01,  1.85s/it]


reducing sentences to a single doc embeding: 1227it [1:30:59,  1.43s/it][A[A[A

embedding all sents with BERT: 1227it [1:30:59,  1.43s/it][A[A
cleaning sentances: 1227it [1:31:02,  1.43s/it][ATokenizing tekst text into sentances: 1227it [1:31:02,  1.43s/it]


reducing sentences to a single doc embeding: 1228it [1:31:01,  1.60s/it][A[A[A

embedding all sents with BERT: 1228it [1:31:01,  1.60s/it][A[A
cleaning sentances: 1228it [1:31:04,  1.60s/it][ATokenizing tekst text into sentances: 1228it [1:31:04,  1.60s/it]


reducing sentences to a single doc embeding: 1229it [1:31:02,  1.31s/it][A[A[A

embedding all sents with BERT: 1229it [1:31:02,  1.31s/it][A[A
cleaning sentances: 1229it [1:31:04,  1.31s/it][ATokenizing tekst text into sentances: 1229it [1:31:04,  1.31s/it]


reducing sentences to a single doc embeding: 1230it [1:31:05,  1.72s/it][A[A[A

embedding all sents with BERT: 1230it [1:31:05,  1.72s/it][A[A
cleaning sentances: 1230it [1:31:07,  1.72s/it][ATokenizing tekst text into sentances: 1230it [1:31:07,  1.72s/it]


reducing sentences to a single doc embeding: 1231it [1:31:06,  1.61s/it][A[A[A

embedding all sents with BERT: 1231it [1:31:06,  1.61s/it][A[A
cleaning sentances: 1231it [1:31:08,  1.61s/it][ATokenizing tekst text into sentances: 1231it [1:31:08,  1.61s/it]


reducing sentences to a single doc embeding: 1232it [1:31:07,  1.44s/it][A[A[A

embedding all sents with BERT: 1232it [1:31:07,  1.44s/it][A[A
cleaning sentances: 1232it [1:31:09,  1.44s/it][ATokenizing tekst text into sentances: 1232it [1:31:09,  1.44s/it]


reducing sentences to a single doc embeding: 1233it [1:31:07,  1.10s/it][A[A[A

embedding all sents with BERT: 1233it [1:31:07,  1.10s/it][A[A
cleaning sentances: 1233it [1:31:10,  1.10s/it][ATokenizing tekst text into sentances: 1233it [1:31:10,  1.10s/it]


reducing sentences to a single doc embeding: 1234it [1:31:09,  1.38s/it][A[A[A

embedding all sents with BERT: 1234it [1:31:09,  1.38s/it][A[A
cleaning sentances: 1234it [1:31:12,  1.38s/it][ATokenizing tekst text into sentances: 1234it [1:31:12,  1.38s/it]


reducing sentences to a single doc embeding: 1235it [1:31:11,  1.34s/it][A[A[A

embedding all sents with BERT: 1235it [1:31:11,  1.34s/it][A[A
cleaning sentances: 1235it [1:31:13,  1.34s/it][ATokenizing tekst text into sentances: 1235it [1:31:13,  1.34s/it]


reducing sentences to a single doc embeding: 1236it [1:31:11,  1.13s/it][A[A[A

embedding all sents with BERT: 1236it [1:31:11,  1.13s/it][A[A
cleaning sentances: 1236it [1:31:14,  1.13s/it][ATokenizing tekst text into sentances: 1236it [1:31:14,  1.13s/it]


reducing sentences to a single doc embeding: 1237it [1:31:13,  1.22s/it][A[A[A

embedding all sents with BERT: 1237it [1:31:13,  1.22s/it][A[A
cleaning sentances: 1237it [1:31:15,  1.22s/it][ATokenizing tekst text into sentances: 1237it [1:31:15,  1.22s/it]


reducing sentences to a single doc embeding: 1238it [1:31:16,  1.83s/it][A[A[A

embedding all sents with BERT: 1238it [1:31:16,  1.83s/it][A[A
cleaning sentances: 1238it [1:31:18,  1.83s/it][ATokenizing tekst text into sentances: 1238it [1:31:18,  1.83s/it]


reducing sentences to a single doc embeding: 1239it [1:31:16,  1.32s/it][A[A[A

embedding all sents with BERT: 1239it [1:31:16,  1.32s/it][A[A
cleaning sentances: 1239it [1:31:18,  1.32s/it][ATokenizing tekst text into sentances: 1239it [1:31:18,  1.32s/it]


reducing sentences to a single doc embeding: 1240it [1:31:17,  1.14s/it][A[A[A

embedding all sents with BERT: 1240it [1:31:17,  1.14s/it][A[A
cleaning sentances: 1240it [1:31:19,  1.14s/it][ATokenizing tekst text into sentances: 1240it [1:31:19,  1.14s/it]


reducing sentences to a single doc embeding: 1241it [1:31:20,  1.89s/it][A[A[A

embedding all sents with BERT: 1241it [1:31:20,  1.89s/it][A[A
cleaning sentances: 1241it [1:31:23,  1.89s/it][ATokenizing tekst text into sentances: 1241it [1:31:23,  1.89s/it]


reducing sentences to a single doc embeding: 1242it [1:31:21,  1.44s/it][A[A[A

embedding all sents with BERT: 1242it [1:31:21,  1.44s/it][A[A
cleaning sentances: 1242it [1:31:23,  1.44s/it][ATokenizing tekst text into sentances: 1242it [1:31:23,  1.44s/it]


reducing sentences to a single doc embeding: 1243it [1:31:27,  2.82s/it][A[A[A

embedding all sents with BERT: 1243it [1:31:27,  2.82s/it][A[A
cleaning sentances: 1243it [1:31:29,  2.82s/it][ATokenizing tekst text into sentances: 1243it [1:31:29,  2.82s/it]


reducing sentences to a single doc embeding: 1244it [1:31:28,  2.45s/it][A[A[A

embedding all sents with BERT: 1244it [1:31:28,  2.45s/it][A[A
cleaning sentances: 1244it [1:31:31,  2.45s/it][ATokenizing tekst text into sentances: 1244it [1:31:31,  2.45s/it]


reducing sentences to a single doc embeding: 1245it [1:31:31,  2.56s/it][A[A[A

embedding all sents with BERT: 1245it [1:31:31,  2.56s/it][A[A
cleaning sentances: 1245it [1:31:34,  2.56s/it][ATokenizing tekst text into sentances: 1245it [1:31:34,  2.56s/it]


reducing sentences to a single doc embeding: 1246it [1:31:32,  1.99s/it][A[A[A

embedding all sents with BERT: 1246it [1:31:32,  1.99s/it][A[A
cleaning sentances: 1246it [1:31:34,  1.99s/it][ATokenizing tekst text into sentances: 1246it [1:31:34,  1.99s/it]


reducing sentences to a single doc embeding: 1247it [1:31:34,  2.00s/it][A[A[A

embedding all sents with BERT: 1247it [1:31:34,  2.00s/it][A[A
cleaning sentances: 1247it [1:31:36,  2.00s/it][ATokenizing tekst text into sentances: 1247it [1:31:36,  2.00s/it]


reducing sentences to a single doc embeding: 1248it [1:31:35,  1.59s/it][A[A[A

embedding all sents with BERT: 1248it [1:31:35,  1.59s/it][A[A
cleaning sentances: 1248it [1:31:37,  1.59s/it][ATokenizing tekst text into sentances: 1248it [1:31:37,  1.59s/it]


reducing sentences to a single doc embeding: 1249it [1:31:48,  5.05s/it][A[A[A

embedding all sents with BERT: 1249it [1:31:48,  5.05s/it][A[A
cleaning sentances: 1249it [1:31:50,  5.05s/it][ATokenizing tekst text into sentances: 1249it [1:31:50,  5.05s/it]


reducing sentences to a single doc embeding: 1250it [1:31:48,  3.73s/it][A[A[A

embedding all sents with BERT: 1250it [1:31:48,  3.73s/it][A[A
cleaning sentances: 1250it [1:31:51,  3.73s/it][ATokenizing tekst text into sentances: 1250it [1:31:51,  3.73s/it]


reducing sentences to a single doc embeding: 1251it [1:31:49,  2.73s/it][A[A[A

embedding all sents with BERT: 1251it [1:31:49,  2.73s/it][A[A
cleaning sentances: 1251it [1:31:51,  2.73s/it][ATokenizing tekst text into sentances: 1251it [1:31:51,  2.73s/it]


reducing sentences to a single doc embeding: 1252it [1:31:58,  4.78s/it][A[A[A

embedding all sents with BERT: 1252it [1:31:58,  4.78s/it][A[A
cleaning sentances: 1252it [1:32:01,  4.78s/it][ATokenizing tekst text into sentances: 1252it [1:32:01,  4.78s/it]


reducing sentences to a single doc embeding: 1253it [1:31:59,  3.55s/it][A[A[A

embedding all sents with BERT: 1253it [1:31:59,  3.55s/it][A[A
cleaning sentances: 1253it [1:32:01,  3.55s/it][ATokenizing tekst text into sentances: 1253it [1:32:01,  3.55s/it]


reducing sentences to a single doc embeding: 1254it [1:32:01,  3.15s/it][A[A[A

embedding all sents with BERT: 1254it [1:32:01,  3.15s/it][A[A
cleaning sentances: 1254it [1:32:03,  3.15s/it][ATokenizing tekst text into sentances: 1254it [1:32:03,  3.15s/it]


reducing sentences to a single doc embeding: 1255it [1:32:03,  2.71s/it][A[A[A

embedding all sents with BERT: 1255it [1:32:03,  2.71s/it][A[A
cleaning sentances: 1255it [1:32:05,  2.71s/it][ATokenizing tekst text into sentances: 1255it [1:32:05,  2.71s/it]


reducing sentences to a single doc embeding: 1256it [1:32:04,  2.14s/it][A[A[A

embedding all sents with BERT: 1256it [1:32:04,  2.14s/it][A[A
cleaning sentances: 1256it [1:32:06,  2.14s/it][ATokenizing tekst text into sentances: 1256it [1:32:06,  2.14s/it]


reducing sentences to a single doc embeding: 1257it [1:32:04,  1.63s/it][A[A[A

embedding all sents with BERT: 1257it [1:32:04,  1.63s/it][A[A
cleaning sentances: 1257it [1:32:06,  1.63s/it][ATokenizing tekst text into sentances: 1257it [1:32:06,  1.63s/it]


reducing sentences to a single doc embeding: 1258it [1:32:10,  2.87s/it][A[A[A

embedding all sents with BERT: 1258it [1:32:10,  2.87s/it][A[A
cleaning sentances: 1258it [1:32:12,  2.87s/it][ATokenizing tekst text into sentances: 1258it [1:32:12,  2.87s/it]


reducing sentences to a single doc embeding: 1259it [1:32:42, 11.67s/it][A[A[A

embedding all sents with BERT: 1259it [1:32:42, 11.67s/it][A[A
cleaning sentances: 1259it [1:32:44, 11.67s/it][ATokenizing tekst text into sentances: 1259it [1:32:44, 11.67s/it]


reducing sentences to a single doc embeding: 1260it [1:32:46,  9.32s/it][A[A[A

embedding all sents with BERT: 1260it [1:32:46,  9.32s/it][A[A
cleaning sentances: 1260it [1:32:48,  9.32s/it][ATokenizing tekst text into sentances: 1260it [1:32:48,  9.32s/it]


reducing sentences to a single doc embeding: 1261it [1:32:47,  6.89s/it][A[A[A

embedding all sents with BERT: 1261it [1:32:47,  6.89s/it][A[A
cleaning sentances: 1261it [1:32:49,  6.89s/it][ATokenizing tekst text into sentances: 1261it [1:32:49,  6.89s/it]


reducing sentences to a single doc embeding: 1262it [1:32:48,  5.00s/it][A[A[A

embedding all sents with BERT: 1262it [1:32:48,  5.00s/it][A[A
cleaning sentances: 1262it [1:32:50,  5.00s/it][ATokenizing tekst text into sentances: 1262it [1:32:50,  5.00s/it]


reducing sentences to a single doc embeding: 1263it [1:32:49,  4.02s/it][A[A[A

embedding all sents with BERT: 1263it [1:32:49,  4.02s/it][A[A
cleaning sentances: 1263it [1:32:52,  4.02s/it][ATokenizing tekst text into sentances: 1263it [1:32:52,  4.02s/it]


reducing sentences to a single doc embeding: 1264it [1:32:50,  3.09s/it][A[A[A

embedding all sents with BERT: 1264it [1:32:50,  3.09s/it][A[A
cleaning sentances: 1264it [1:32:53,  3.09s/it][ATokenizing tekst text into sentances: 1264it [1:32:53,  3.09s/it]


reducing sentences to a single doc embeding: 1265it [1:33:10,  7.96s/it][A[A[A

embedding all sents with BERT: 1265it [1:33:10,  7.96s/it][A[A
cleaning sentances: 1265it [1:33:12,  7.96s/it][ATokenizing tekst text into sentances: 1265it [1:33:12,  7.96s/it]


reducing sentences to a single doc embeding: 1266it [1:33:21,  8.97s/it][A[A[A

embedding all sents with BERT: 1266it [1:33:21,  8.97s/it][A[A
cleaning sentances: 1266it [1:33:23,  8.97s/it][ATokenizing tekst text into sentances: 1266it [1:33:23,  8.97s/it]


reducing sentences to a single doc embeding: 1267it [1:33:21,  6.43s/it][A[A[A

embedding all sents with BERT: 1267it [1:33:21,  6.43s/it][A[A
cleaning sentances: 1267it [1:33:24,  6.43s/it][ATokenizing tekst text into sentances: 1267it [1:33:24,  6.43s/it]


reducing sentences to a single doc embeding: 1268it [1:33:32,  7.78s/it][A[A[A

embedding all sents with BERT: 1268it [1:33:32,  7.78s/it][A[A
cleaning sentances: 1268it [1:33:35,  7.78s/it][ATokenizing tekst text into sentances: 1268it [1:33:35,  7.78s/it]


reducing sentences to a single doc embeding: 1269it [1:33:36,  6.53s/it][A[A[A

embedding all sents with BERT: 1269it [1:33:36,  6.53s/it][A[A
cleaning sentances: 1269it [1:33:38,  6.53s/it][ATokenizing tekst text into sentances: 1269it [1:33:38,  6.53s/it]


reducing sentences to a single doc embeding: 1270it [1:33:41,  6.01s/it][A[A[A

embedding all sents with BERT: 1270it [1:33:41,  6.01s/it][A[A
cleaning sentances: 1270it [1:33:43,  6.01s/it][ATokenizing tekst text into sentances: 1270it [1:33:43,  6.01s/it]


reducing sentences to a single doc embeding: 1271it [1:33:43,  4.71s/it][A[A[A

embedding all sents with BERT: 1271it [1:33:43,  4.71s/it][A[A
cleaning sentances: 1271it [1:33:45,  4.71s/it][ATokenizing tekst text into sentances: 1271it [1:33:45,  4.71s/it]


reducing sentences to a single doc embeding: 1272it [1:33:45,  3.98s/it][A[A[A

embedding all sents with BERT: 1272it [1:33:45,  3.98s/it][A[A
cleaning sentances: 1272it [1:33:47,  3.98s/it][ATokenizing tekst text into sentances: 1272it [1:33:47,  3.98s/it]


reducing sentences to a single doc embeding: 1273it [1:33:45,  2.87s/it][A[A[A

embedding all sents with BERT: 1273it [1:33:45,  2.87s/it][A[A
cleaning sentances: 1273it [1:33:47,  2.87s/it][ATokenizing tekst text into sentances: 1273it [1:33:47,  2.87s/it]


reducing sentences to a single doc embeding: 1274it [1:33:48,  2.90s/it][A[A[A

embedding all sents with BERT: 1274it [1:33:48,  2.90s/it][A[A
cleaning sentances: 1274it [1:33:50,  2.90s/it][ATokenizing tekst text into sentances: 1274it [1:33:50,  2.90s/it]


reducing sentences to a single doc embeding: 1275it [1:33:51,  2.93s/it][A[A[A

embedding all sents with BERT: 1275it [1:33:51,  2.93s/it][A[A
cleaning sentances: 1275it [1:33:53,  2.93s/it][ATokenizing tekst text into sentances: 1275it [1:33:53,  2.93s/it]


reducing sentences to a single doc embeding: 1276it [1:33:52,  2.27s/it][A[A[A

embedding all sents with BERT: 1276it [1:33:52,  2.27s/it][A[A
cleaning sentances: 1276it [1:33:54,  2.27s/it][ATokenizing tekst text into sentances: 1276it [1:33:54,  2.27s/it]


reducing sentences to a single doc embeding: 1277it [1:34:00,  3.92s/it][A[A[A

embedding all sents with BERT: 1277it [1:34:00,  3.92s/it][A[A
cleaning sentances: 1277it [1:34:02,  3.92s/it][ATokenizing tekst text into sentances: 1277it [1:34:02,  3.92s/it]


reducing sentences to a single doc embeding: 1278it [1:34:00,  2.92s/it][A[A[A

embedding all sents with BERT: 1278it [1:34:00,  2.92s/it][A[A
cleaning sentances: 1278it [1:34:02,  2.92s/it][ATokenizing tekst text into sentances: 1278it [1:34:02,  2.92s/it]


reducing sentences to a single doc embeding: 1279it [1:34:01,  2.35s/it][A[A[A

embedding all sents with BERT: 1279it [1:34:01,  2.35s/it][A[A
cleaning sentances: 1279it [1:34:03,  2.35s/it][ATokenizing tekst text into sentances: 1279it [1:34:03,  2.35s/it]


reducing sentences to a single doc embeding: 1280it [1:34:02,  1.97s/it][A[A[A

embedding all sents with BERT: 1280it [1:34:02,  1.97s/it][A[A
cleaning sentances: 1280it [1:34:05,  1.97s/it][ATokenizing tekst text into sentances: 1280it [1:34:05,  1.97s/it]


reducing sentences to a single doc embeding: 1281it [1:34:05,  2.08s/it][A[A[A

embedding all sents with BERT: 1281it [1:34:05,  2.08s/it][A[A
cleaning sentances: 1281it [1:34:07,  2.08s/it][ATokenizing tekst text into sentances: 1281it [1:34:07,  2.08s/it]


reducing sentences to a single doc embeding: 1282it [1:34:07,  2.09s/it][A[A[A

embedding all sents with BERT: 1282it [1:34:07,  2.09s/it][A[A
cleaning sentances: 1282it [1:34:09,  2.09s/it][ATokenizing tekst text into sentances: 1282it [1:34:09,  2.09s/it]


reducing sentences to a single doc embeding: 1283it [1:34:07,  1.70s/it][A[A[A

embedding all sents with BERT: 1283it [1:34:07,  1.70s/it][A[A
cleaning sentances: 1283it [1:34:10,  1.70s/it][ATokenizing tekst text into sentances: 1283it [1:34:10,  1.70s/it]


reducing sentences to a single doc embeding: 1284it [1:34:18,  4.43s/it][A[A[A

embedding all sents with BERT: 1284it [1:34:18,  4.43s/it][A[A
cleaning sentances: 1284it [1:34:21,  4.43s/it][ATokenizing tekst text into sentances: 1284it [1:34:21,  4.43s/it]/home/ml/search_engine_nlp_tools/py36nlp/lib64/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
Tokenizing tekst text into sentances: 0it [00:00, ?it/s]
cleaning sentances: 0it [00:00, ?it/s][A

embedding all sents with BERT: 0it [00:00, ?it/s][A[A


reducing sentences to a single doc embeding: 0it [00:00, ?it/s][A[A[A


reducing sentences to a single doc embeding: 1it [00:04,  4.55s/it][A[A[A

embedding all sents with BERT: 1it [00:04,  4.55s/it][A[A
cleaning sentances: 1it [00:09,  9.28s/it][ATokenizing tekst text into sentances: 1it [00:09,  9.28s/it]


reducing sentences to a single doc embeding: 2it [00:06,  3.09s/it][A[A[A

embedding all sents with BERT: 2it [00:06,  3.09s/it][A[A
cleaning sentances: 2it [00:11,  5.04s/it][ATokenizing tekst text into sentances: 2it [00:11,  5.04s/it]


reducing sentences to a single doc embeding: 3it [00:09,  2.86s/it][A[A[A

embedding all sents with BERT: 3it [00:09,  2.86s/it][A[A
cleaning sentances: 3it [00:13,  3.92s/it][ATokenizing tekst text into sentances: 3it [00:13,  3.92s/it]


reducing sentences to a single doc embeding: 4it [00:11,  2.81s/it][A[A[A

embedding all sents with BERT: 4it [00:11,  2.81s/it][A[A
cleaning sentances: 4it [00:16,  3.45s/it][ATokenizing tekst text into sentances: 4it [00:16,  3.45s/it]


reducing sentences to a single doc embeding: 5it [00:16,  3.38s/it][A[A[A

embedding all sents with BERT: 5it [00:16,  3.38s/it][A[A
cleaning sentances: 5it [00:21,  3.79s/it][ATokenizing tekst text into sentances: 5it [00:21,  3.79s/it]


reducing sentences to a single doc embeding: 6it [00:18,  2.94s/it][A[A[A

embedding all sents with BERT: 6it [00:18,  2.94s/it][A[A
cleaning sentances: 6it [00:23,  3.21s/it][ATokenizing tekst text into sentances: 6it [00:23,  3.21s/it]


reducing sentences to a single doc embeding: 7it [00:20,  2.67s/it][A[A[A

embedding all sents with BERT: 7it [00:20,  2.67s/it][A[A
cleaning sentances: 7it [00:25,  2.85s/it][ATokenizing tekst text into sentances: 7it [00:25,  2.85s/it]


reducing sentences to a single doc embeding: 8it [00:21,  2.27s/it][A[A[A

embedding all sents with BERT: 8it [00:21,  2.27s/it][A[A
cleaning sentances: 8it [00:26,  2.39s/it][ATokenizing tekst text into sentances: 8it [00:26,  2.39s/it]


reducing sentences to a single doc embeding: 9it [00:23,  2.15s/it][A[A[A

embedding all sents with BERT: 9it [00:23,  2.15s/it][A[A
cleaning sentances: 9it [00:28,  2.23s/it][ATokenizing tekst text into sentances: 9it [00:28,  2.23s/it]


reducing sentences to a single doc embeding: 10it [00:25,  1.90s/it][A[A[A

embedding all sents with BERT: 10it [00:25,  1.90s/it][A[A
cleaning sentances: 10it [00:29,  1.96s/it][ATokenizing tekst text into sentances: 10it [00:29,  1.96s/it]


reducing sentences to a single doc embeding: 11it [00:26,  1.70s/it][A[A[A

embedding all sents with BERT: 11it [00:26,  1.70s/it][A[A
cleaning sentances: 11it [00:31,  1.74s/it][ATokenizing tekst text into sentances: 11it [00:31,  1.74s/it]


reducing sentences to a single doc embeding: 12it [00:28,  1.88s/it][A[A[A

embedding all sents with BERT: 12it [00:28,  1.88s/it][A[A
cleaning sentances: 12it [00:33,  1.91s/it][ATokenizing tekst text into sentances: 12it [00:33,  1.91s/it]


reducing sentences to a single doc embeding: 13it [00:29,  1.54s/it][A[A[A

embedding all sents with BERT: 13it [00:29,  1.54s/it][A[A
cleaning sentances: 13it [00:34,  1.56s/it][ATokenizing tekst text into sentances: 13it [00:34,  1.56s/it]


reducing sentences to a single doc embeding: 14it [00:31,  1.65s/it][A[A[A

embedding all sents with BERT: 14it [00:31,  1.65s/it][A[A
cleaning sentances: 14it [00:36,  1.66s/it][ATokenizing tekst text into sentances: 14it [00:36,  1.66s/it]


reducing sentences to a single doc embeding: 15it [00:32,  1.58s/it][A[A[A

embedding all sents with BERT: 15it [00:32,  1.58s/it][A[A
cleaning sentances: 15it [00:37,  1.59s/it][ATokenizing tekst text into sentances: 15it [00:37,  1.59s/it]


reducing sentences to a single doc embeding: 16it [00:34,  1.56s/it][A[A[A

embedding all sents with BERT: 16it [00:34,  1.56s/it][A[A
cleaning sentances: 16it [00:39,  1.57s/it][ATokenizing tekst text into sentances: 16it [00:39,  1.57s/it]


reducing sentences to a single doc embeding: 17it [00:36,  1.61s/it][A[A[A

embedding all sents with BERT: 17it [00:36,  1.61s/it][A[A
cleaning sentances: 17it [00:40,  1.61s/it][ATokenizing tekst text into sentances: 17it [00:40,  1.61s/it]


reducing sentences to a single doc embeding: 18it [00:37,  1.67s/it][A[A[A

embedding all sents with BERT: 18it [00:37,  1.67s/it][A[A
cleaning sentances: 18it [00:42,  1.67s/it][ATokenizing tekst text into sentances: 18it [00:42,  1.67s/it]


reducing sentences to a single doc embeding: 19it [00:39,  1.58s/it][A[A[A

embedding all sents with BERT: 19it [00:39,  1.58s/it][A[A
cleaning sentances: 19it [00:43,  1.58s/it][ATokenizing tekst text into sentances: 19it [00:43,  1.58s/it]


reducing sentences to a single doc embeding: 20it [00:40,  1.47s/it][A[A[A

embedding all sents with BERT: 20it [00:40,  1.47s/it][A[A
cleaning sentances: 20it [00:45,  1.47s/it][ATokenizing tekst text into sentances: 20it [00:45,  1.47s/it]reducing sentences to a single doc embeding: 20it [00:41,  2.06s/it]
Opening connection
loading data
Closing connection
Traceback (most recent call last):
  File "__main__.py", line 12, in <module>
    AH.update_all() 
  File "/home/ml/search_engine_nlp_tools/data_handler/update_db_handler.py", line 25, in update_all
    self.update_documents()
  File "/home/ml/search_engine_nlp_tools/data_handler/update_db_handler.py", line 7, in update_documents
    values = zip([i.tolist() for i in self.embeded_doc_map], self.doc_id)
  File "/home/ml/search_engine_nlp_tools/data_handler/update_db_handler.py", line 7, in <listcomp>
    values = zip([i.tolist() for i in self.embeded_doc_map], self.doc_id)
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib/python3.6/site-packages/tqdm/std.py", line 1178, in __iter__
    for obj in iterable:
  File "/home/ml/search_engine_nlp_tools/algorithm_handler/algorithm_handler.py", line 48, in mean_of_emb_and_keep_sents
    emb_sents = list(emb_sents)
  File "/home/ml/search_engine_nlp_tools/algorithm_handler/algorithm_handler.py", line 36, in <lambda>
    embed_sent_l = lambda sent: embed_sent(sent)
  File "/home/ml/search_engine_nlp_tools/algorithm_handler/algorithm_handler.py", line 20, in embed_sent
    return model.embed_text(sent)[1]
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib/python3.6/site-packages/danlp/models/bert_models.py", line 412, in embed_text
    outputs = self.model(tokens_tensor, segments_tensors)
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py", line 853, in forward
    return_dict=return_dict,
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py", line 488, in forward
    output_attentions,
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py", line 407, in forward
    output_attentions=output_attentions,
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py", line 346, in forward
    attention_output = self.output(self_outputs[0], hidden_states)
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py", line 298, in forward
    hidden_states = self.dense(hidden_states)
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib64/python3.6/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/ml/search_engine_nlp_tools/py36nlp/lib64/python3.6/site-packages/torch/nn/functional.py", line 1692, in linear
    output = input.matmul(weight.t())
KeyboardInterrupt
Tokenizing tekst text into sentances: 20it [00:46,  2.31s/it]
cleaning sentances: 20it [00:46,  2.31s/it]
embedding all sents with BERT: 20it [00:41,  2.07s/it]